{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mestrado em Modelagem Matematica da Informacao\n",
    "----------------------------------------------\n",
    "Disciplina: Modelagem e Mineracao de Dados\n",
    "------------------------------------------\n",
    "\n",
    "Master Program - Mathematical Modeling of Information\n",
    "-----------------------------------------------------\n",
    "Course: Data Mining and Modeling\n",
    "--------------------------------\n",
    "\n",
    "Professor: Renato Rocha Souza\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic: Geographical and Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib.externals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8efc3d0af333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m   \u001b[0;31m#http://matplotlib.org/basemap/api/basemap_api.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/mpl_toolkits/basemap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes_grid1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/mpl_toolkits/axes_grid1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                         unicode_literals)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maxes_size\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib.externals'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import twitter\n",
    "import nltk\n",
    "import re\n",
    "import networkx as nx\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from itertools import chain\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import webbrowser\n",
    "import codecs\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap   #http://matplotlib.org/basemap/api/basemap_api.html\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import geopy.geocoders as gg\n",
    "from nominatim import Nominatim\n",
    "import Levenshtein\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Image, HTML, IFrame, FileLink, FileLinks #needed to render in notebook\n",
    "from IPython.core.display import display\n",
    "#Install http://www.graphviz.org/ & \n",
    "#Instal https://pypi.python.org/pypi/pydotplus\n",
    "import pydotplus\n",
    "\n",
    "%matplotlib inline\n",
    "# Set default figure size for this notebook\n",
    "plt.rcParams['figure.figsize'] = (16.0, 12.8)\n",
    "#plt.switch_backend('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the path to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "templates = \"../templates/\"\n",
    "outputs = \"../outputs/\"\n",
    "\n",
    "dotfile = \"graph_retweet.dot\"\n",
    "pngfile = \"graph_retweet.png\"\n",
    "protofile = \"graph_retweet.html\"\n",
    "tweetsfile = \"Tweets_dump.txt\"\n",
    "template_proto = 'template_protoviz.html'\n",
    "\n",
    "pathdotfile = os.path.join(outputs,dotfile)\n",
    "pathpngfile = os.path.join(outputs,pngfile)\n",
    "pathprotofile = os.path.join(outputs,protofile)\n",
    "pathtweetsfile = os.path.join(outputs,tweetsfile)\n",
    "pathtemplate = os.path.join(templates,template_proto)\n",
    "\n",
    "stoplist_en = nltk.corpus.stopwords.words('english')\n",
    "stoplist_pt = nltk.corpus.stopwords.words('portuguese')\n",
    "ignorewords = stoplist_en + stoplist_pt + ['',' ','-','rt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using geographical resources within Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pypi.python.org/pypi/geopy  \n",
    "\n",
    "gg.OpenMapQuest()\n",
    "geolocator = gg.GoogleV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To geolocate a query to an address and coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rua Dona Mariana - Botafogo, Rio de Janeiro - RJ, Brazil\n",
      "-22.9529422 -43.1886221\n"
     ]
    }
   ],
   "source": [
    "logradouro = \"Dona Mario ana, Botafoga\" #Note that there are typos in the names\n",
    "address, (latitude, longitude) = geolocator.geocode(logradouro)\n",
    "print(address)\n",
    "print(latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7755102040816326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measuring editing distances between names:\n",
    "Levenshtein.ratio('Dona Mariano, Botafoga', 'Rua Dona Mariana - Botafogo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800px\"\n",
       "            height=\"600px\"\n",
       "            src=\"http://maps.google.com/maps?q=Rua Dona Mariana - Botafogo, Rio de Janeiro - RJ, Brazil&loc:-22.9529422+-43.1886221&z=17&t=k&output=embed\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7faacf9232b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://developers.google.com/maps/documentation/staticmaps/\n",
    "#http://stackoverflow.com/questions/2660201/what-parameters-should-i-use-in-a-google-maps-url-to-go-to-a-lat-lon/9919251#9919251\n",
    "#m – normal map k – satellite h – hybrid p – terrain\n",
    "\n",
    "def gmap(address,lat,lon,zoom=15,tmap='m'):\n",
    "    # Google Maps URL template for an iframe\n",
    "    google_maps_url = 'http://maps.google.com/maps?q={0}&loc:{1}+{2}&z={3}&t={4}&output=embed'.format(address,\n",
    "                                                                                                     lat,\n",
    "                                                                                                     lon,\n",
    "                                                                                                     zoom,\n",
    "                                                                                                     tmap,)\n",
    "    display(IFrame(google_maps_url, '800px', '600px'))\n",
    "    \n",
    "gmap(address, latitude, longitude,17,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the address corresponding to a set of coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eixo Monumental, 139 - Brasília, DF, Brazil\n",
      "Praça dos Três Poderes - Brasília, DF, Brazil\n",
      "Brasília - Brasilia, Federal District, Brazil\n",
      "Brasilia, Federal District, Brazil\n",
      "Asa Sul Entrequadra Sul 414/415 - Brasília, DF, 70297-400, Brazil\n",
      "Brasilia - Federal District, Brazil\n",
      "Federal District, Brazil\n",
      "Brazil\n"
     ]
    }
   ],
   "source": [
    "addresses = geolocator.reverse(\"-15.798,-47.865\")\n",
    "for address in addresses:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python and QGIS for geospatial visualizations - a Case Study  \n",
    "https://www.airpair.com/python/posts/using-python-and-qgis-for-geospatial-visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = \"http://www.nuforc.org/webreports/\"\n",
    "index_url = \"http://www.nuforc.org/webreports/ndxevent.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def can_cast_as_dt(dateStr, fmt):\n",
    "    try:\n",
    "        datetime.strptime(dateStr, fmt)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def parse_dt(dateStr):\n",
    "    # the data in the website comes in two different formats, try both \n",
    "    for fmt in [\"%m/%d/%y %H:%M\", \"%m/%d/%y\"]:\n",
    "        try:\n",
    "            return datetime.strptime(dateStr, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "def get_data_from_url(url):\n",
    "    print(\"Processing {}\".format(url))\n",
    "    data = []\n",
    "    source = BeautifulSoup(urllib.request.urlopen(url), \"html5lib\")\n",
    "    for row in source('tr'):\n",
    "        if not row('td'):\n",
    "            continue # header row\n",
    "        row_data = row('td')\n",
    "        # parse the datetime from the string\n",
    "        date_time = parse_dt(row_data[0].text)\n",
    "        city = row_data[1].text\n",
    "        state = row_data[2].text\n",
    "        shape = row_data[3].text\n",
    "        duration = row_data[4].text\n",
    "        data.append((date_time, city, state, shape, duration))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing http://www.nuforc.org/webreports/ndxe201702.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201701.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201612.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201611.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201610.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201609.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201608.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201607.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201606.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201605.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201604.html\n",
      "Processing http://www.nuforc.org/webreports/ndxe201603.html\n"
     ]
    }
   ],
   "source": [
    "# get the index page\n",
    "raw_page = urllib.request.urlopen(index_url)\n",
    "source = BeautifulSoup(raw_page, \"html5lib\")\n",
    "# get all the links in the index page\n",
    "func1 = lambda x: (x.text, base_url + x['href'])\n",
    "monthly_urls = list(map(func1,source('a')))\n",
    "# get  the last 12 links that have a text like 06/2015\n",
    "func2 = lambda x: can_cast_as_dt(x[0], \"%m/%Y\")\n",
    "last_year_urls = filter(func2, monthly_urls[0:13]) \n",
    "# extract the data from each monthly page and flatten the lists of tuples\n",
    "last_year_ufos = list(chain(*map(lambda x: get_data_from_url(x[1]), last_year_urls)))\n",
    "# initialize a pandas DataFrame with the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df = pd.DataFrame(last_year_ufos, columns=[\"start\",\"city\",\"state\",\"shape\",\"duration_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-22 19:30:00</td>\n",
       "      <td>Swanville</td>\n",
       "      <td>ME</td>\n",
       "      <td>Light</td>\n",
       "      <td>45 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Simpsonville</td>\n",
       "      <td>SC</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1-2 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>CO</td>\n",
       "      <td>Fireball</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-21 11:15:00</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Formation</td>\n",
       "      <td>10 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-21 07:00:00</td>\n",
       "      <td>Basehor</td>\n",
       "      <td>KS</td>\n",
       "      <td>Light</td>\n",
       "      <td>2.3 hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start          city state      shape duration_description\n",
       "0 2017-02-22 19:30:00     Swanville    ME      Light           45 minutes\n",
       "1 2017-02-21 19:00:00  Simpsonville    SC       Oval          1-2 minutes\n",
       "2 2017-02-21 19:00:00     Nederland    CO   Fireball              unknown\n",
       "3 2017-02-21 11:15:00        Peoria    AZ  Formation           10 minutes\n",
       "4 2017-02-21 07:00:00       Basehor    KS      Light            2.3 hours"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufos_df.replace(to_replace='', value=np.nan, inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "ufos_df = ufos_df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-22 19:30:00</td>\n",
       "      <td>Swanville</td>\n",
       "      <td>ME</td>\n",
       "      <td>Light</td>\n",
       "      <td>45 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Simpsonville</td>\n",
       "      <td>SC</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1-2 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>CO</td>\n",
       "      <td>Fireball</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-21 11:15:00</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Formation</td>\n",
       "      <td>10 minutes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-21 07:00:00</td>\n",
       "      <td>Basehor</td>\n",
       "      <td>KS</td>\n",
       "      <td>Light</td>\n",
       "      <td>2.3 hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start          city state      shape duration_description\n",
       "0 2017-02-22 19:30:00     Swanville    ME      Light           45 minutes\n",
       "1 2017-02-21 19:00:00  Simpsonville    SC       Oval          1-2 minutes\n",
       "2 2017-02-21 19:00:00     Nederland    CO   Fireball              unknown\n",
       "3 2017-02-21 11:15:00        Peoria    AZ  Formation           10 minutes\n",
       "4 2017-02-21 07:00:00       Basehor    KS      Light            2.3 hours"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that infers the duration from the text \n",
    "def infer_duration_in_seconds(text):\n",
    "    # try different regexps to extract the total seconds\n",
    "    text = text.replace('<','')\n",
    "    text = text.replace('>','')\n",
    "    text = text.replace('?','')\n",
    "    text = text.replace('+','')\n",
    "    text = text.replace('~','')\n",
    "    metric_text = [\"second\",\"s\",\"Second\",\"segundo\",\"minute\",\"m\",\"min\",\"Minute\",\"hour\",\"h\",\"Hour\",'Currently']\n",
    "    metric_seconds = [1,1,1,1,60,60,60,3600,3600,3600,10]\n",
    "    for metric,mult in zip(metric_text, metric_seconds):\n",
    "        regex = \"\\s*(\\d+)\\+?\\s*{}s?\".format(metric)\n",
    "        res = re.findall(regex,text)\n",
    "        if len(res)>0:\n",
    "            return int(float(res[0]) * mult)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract the duration in seconds\n",
    "ufos_df[\"duration_secs\"] = ufos_df[\"duration_description\"].apply(infer_duration_in_seconds)\n",
    "\n",
    "# now we can infer the end time of the UFO sighting as well\n",
    "# which will be useful for the animation later\n",
    "ufos_df[\"end\"] = ufos_df.apply(lambda x:x[\"start\"] + timedelta(seconds=x[\"duration_secs\"]),axis=1)\n",
    "ufos_df = ufos_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_description</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-22 19:30:00</td>\n",
       "      <td>Swanville</td>\n",
       "      <td>ME</td>\n",
       "      <td>Light</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>2700</td>\n",
       "      <td>2017-02-22 20:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Simpsonville</td>\n",
       "      <td>SC</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1-2 minutes</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-02-21 19:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>CO</td>\n",
       "      <td>Fireball</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-21 11:15:00</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Formation</td>\n",
       "      <td>10 minutes</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-02-21 11:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-21 07:00:00</td>\n",
       "      <td>Basehor</td>\n",
       "      <td>KS</td>\n",
       "      <td>Light</td>\n",
       "      <td>2.3 hours</td>\n",
       "      <td>10800</td>\n",
       "      <td>2017-02-21 10:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start          city state      shape duration_description  \\\n",
       "0 2017-02-22 19:30:00     Swanville    ME      Light           45 minutes   \n",
       "1 2017-02-21 19:00:00  Simpsonville    SC       Oval          1-2 minutes   \n",
       "2 2017-02-21 19:00:00     Nederland    CO   Fireball              unknown   \n",
       "3 2017-02-21 11:15:00        Peoria    AZ  Formation           10 minutes   \n",
       "4 2017-02-21 07:00:00       Basehor    KS      Light            2.3 hours   \n",
       "\n",
       "   duration_secs                 end  \n",
       "0           2700 2017-02-22 20:15:00  \n",
       "1            120 2017-02-21 19:02:00  \n",
       "2              0 2017-02-21 19:00:00  \n",
       "3            600 2017-02-21 11:25:00  \n",
       "4          10800 2017-02-21 10:00:00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boundingbox': ['36.930691', '37.2206658', '-76.6460929', '-76.3616099'],\n",
       "  'class': 'place',\n",
       "  'display_name': 'Newport News, Newport News City, Virginia, United States of America',\n",
       "  'icon': 'http://nominatim.openstreetmap.org/images/mapicons/poi_place_city.p.20.png',\n",
       "  'importance': 0.70997691224355,\n",
       "  'lat': '37.016827',\n",
       "  'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright',\n",
       "  'lon': '-76.4505195',\n",
       "  'osm_id': '206655',\n",
       "  'osm_type': 'relation',\n",
       "  'place_id': '158807260',\n",
       "  'type': 'city'},\n",
       " {'boundingbox': ['37.0728977', '37.0731352', '-76.478543', '-76.478097'],\n",
       "  'class': 'amenity',\n",
       "  'display_name': 'Station 8, Kingstowne Drive, Deer Park, Newport News, Newport News City, Virginia, 23606, United States of America',\n",
       "  'icon': 'http://nominatim.openstreetmap.org/images/mapicons/amenity_firestation3.p.20.png',\n",
       "  'importance': 0.201,\n",
       "  'lat': '37.0730099',\n",
       "  'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright',\n",
       "  'lon': '-76.4783236633768',\n",
       "  'osm_id': '248256123',\n",
       "  'osm_type': 'way',\n",
       "  'place_id': '123766259',\n",
       "  'type': 'fire_station'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://wiki.openstreetmap.org/wiki/Nominatim_usage_policy\n",
    "# https://github.com/twain47/Nominatim/blob/master/docs/Installation.md\n",
    "geolocator = Nominatim()\n",
    "\n",
    "geolocator.query('Newport News')\n",
    "#geolocator.query(\"Houston, TX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-c:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "-c:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "-c:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Be careful with too many queries made to the server in a short period of time\n",
    "\n",
    "ufos_df[\"lat\"] = 0\n",
    "ufos_df[\"lon\"] = 0\n",
    "for i in range(len(ufos_df[0:10])):\n",
    "    try:\n",
    "        resp_json = geolocator.query(ufos_df['city'][i])\n",
    "        ufos_df[\"lat\"][i] = resp_json[0]['lat']\n",
    "        ufos_df[\"lon\"][i] = resp_json[0]['lon']\n",
    "    except:\n",
    "        ufos_df[\"lat\"][i] = 0\n",
    "        ufos_df[\"lat\"][i] = 0\n",
    "    time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration_description</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>end</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-22 19:30:00</td>\n",
       "      <td>Swanville</td>\n",
       "      <td>ME</td>\n",
       "      <td>Light</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>2700</td>\n",
       "      <td>2017-02-22 20:15:00</td>\n",
       "      <td>45.918332</td>\n",
       "      <td>-94.6381820136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Simpsonville</td>\n",
       "      <td>SC</td>\n",
       "      <td>Oval</td>\n",
       "      <td>1-2 minutes</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-02-21 19:02:00</td>\n",
       "      <td>34.7370639</td>\n",
       "      <td>-82.2542833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>Nederland</td>\n",
       "      <td>CO</td>\n",
       "      <td>Fireball</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-21 19:00:00</td>\n",
       "      <td>52.2379891</td>\n",
       "      <td>5.53460738161551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-02-21 11:15:00</td>\n",
       "      <td>Peoria</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Formation</td>\n",
       "      <td>10 minutes</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-02-21 11:25:00</td>\n",
       "      <td>40.6938609</td>\n",
       "      <td>-89.5891007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-02-21 07:00:00</td>\n",
       "      <td>Basehor</td>\n",
       "      <td>KS</td>\n",
       "      <td>Light</td>\n",
       "      <td>2.3 hours</td>\n",
       "      <td>10800</td>\n",
       "      <td>2017-02-21 10:00:00</td>\n",
       "      <td>39.1416692</td>\n",
       "      <td>-94.9385762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-02-20 21:30:00</td>\n",
       "      <td>Roswell</td>\n",
       "      <td>GA</td>\n",
       "      <td>Oval</td>\n",
       "      <td>10 minutes</td>\n",
       "      <td>600</td>\n",
       "      <td>2017-02-20 21:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-104.5229517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-02-20 21:00:00</td>\n",
       "      <td>Tucson</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Light</td>\n",
       "      <td>25 minutes</td>\n",
       "      <td>1500</td>\n",
       "      <td>2017-02-20 21:25:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-110.9264758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-02-20 21:00:00</td>\n",
       "      <td>Stacy</td>\n",
       "      <td>MN</td>\n",
       "      <td>Circle</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-20 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-120.0213171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-02-20 20:50:00</td>\n",
       "      <td>Porter township</td>\n",
       "      <td>PA</td>\n",
       "      <td>Light</td>\n",
       "      <td>20 minutes</td>\n",
       "      <td>1200</td>\n",
       "      <td>2017-02-20 21:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-79.3871033078269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-02-20 20:10:00</td>\n",
       "      <td>Parachute</td>\n",
       "      <td>CO</td>\n",
       "      <td>Oval</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-20 20:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start             city state      shape duration_description  \\\n",
       "0  2017-02-22 19:30:00        Swanville    ME      Light           45 minutes   \n",
       "1  2017-02-21 19:00:00     Simpsonville    SC       Oval          1-2 minutes   \n",
       "2  2017-02-21 19:00:00        Nederland    CO   Fireball              unknown   \n",
       "3  2017-02-21 11:15:00           Peoria    AZ  Formation           10 minutes   \n",
       "4  2017-02-21 07:00:00          Basehor    KS      Light            2.3 hours   \n",
       "6  2017-02-20 21:30:00          Roswell    GA       Oval           10 minutes   \n",
       "7  2017-02-20 21:00:00           Tucson    AZ      Light           25 minutes   \n",
       "8  2017-02-20 21:00:00            Stacy    MN     Circle                   30   \n",
       "9  2017-02-20 20:50:00  Porter township    PA      Light           20 minutes   \n",
       "10 2017-02-20 20:10:00        Parachute    CO       Oval                   20   \n",
       "\n",
       "    duration_secs                 end         lat                lon  \n",
       "0            2700 2017-02-22 20:15:00   45.918332  -94.6381820136364  \n",
       "1             120 2017-02-21 19:02:00  34.7370639        -82.2542833  \n",
       "2               0 2017-02-21 19:00:00  52.2379891   5.53460738161551  \n",
       "3             600 2017-02-21 11:25:00  40.6938609        -89.5891007  \n",
       "4           10800 2017-02-21 10:00:00  39.1416692        -94.9385762  \n",
       "6             600 2017-02-20 21:40:00           0       -104.5229517  \n",
       "7            1500 2017-02-20 21:25:00           0       -110.9264758  \n",
       "8               0 2017-02-20 21:00:00           0       -120.0213171  \n",
       "9            1200 2017-02-20 21:10:00           0  -79.3871033078269  \n",
       "10              0 2017-02-20 20:10:00           0                  0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://stackoverflow.com/questions/17098654/how-to-store-data-frame-using-pandas-python\n",
    "ufos_df.to_pickle(os.path.join(outputs,'ufos_df.pkl'))\n",
    "ufos_df = pd.read_pickle(os.path.join(outputs,'ufos_df.pkl'))\n",
    "ufos_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: dropna will drop any columns with None values, which is desirable\n",
    "ufos_df[[\"start\",\"end\",\"lon\",\"lat\",\"shape\"]].dropna().to_csv(os.path.join(outputs,'ufo_data.csv'),\n",
    "                                                             index=False, \n",
    "                                                             encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using geographical resources for twitter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bear/python-twitter (before was http://code.google.com/p/python-twitter/)  \n",
    "https://dev.twitter.com/docs  \n",
    "\n",
    "Twitter API Keys  \n",
    "Please generate yours...  \n",
    "Go to http://twitter.com/apps/new to create an app and get these items  \n",
    "See https://dev.twitter.com/docs/auth/oauth for more information on Twitter's OAuth implementation  \n",
    "https://dev.twitter.com/rest/reference/get/account/verify_credentials  \n",
    "https://dev.twitter.com/docs/auth/oauth  \n",
    "https://dev.twitter.com/apps/new  \n",
    "\n",
    "Inspiration: http://onemilliontweetmap.com/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twitter_tokens.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3359803d088f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter_tokens.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtwitter_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconsumer_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconsumer_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccess_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitter_tokens.txt'"
     ]
    }
   ],
   "source": [
    "with open('twitter_tokens.txt', 'r') as twitter_tokens:\n",
    "    tokens = twitter_tokens.read().split(',')\n",
    "consumer_key = tokens[0].strip()\n",
    "consumer_secret = tokens[1].strip()\n",
    "access_token = tokens[2].strip()\n",
    "access_token_secret = tokens[3].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessing Twitter (with or without authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#api = twitter.Api() # Accessing with no authentication\n",
    "api = twitter.Api(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent (random) public messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msgpublicas = api.GetStreamSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'text' in tweet:\n",
    "        print(u'{}\\n'.format(tweet['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'user' in tweet:\n",
    "        break\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url='http://mike.teczno.com/img/raffi-krikorian-map-of-a-tweet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent messages from an user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msguser = api.GetUserTimeline(tweet['user']['id'])\n",
    "print([s.text for s in msguser])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent messages from the authenticated user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msguser = api.GetUserTimeline('rrsouza')\n",
    "#msguser = api.GetUserTimeline('29959702')\n",
    "print([s.text for s in msguser])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After authentication, more options are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userfollow = api.GetFriends()\n",
    "print([u.name for u in userfollow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for a term in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_for_term(termo, pages, results):\n",
    "    '''Search and return tweets on a subject (5 pages of 100 results each)\n",
    "    Save results in a file defined in \"pathtweetsfile\" '''\n",
    "    search_results = []\n",
    "    tweets = []\n",
    "    tweets_txt = []\n",
    "    tweets_words = []\n",
    "    names = []\n",
    "    last = api.GetSearch(term=termo, count=1)\n",
    "    search_results.append(last)\n",
    "    list_ids = []\n",
    "    list_ids.append(last[0].id)\n",
    "    for i in range(pages):\n",
    "        id_last = last[0].id\n",
    "        new_tweets = api.GetSearch(term=termo, count=results, max_id=min(list_ids))\n",
    "        for i in range(len(new_tweets)):\n",
    "            list_ids.append(new_tweets[i].id)\n",
    "        search_results.append(new_tweets)\n",
    "    for i in range(len(search_results)):\n",
    "        for j in range(len(search_results[i])):\n",
    "            tweets.append(search_results[i][j])\n",
    "    tweets_txt += [tweet.text.split(u' ') for tweet in tweets]\n",
    "    for i in range(len(tweets)):\n",
    "        tweets_words += [word.lower().strip(u':@&$!?') for word in tweets_txt[i]]\n",
    "    for i in range(len(tweets)): \n",
    "        names += [word.strip(u':@&$!?') for word in tweets_txt[i] if word.istitle() and len(word) > 2]\n",
    "    with open(pathtweetsfile,'w') as out:\n",
    "        for tweet in tweets_txt:\n",
    "            out.write(u'\\n{}'.format(tweet))\n",
    "    return tweets, tweets_txt, tweets_words, names\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(text) / len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_query = u'hillary'\n",
    "\n",
    "search = api.GetSearch(twitter_query)\n",
    "print([s.text for s in search])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our customized function that retrieves 5 x 100 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets, tweets_txt, tweets_words, names = search_for_term(twitter_query,5,10)\n",
    "    \n",
    "print('Word count: {}'.format(len(tweets_words)))\n",
    "print('Repertoire: {}'.format(len(set(tweets_words))))\n",
    "print('Lexical diversity: {}'.format(lexical_diversity(tweets_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(tweets_words)\n",
    "freq_dist.plot(40)\n",
    "freq_dist.plot(40, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print('10 most frequent words')\n",
    "print(freq_dist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sorted list of words')\n",
    "print(sorted(set(tweets_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, without stopwords. See variable \"ignorewords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_tweets_words = [word for word in tweets_words if word not in ignorewords]\n",
    "    \n",
    "freq_new = nltk.FreqDist(new_tweets_words)    \n",
    "freq_new.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_new.plot(50, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('10 most frequent words')\n",
    "print(freq_new.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sorted list of words')\n",
    "print(sorted(set(new_tweets_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(new_tweets_words.count('poll'))\n",
    "print(freq_new['voters']) #same as before\n",
    "print(freq_new.freq('virginia')) #relative to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating small words or words with specific sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigger_tweets_words = [word for word in new_tweets_words if len(word) > 2]\n",
    "#mediumsized_tweets_words = [word for word in new_tweets_words if len(word) > 2 and len(word) < 9]\n",
    "freq_bigger = nltk.FreqDist(bigger_tweets_words)    \n",
    "freq_bigger.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citacoes = [word for word in tweets_words if '@' in word]\n",
    "#citacoes = [word for word in tweets_words if word.startswith('@')]\n",
    "freq_citacoes = nltk.FreqDist(citacoes)\n",
    "freq_citacoes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_citacoes.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags = [word for word in tweets_words if word.startswith('#')]\n",
    "freq_hashtags = nltk.FreqDist(hashtags)\n",
    "freq_hashtags.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_hashtags.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Frequent words  \n",
    "Can be used with any of the previous lists'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequent_words = [word.lower() for word in new_tweets_words if tweets_words.count(word) > 5]\n",
    "freq_dist2 = nltk.FreqDist(frequent_words)\n",
    "freq_dist2.plot(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_size_words = nltk.FreqDist([len(w) for w in new_tweets_words])\n",
    "freq_size_words.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_size_words.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigramas_tweets = nltk.bigrams(new_tweets_words)\n",
    "freqbig = nltk.FreqDist(bigramas_tweets)\n",
    "freqbig.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names (capitalized words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_names = nltk.FreqDist(names)\n",
    "freq_names.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badwords =   ['abandoned','abused','accused','addicted','afraid','aggravated',\n",
    "              'aggressive','alone','angry','anguish','annoyed','anxious','apprehensive',\n",
    "              'argumentative','artificial','ashamed','assaulted','at a loss','at risk',\n",
    "              'atrocious','attacked','avoided','awful','awkward','bad','badgered','baffled',\n",
    "              'banned','barren','beat','beaten down','belittled','berated','betrayed',\n",
    "              'bitched at','bitter','bizzare','blacklisted','blackmailed','blamed','bleak',\n",
    "              'blown away','blur','bored','boring','bossed-around','bothered','bothersome',\n",
    "              'bounded','boxed-in','broken','bruised','brushed-off','bugged','bullied',\n",
    "              'bummed','bummed out','burdened','burdensome','burned','burned-out',\n",
    "              'caged in','careless','chaotic','chased','cheated','cheated on','chicken',\n",
    "              'claustrophobic','clingy','closed','clueless','clumsy','coaxed',\n",
    "              'codependent','coerced','cold','cold-hearted','combative','commanded',\n",
    "              'compared','competitive','compulsive','conceited','concerned',\n",
    "              'condescended to','confined','conflicted','confronted','confused',\n",
    "              'conned','consumed','contemplative','contempt','contentious','controlled',\n",
    "              'convicted','cornered','corralled','cowardly','crabby','cramped','cranky',\n",
    "              'crap','crappy','crazy','creeped out','creepy','critical','criticized',\n",
    "              'cross','crowded','cruddy','crummy','crushed','cut-down','cut-off','cynical',\n",
    "              'damaged','damned','dangerous','dark','dazed','dead','deceived','deep',\n",
    "              'defamed','defeated','defective','defenseless','defensive','defiant',\n",
    "              'deficient','deflated','degraded','dehumanized','dejected','delicate',\n",
    "              'deluded','demanding','demeaned','demented','demoralized','demotivated',\n",
    "              'dependent','depleted','depraved','depressed','deprived','deserted',\n",
    "              'deserving of pain/punishment','desolate','despair','despairing',\n",
    "              'desperate','despicable','despised','destroyed','destructive',\n",
    "              'detached','detest','detestable','detested','devalued','devastated',\n",
    "              'deviant','devoid','diagnosed','dictated to','different','difficult',\n",
    "              'directionless','dirty','disabled','disagreeable','disappointed',\n",
    "              'disappointing','disapproved of','disbelieved','discardable','discarded',\n",
    "              'disconnected','discontent','discouraged','discriminated','disdain',\n",
    "              'disdainful','disempowered','disenchanted','disgraced','disgruntled',\n",
    "              'disgust','disgusted','disheartened','dishonest','dishonorable',\n",
    "              'disillusioned','dislike','disliked','dismal','dismayed','disorganized',\n",
    "              'disoriented','disowned','displeased','disposable','disregarded',\n",
    "              'disrespected','dissatisfied','distant','distracted','distraught',\n",
    "              'distressed','disturbed','dizzy','dominated','doomed','double-crossed',\n",
    "              'doubted','doubtful','down','down and out','down in the dumps',\n",
    "              'downhearted','downtrodden','drained','dramatic','dread','dreadful',\n",
    "              'dreary','dropped','drunk','dry','dumb','dumped','dumped on','duped',\n",
    "              'edgy','egocentric','egotistic','egotistical','elusive','emancipated',\n",
    "              'emasculated','embarrassed','emotional','emotionless','emotionally bankrupt',\n",
    "              'empty','encumbered','endangered','enraged','enslaved','entangled','evaded',\n",
    "              'evasive','evicted','excessive','excluded','exhausted','exploited','exposed',\n",
    "              'fail','failful','fake','false','fear','fearful','fed up','flawed','forced',\n",
    "              'forgetful','forgettable','forgotten','fragile','freaked out','frightened',\n",
    "              'frigid','frustrated','furious','gloomy','glum','gothic','grey','grief','grim',\n",
    "              'gross','grossed-out','grotesque','grouchy','grounded','grumpy','guilt-tripped',\n",
    "              'guilty','harassed','hard','hard-hearted','harmed','hassled','hate','hateful',\n",
    "              'hatred','haunted','heartbroken','heartless','heavy-hearted','helpless',\n",
    "              'hesitant','hideous','hindered','hopeless','horrible','horrified','horror',\n",
    "              'hostile','hot-tempered','humiliated','hung up','hung over','hurried','hurt',\n",
    "              'hysterical','idiot','idiotic','ignorant','ignored','ill','ill-tempered',\n",
    "              'imbalanced','imposed-upon','impotent','imprisoned','impulsive','in the dumps',\n",
    "              'in the way','inactive','inadequate','incapable','incommunicative','incompetent',\n",
    "              'incompatible','incomplete','incorrect','indecisive','indifferent',\n",
    "              'indoctrinated','inebriated','ineffective','inefficient','inferior',\n",
    "              'infuriated','inhibited','inhumane','injured','injusticed','insane',\n",
    "              'insecure','insignificant','insincere','insufficient','insulted',\n",
    "              'intense','interrogated','interrupted','intimidated','intoxicated',\n",
    "              'invalidated','invisible','irrational','irritable','irritated',\n",
    "              'isolated','jaded','jealous','jerked around','joyless','judged',\n",
    "              'kept apart','kept away','kept in','kept out','kept quiet','labeled',\n",
    "              'laughable','laughed at','lazy','leaned on','lectured to','left out',\n",
    "              'let down','lied about','lied to','limited','little','lonely','lonesome',\n",
    "              'longing','lost','lousy','loveless','low','mad','made fun of','man handled',\n",
    "              'manipulated','masochistic','messed with','messed up','messy','miffed',\n",
    "              'miserable','misled','mistaken','mistreated','mistrusted','misunderstood',\n",
    "              'mixed-up','mocked','molested','moody','nagged','needy','negative',\n",
    "              'nervous','neurotic','nonconforming','numb','nuts','nutty','objectified',\n",
    "              'obligated','obsessed','obsessive','obstructed','odd','offended',\n",
    "              'on display','opposed','oppressed','out of place','out of touch',\n",
    "              'over-controlled','over-protected','overwhelmed','pain','panic','paranoid',\n",
    "              'passive','pathetic','pessimistic','petrified','phony','picked on','pissed',\n",
    "              'pissed off','plain','played with','pooped','poor','powerless','pre-judged',\n",
    "              'preached to','preoccupied','predjudiced','pressured','prosecuted',\n",
    "              'provoked','psychopathic','psychotic','pulled apart','pulled back',\n",
    "              'punished','pushed','pushed away','put down','puzzled','quarrelsome',\n",
    "              'queer','questioned','quiet','rage','raped','rattled','regret','rejected',\n",
    "              'resented','resentful','responsible','retarded','revengeful','ridiculed',\n",
    "              'ridiculous','robbed','rotten','sad','sadistic','sarcastic','scared',\n",
    "              'scarred','screwed','screwed over','screwed up','self-centered','self-conscious',\n",
    "              'self-destructive','self-hatred','selfish','sensitive','shouted at','shy',\n",
    "              'singled-out','slow','small','smothered','snapped at','spiteful','stereotyped',\n",
    "              'strange','stressed','stretched','stuck','stupid','submissive','suffering',\n",
    "              'suffocated','suicidal','superficial','suppressed','suspicious','worse','worst'\n",
    "              ,'bankrupcy','jobs','shit','#sob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodwords =  ['Abundant','Accomplished','Achieving','Active','Admirable','Adorable',\n",
    "              'Adventurous','Admired','Affluent','Agreeable','Alert','Aligned','Alive',\n",
    "              'Amazing','Appealing','Appreciate','Artistic','Astounding','Astute',\n",
    "              'Attentive','Attractive','Auspicious','Authentic','Awake','Aware','Awesome',\n",
    "              'Beaming','Beautiful','Better','Best','Blessed','Bliss','Bold','Bright','Brilliant',\n",
    "              'Brisk','Buoyant','Calm','Capable','Centered','Certain','Charming',\n",
    "              'Cheerful','Clear','Clever','Competent','Complete','Confident','Connected',\n",
    "              'Conscious','Considerate','Convenient','Courageous','Creative','Daring',\n",
    "              'Dazzling','Delicious','Delightful','Desirable','Determined','Diligent',\n",
    "              'Discerning','Discover','Dynamic','Eager','Easy','Efficient','Effortless',\n",
    "              'Elegant','Eloquent','Energetic','Endless','Enhancing','Engaging','Enormous'\n",
    "              ,'Enterprising','Enthusiastic','Enticing','Excellent','Exceptional','Exciting'\n",
    "              ,'Experienced','Exquisite','Fabulous','Fair','Far-Sighted','Fascinating',\n",
    "              'Fine','Flattering','Flourishing','Fortunate','Free','Friendly','Fulfilled',\n",
    "              'Fun','Generous','Genuine','Gifted','Glorious','Glowing','Good','Good-Looking',\n",
    "              'Gorgeous','Graceful','Gracious','Grand','Great','Handsome','Happy','Hardy',\n",
    "              'Harmonious','Healed','Healthy','Helpful','Honest','Humorous','Ideal',\n",
    "              'Imaginative','Impressive','Industrious','Ingenious','Innovative','Inspired',\n",
    "              'Intelligent','Interested','Interesting','Intuitive','Inventive','Invincible',\n",
    "              'Inviting','Irresistible','Joyous','Judicious','Keen','Kind','Knowing','Leader',\n",
    "              'Limitless','Lively','Loving','Lucky','Luminous','Magical','Magnificent',\n",
    "              'Marvellous','Masterful','Mighty','Miraculous','Motivated','Natural','Neat',\n",
    "              'Nice','Nurturing','Noble','Optimistic','Outstanding','Passionate','Peaceful',\n",
    "              'Perfect','Persevering','Persistent','Playful','Pleasing','Plentiful','Positive',\n",
    "              'Powerful','Precious','Prepared','Productive','Profound','Prompt','Prosperous',\n",
    "              'Proud','Qualified','Quick','Radiant','Reasonable','Refined','Refreshing',\n",
    "              'Relaxing','Reliable','Remarkable','Resolute','Resourceful','Respected',\n",
    "              'Rewarding','Robust','Safe','Satisfied','Secure','Seductive','Self-Reliant',\n",
    "              'Sensational','Sensible','Sensitive','Serene','Sharing','Skilful','Smart',\n",
    "              'Smashing','Smooth','Sparkling','Spiritual','Splendid','Strong','Stunning',\n",
    "              'Successful','Superb','Swift','Talented','Tenacious','Terrific','Thankful',\n",
    "              'Thrilling','Thriving','Timely','Trusting','Truthful','Ultimate','Unique',\n",
    "              'Valiant','Valuable','Versatile','Vibrant','Victorious','Vigorous','Vivacious',\n",
    "              'Vivid','Warm','Wealthy','Well','Whole','Wise','Wonderful','Worthy','Young',\n",
    "              'Youthful','Zeal','Zest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def percentage(count, total):\n",
    "    return 100 * count / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(texto, goodwords, badwords):\n",
    "    '''\n",
    "    Not a sophisticated one, but the main idea is present.\n",
    "    Please read: http://alias-i.com/lingpipe/demos/tutorial/sentiment/read-me.html\n",
    "    '''\n",
    "    goodness = 0\n",
    "    badness = 0    \n",
    "    for word in goodwords:\n",
    "        goodness += percentage(texto.count(word.lower()), len(texto))\n",
    "    for word in badwords:\n",
    "        badness += percentage(texto.count(word.lower()), len(texto))\n",
    "    if badness:\n",
    "        ratio = goodness/float(badness)\n",
    "    print(u'Grau de negatividade: {}'.format(badness))\n",
    "    print(u'Grau de positividade: {}'.format(goodness))\n",
    "    if badness:\n",
    "        print(u'Razão: {}'.format(ratio))\n",
    "    return goodness, badness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_analysis(tweets_words, goodwords, badwords);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.python.org/pypi/basemap/1.0.7  \n",
    "http://matplotlib.org/basemap/  \n",
    "http://matplotlib.org/basemap/users/installing.html  \n",
    "http://nbviewer.ipython.org/github/ehmatthes/intro_programming/blob/master/notebooks/visualization_earthquakes.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msgpublicas = api.GetStreamSample()\n",
    "geo_points = 0\n",
    "lats, lons = [], []\n",
    "users = []\n",
    "while geo_points < 20:\n",
    "    tweet = next(msgpublicas)\n",
    "    if 'coordinates' in tweet.keys() and tweet['coordinates'] != None:\n",
    "        coords = tweet['coordinates']['coordinates']\n",
    "        user = tweet['user']['id']\n",
    "        print(u'Usuário {} nas coordenadas {}'.format(user, [coords[1],coords[0]]))\n",
    "        lons.append(float(coords[0]))\n",
    "        lats.append(float(coords[1]))\n",
    "        users.append(user)\n",
    "        geo_points +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map = Basemap(projection='robin', resolution = 'l', area_thresh = 1000.0, lat_0=0, lon_0=0)\n",
    "\n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = '#FFFFCC') #http://www.w3schools.com/tags/ref_colorpicker.asp\n",
    "map.drawmapboundary()\n",
    "map.drawmeridians(np.arange(0, 360, 30))\n",
    "map.drawparallels(np.arange(-90, 90, 30))\n",
    "\n",
    "x,y = map(lons, lats)\n",
    "map.plot(x, y, 'ro', markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map = Basemap(projection='merc', lat_0 = -22, lon_0 = -56.5,resolution = 'h', area_thresh = 0.1, \n",
    "              llcrnrlon=-83.0, llcrnrlat=-57.0, urcrnrlon=-30.0, urcrnrlat=13.0)\n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    "\n",
    "x,y = map(lons, lats)\n",
    "map.plot(x, y, 'bo', markersize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom em uma coordenada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmap(lat,lon,zoom=10):\n",
    "    # Google Maps URL template for an iframe\n",
    "    google_maps_url = \"http://maps.google.com/maps?q={0}+{1}&ie=UTF8&t=h&z={2}&{0},{1}&output=embed\".format(lat,lon,zoom)\n",
    "    display(IFrame(google_maps_url, '800px', '600px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmap(lats[0],lons[0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Basemap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-521a5b1ab0ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m m = Basemap(projection='merc', llcrnrlat=south, urcrnrlat=north,\n\u001b[0m\u001b[1;32m      9\u001b[0m             llcrnrlon=west, urcrnrlon=east, lat_ts=south, resolution='i')\n\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muber_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muber_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Basemap' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJDCAYAAADD62EJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faace7e59b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "west, south, east, north = -74.26, 40.50, -73.70, 40.92\n",
    " \n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111)\n",
    " \n",
    "m = Basemap(projection='merc', llcrnrlat=south, urcrnrlat=north,\n",
    "            llcrnrlon=west, urcrnrlon=east, lat_ts=south, resolution='i')\n",
    "x, y = m(uber_data['Lon'].values, uber_data['Lat'].values)\n",
    "m.hexbin(x, y, gridsize=1000,\n",
    "         bins='log', cmap=cm.YlOrRd_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs\n",
    "\n",
    "http://networkx.lanl.gov/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rt_origins(tweet):\n",
    "    ''' Regex adapted from \n",
    "    http://stackoverflow.com/questions/655903/python-regular-expression-for-retweets'''\n",
    "    rt_patterns = re.compile(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", re.IGNORECASE)\n",
    "    rt_origins = []\n",
    "    try:\n",
    "        rt_origins += [mention.strip() for mention in rt_patterns.findall(tweet)[0][1].split()]\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return [rto.strip(\"@\") for rto in rt_origins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_graph_retweets(tweets):\n",
    "    g = nx.DiGraph()\n",
    "    for tweet in tweets:\n",
    "        rt_origins = get_rt_origins(tweet.text)\n",
    "        if not rt_origins:\n",
    "            continue\n",
    "        for rt_origin in rt_origins:\n",
    "            g.add_edge(rt_origin, tweet.user.screen_name, {'tweet_id': tweet.id})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_rt = create_graph_retweets(tweets)\n",
    "print(\"Number of nodes is: {}\\n\".format(g_rt.number_of_nodes()))\n",
    "print(\"Number of edges is: {}\\n\".format(g_rt.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_dic = sorted(g_rt.degree().items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = nx.degree(g_rt)\n",
    "plt.plot(sorted(dic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_dotfile(g):\n",
    "    try:\n",
    "        nx.drawing.write_dot(g, pathdotfile)\n",
    "        print >> sys.stderr, 'Graph exported for file: {}'.format(pathdotfile)\n",
    "    except (ImportError, UnicodeEncodeError, AttributeError): \n",
    "        # Este bloco serve para usuarios de windows, que certamente terao problemas\n",
    "        # com o metodo nx.drawing.write_dot. Tambem serve para os casos em que temos\n",
    "        # problemas com o unicode\n",
    "        dot = [u'\"{}\" -> \"{}\" [tweet_id={}]'.format(n1, n2, g[n1][n2]['tweet_id']) for (n1, n2) in g.edges()]\n",
    "        f = codecs.open(pathdotfile, 'w', encoding='utf-8')\n",
    "        f.write('''strict digraph {{}}'''.format(';\\n'.join(dot), ))\n",
    "        f.close()\n",
    "        print(sys.stderr, 'Graph exported for file: {}'.format(pathdotfile))\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_dotfile(g_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a dotfile\n",
    "\n",
    "Obs: To generate a png graph from the dotfile, type in the Unix Prompt: \n",
    "'circo -Tpng -Gcharset=latin1 -Ograph_retweet graph_retweet.dot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Chamando um programa externo ao Ipython com o operador !\n",
    "!dot -Tpng ../outputs/graph_retweet.dot -o ../outputs/graph_retweet.png\n",
    "Image(pathpngfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx_labels(g_rt,pos=nx.spring_layout(g_rt),font_size=9)\n",
    "nx.draw(g_rt)\n",
    "#nx.draw_random(g_rt)\n",
    "#nx.draw_circular(g_rt)\n",
    "#nx.draw_spectral(g_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a HTML file that uses javascript for visualizing the graph (needs a template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_protovis_file(g):\n",
    "    '''A visualization alternative is \"protovis\" javascript\n",
    "    It uses the files \"template_protoviz.html and \"protovis-r3.2.js\"\n",
    "    '''\n",
    "    nodes = g.nodes()\n",
    "    indexed_nodes = {}\n",
    "    idx = 0\n",
    "    for n in nodes:\n",
    "        indexed_nodes.update([(n, idx,)])\n",
    "        idx += 1\n",
    "    links = []\n",
    "    for n1, n2 in g.edges():\n",
    "        links.append({'source': indexed_nodes[n2],'target': indexed_nodes[n1]})\n",
    "    json_data = json.dumps({\"nodes\" : [{\"nodeName\" : n} for n in nodes], \"links\" : links}, indent=4)\n",
    "    html = open(pathtemplate).read().format(json_data,)\n",
    "    f = open(pathprotofile, 'w')\n",
    "    f.write(html)\n",
    "    f.close()\n",
    "    print(sys.stderr, 'Graph exported for file: {}'.format(pathprotofile))\n",
    "    return f.name, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = save_protovis_file(g_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!chromium ../outputs/graph_retweet.html\n",
    "# http://docs.python.org/library/webbrowser.html \n",
    "\n",
    "webbrowser.open(pathprotofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the GeoViews package  \n",
    "https://www.continuum.io/blog/developer-blog/introducing-geoviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://github.com/SciTools/Cartopy\n",
    "#https://github.com/ioam/geoviews\n",
    "\n",
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import iris\n",
    "import cartopy\n",
    "\n",
    "from cartopy import crs\n",
    "from cartopy import feature as cf\n",
    "from geoviews import feature as gf\n",
    "\n",
    "hv.notebook_extension('bokeh','matplotlib')\n",
    "%output backend='matplotlib'\n",
    "%opts Feature [projection=crs.Robinson()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0476ac1c61ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode()\n",
    " \n",
    "# state population estimates for July 2015 from US Census Bureau\n",
    "# www.census.gov/popest/data/state/totals/2015/tables/NST-EST2015-01.csv\n",
    "state_population = np.asarray([738432, 4858979, 2978204, 6828065, 39144818, 5456574,\\\n",
    "                               3590886, 672228, 945934, 20271272, 10214860, 1431603,\\\n",
    "                               3123899, 1654930, 12859995, 6619680, 2911641, 4425092,\\\n",
    "                               4670724, 6794422, 6006401, 1329328, 9922576, 5489594,\\\n",
    "                               6083672, 2992333, 1032949, 10042802, 756927, 1896190,\\\n",
    "                               1330608, 8958013, 2085109, 2890845, 19795791, 11613423,\\\n",
    "                               3911338, 4028977, 12802503, 1056298, 4896146, 858469,\\\n",
    "                               6600299, 27469114, 2995919, 8382993, 626042, 7170351,\\\n",
    "                               5771337, 1844128, 586107])\n",
    " \n",
    "# police officer deaths per 100,000 people in state\n",
    "police_percapita = police_perstate / state_population * 100000\n",
    " \n",
    "# District of Columbia outlier (1 law enforcement death per 500 people) adjusted\n",
    "police_percapita[7] = police_percapita[7] / 10\n",
    " \n",
    "# plotly code for choropleth map\n",
    "police_scale = [[0, 'rgb(229, 239, 245)'],[1, 'rgb(1, 97, 156)']]\n",
    " \n",
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        colorscale = police_scale,\n",
    "        autocolorscale = False,\n",
    "        showscale = False,\n",
    "        locations = us_states,\n",
    "        z = police_percapita,\n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255, 255, 255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        ) ]\n",
    " \n",
    "layout = dict(\n",
    "        title = 'Police Officer Deaths per 100,000 People in United States (1791-2016)',\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            projection = dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)',\n",
    "            countrycolor = 'rgb(255, 255, 255)')\n",
    "             )\n",
    " \n",
    "figure = dict(data=data, layout=layout)\n",
    " \n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
