{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Google's Word2Vec for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial  \n",
    "https://github.com/wendykan/DeepLearningMovies  \n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  \n",
    "\n",
    "\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.\n",
    "\n",
    "Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's charge the batteries for our analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K4200 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set  \n",
    "--\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "Data fields\n",
    "\n",
    "id - Unique ID of each review  \n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews  \n",
    "review - Text of the review  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = \"../datasets/Kaggle/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(datapath, 'BOW_labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(os.path.join(datapath, 'BOW_testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(os.path.join(datapath, \"BOW_unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} labeled train reviews, \\\n",
    "{} labeled test reviews, and \\\n",
    "{} unlabeled reviews\\n\".format(train[\"review\"].size,\n",
    "                               test[\"review\"].size,\n",
    "                               unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: For Beginners - Bag of Words\n",
    "--\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words  \n",
    "\n",
    "What is NLP?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. This page will help you get started with loading and cleaning the IMDB movie reviews, then applying a simple Bag of Words model to get surprisingly accurate predictions of whether a review is thumbs-up or thumbs-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text_Cleaning_Utilities(object):\n",
    "    \"\"\"Tools for processing text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_wordlist(text, \n",
    "                         remove_stopwords=False, \n",
    "                         remove_html=False, \n",
    "                         remove_non_letters=False, \n",
    "                         steeming=False):\n",
    "        '''Split a text into a list of words'''\n",
    "        #text = text.replace('-\\n','')\n",
    "        text = text.lower()\n",
    "        if remove_html:\n",
    "            text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "        if remove_non_letters:\n",
    "            text = re.sub(\"[^-A-Za-z0-9_]\", \" \", text)\n",
    "        list_words = word_tokenize(text)\n",
    "        list_words = [w.strip(string.punctuation) for w in list_words if w not in string.punctuation]\n",
    "        list_words = [w for w in list_words if len(w) > 1]\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            list_words = [w for w in list_words if w not in stops]\n",
    "        if steeming:\n",
    "            stemmer = PorterStemmer()\n",
    "            list_words = [stemmer.stem(item) for item in list_words]\n",
    "        return list_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_texts(dataframe, column, \n",
    "                            remove_stopwords=False, \n",
    "                            remove_html=False, \n",
    "                            remove_non_letters=False, \n",
    "                            steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(' '.join(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                                 remove_stopwords=remove_stopwords,\n",
    "                                                                                 remove_html=remove_html,\n",
    "                                                                                 remove_non_letters=remove_non_letters,\n",
    "                                                                                 steeming=steeming)))\n",
    "            \n",
    "        return clean_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_list_of_tokens(dataframe, column, \n",
    "                             remove_stopwords=False, \n",
    "                             remove_html=False, \n",
    "                             remove_non_letters=False, \n",
    "                             steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                        remove_stopwords=remove_stopwords,\n",
    "                                                                        remove_html=remove_html,\n",
    "                                                                        remove_non_letters=remove_non_letters,\n",
    "                                                                        steeming=steeming))\n",
    "            \n",
    "        return clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all the datasets and getting word lists\n",
    "--\n",
    "first set is without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                  'review', \n",
    "                                                                  remove_stopwords=True,\n",
    "                                                                  remove_html=True,)\n",
    "clean_test_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                 'review', \n",
    "                                                                 remove_stopwords=True,\n",
    "                                                                 remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched w'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second set mantains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                     'review',\n",
    "                                                                     remove_html=True,)\n",
    "\n",
    "clean_test_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                    'review',\n",
    "                                                                    remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with all this stuff going down at the moment with mj ve started listening to his'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally in film who main themes are of mortality nostalgia and loss of innocen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f:\n",
    "    pickle.dump((clean_train_reviews, \n",
    "                 clean_test_reviews,\n",
    "                 clean_train_reviews_sw, \n",
    "                 clean_test_reviews_sw),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    (clean_train_reviews, \n",
    "     clean_test_reviews,\n",
    "     clean_train_reviews_sw,\n",
    "     clean_test_reviews_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features from a Bag of Words (Using scikit-learn)\n",
    "--\n",
    "\n",
    "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"  \n",
    "Sentence 2: \"The dog ate the cat and the hat\"  \n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: [ 2, 1, 1, 1, 1, 0, 0, 0 ]\n",
    "\n",
    "Similarly, the features for Sentence 2 are: [ 3, 1, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features.  \n",
    "We will test two strategies: CountVectorizer (term frequecies - TF) and TFIDF Vectorizer:  \n",
    "First we'll start with plain word counts (TF):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer_tf = CountVectorizer(input='content', \n",
    "                               encoding='utf-8', \n",
    "                               decode_error='strict', \n",
    "                               strip_accents=None, \n",
    "                               lowercase=True, \n",
    "                               preprocessor=None, \n",
    "                               tokenizer=None, \n",
    "                               stop_words=None, \n",
    "                               #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                               ngram_range=(1, 2),\n",
    "                               analyzer='word', \n",
    "                               max_df=1.0, \n",
    "                               min_df=1, \n",
    "                               max_features=5000, \n",
    "                               vocabulary=None, \n",
    "                               binary=False, \n",
    "                               dtype=np.int64,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "second, it transforms our training data into feature vectors. \n",
    "The input to fit_transform should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tf = vectorizer_tf.fit_transform(clean_train_reviews)\n",
    "train_data_features_tf = train_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tf = vectorizer_tf.fit_transform(clean_test_reviews)\n",
    "test_data_features_tf = test_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use TfIDf vectors and the train/test cleaned reviews with stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(input='content',\n",
    "                                  #encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents=None,\n",
    "                                  lowercase=True,\n",
    "                                  preprocessor=None,\n",
    "                                  tokenizer=None,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words=None,\n",
    "                                  #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=5000,\n",
    "                                  vocabulary=None, \n",
    "                                  binary=False, \n",
    "                                  dtype=np.int64,\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_train_reviews_sw)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw)\n",
    "test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'wb') as f:\n",
    "    pickle.dump((train_data_features_tf, \n",
    "                 test_data_features_tf,\n",
    "                 train_data_features_tfidf,\n",
    "                 test_data_features_tfidf),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Train set for Cross Validation  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://github.com/zygmuntz/classifying-text/blob/master/bow_validate.py  \n",
    "Alternatively, we can use the indexes to divide the train samples  \n",
    "\n",
    "train_i, test_i = train_test_split(np.arange(len(train)), train_size = 0.8, random_state = 44)  \n",
    "\n",
    "After generating indexes, we can divide ou datasets:  \n",
    "traincv = train_data_features1[train_i]  \n",
    "testcv = train_data_features1[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plain Word Counts\n",
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(train_data_features_tf,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "X_traincv_tfidf, X_testcv_tfidf, y_traincv_tfidf, y_testcv_tfidf = train_test_split(train_data_features_tfidf,\n",
    "                                                                                    train[\"sentiment\"],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training some Classifiers  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! Here, we'll use some classifiers implementations included in  the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_tf = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=0, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8632\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tf_tts = clf_RF_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_RF_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44666667,  0.55333333],\n",
       "       [ 0.38666667,  0.61333333],\n",
       "       [ 0.31333333,  0.68666667],\n",
       "       ..., \n",
       "       [ 0.39333333,  0.60666667],\n",
       "       [ 0.8       ,  0.2       ],\n",
       "       [ 0.22      ,  0.78      ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tf.predict_proba(X_testcv_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train on the TfIdf samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_tfidf = RandomForestClassifier(n_estimators=300, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth=None, \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_features='auto', \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      bootstrap=False, \n",
    "                                      oob_score=False, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=0, \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8536\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tfidf_tts = clf_RF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_RF_tfidf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42333333,  0.57666667],\n",
       "       [ 0.36666667,  0.63333333],\n",
       "       [ 0.33      ,  0.67      ],\n",
       "       ..., \n",
       "       [ 0.25666667,  0.74333333],\n",
       "       [ 0.75333333,  0.24666667],\n",
       "       [ 0.33      ,  0.67      ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tfidf.predict_proba(X_testcv_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=0,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_tts = clf_LR_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_LR_tfidf = LR(penalty='l2',\n",
    "                  dual=False,\n",
    "                  tol=0.0001,\n",
    "                  C=1.0,\n",
    "                  fit_intercept=True,\n",
    "                  intercept_scaling=1,\n",
    "                  class_weight=None,\n",
    "                  random_state=0,\n",
    "                  solver='liblinear',\n",
    "                  max_iter=100,\n",
    "                  multi_class='ovr',\n",
    "                  verbose=0).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tfidf_tts = clf_LR_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_LR_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC_tf = GradientBoostingClassifier(loss='deviance',\n",
    "                                        learning_rate=0.1,\n",
    "                                        n_estimators=100,\n",
    "                                        subsample=1.0,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        max_depth=3,\n",
    "                                        init=None,\n",
    "                                        random_state=0,\n",
    "                                        max_features=None,\n",
    "                                        verbose=0,\n",
    "                                        max_leaf_nodes=None,\n",
    "                                        warm_start=False,\n",
    "                                        presort='auto').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tf_tts = clf_GBC_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_GBC_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_GBC_tfidf = GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=100,\n",
    "                                           subsample=1.0,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=1,\n",
    "                                           min_weight_fraction_leaf=0.0,\n",
    "                                           max_depth=3,\n",
    "                                           init=None,\n",
    "                                           random_state=0,\n",
    "                                           max_features=None,\n",
    "                                           verbose=0,\n",
    "                                           max_leaf_nodes=None,\n",
    "                                           warm_start=False,\n",
    "                                           presort='auto').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tfidf_tts = clf_GBC_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_GBC_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do some voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf_vot_tf = VotingClassifier(estimators=[('rf', clf_RF_tf),\n",
    "                                          ('lr', clf_LR_tf),\n",
    "                                          ('gbc', clf_GBC_tf)], voting='soft').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tf_tts = clf_vot_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_vot_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_vot_tfidf = VotingClassifier(estimators=[('rf', clf_RF_tfidf),\n",
    "                                             ('lr', clf_LR_tfidf),\n",
    "                                             ('gbc', clf_GBC_tfidf)], voting='soft').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8788\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tfidf_tts = clf_vot_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_vot_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'wb') as f:\n",
    "    pickle.dump((clf_RF_tf, eval_RF_tf_tts,\n",
    "                 clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "                 clf_LR_tf, eval_LR_tf_tts,\n",
    "                 clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "                 clf_GBC_tf, eval_GBC_tf_tts,\n",
    "                 clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "                 clf_vot_tf, eval_vot_tf_tts,\n",
    "                 clf_vot_tfidf, eval_vot_tfidf_tts),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the classifiers from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'rb') as f:\n",
    "    (clf_RF_tf, eval_RF_tf_tts,\n",
    "     clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "     clf_LR_tf, eval_LR_tf_tts,\n",
    "     clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "     clf_GBC_tf, eval_GBC_tf_tts,\n",
    "     clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "     clf_vot_tf, eval_vot_tf_tts,\n",
    "     clf_vot_tfidf, eval_vot_tfidf_tts) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAIyCAYAAAAE8jZRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb7a7bc668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_results = {'RandomForest_TF': eval_RF_tf_tts,\n",
    "               'RandomForest_TFIDF': eval_RF_tfidf_tts,\n",
    "               'LogiReg_TF': eval_LR_tf_tts,\n",
    "               'LogiReg_TFIDF': eval_LR_tfidf_tts,\n",
    "               'GradBoost_TF': eval_GBC_tf_tts,\n",
    "               'GradBoost_TFIDF': eval_GBC_tfidf_tts,\n",
    "               'Voting_TF': eval_vot_tf_tts,\n",
    "               'Voting_TFIDF': eval_vot_tfidf_tts,\n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.78,ymax = 0.92)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--\n",
    "\n",
    "All that remains is to run the best classifier on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
    "\n",
    "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "id        25000 non-null object\n",
      "review    25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the logistic regression with tfidf vectors to make sentiment label predictions\n",
    "result = clf_LR_tfidf.predict(test_data_features_tfidf)\n",
    "result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          0\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          0\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Alternative Vectors\n",
    "--\n",
    "\n",
    "In the subsequent sections, we are going to explore alternate ways to codify text into vectors. We are going to explore three techniques, namely Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) and Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the models, the more we have texts, the better. The size of the Corpus is essential for having good results. We don't need labels in order to create the models, so we will use the train examples and also some unlabeled reviews. The list of cleaned sentences will be used for all the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Parsing sentences from test set\n",
      "Parsing sentences with stopwords from training set\n",
      "Parsing sentences with stopwords from unlabeled set\n",
      "Parsing sentences with stopwords from test set\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "labeled_sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                                 'review', \n",
    "                                                                 remove_html=True,\n",
    "                                                                 remove_stopwords=True,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "all_sentences = labeled_sentences + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                 'review', \n",
    "                                                                                 remove_html=True,\n",
    "                                                                                 remove_stopwords=True,)\n",
    "\n",
    "print(\"Parsing sentences from test set\")\n",
    "test_labeled_sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(test,\n",
    "                                                                      'review', \n",
    "                                                                      remove_html=True,\n",
    "                                                                      remove_stopwords=True,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from training set\")\n",
    "labeled_sentences_sw = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                                    'review', \n",
    "                                                                    remove_html=True,\n",
    "                                                                    remove_stopwords=False,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from unlabeled set\")\n",
    "all_sentences_sw = labeled_sentences_sw + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                       'review', \n",
    "                                                                                       remove_html=True,\n",
    "                                                                                       remove_stopwords=False,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from test set\")\n",
    "test_labeled_sentences_sw = Text_Cleaning_Utilities.df_to_list_of_tokens(test,\n",
    "                                                                         'review', \n",
    "                                                                         remove_html=True,\n",
    "                                                                         remove_stopwords=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "75000\n",
      "25000\n",
      "25000\n",
      "75000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(labeled_sentences))\n",
    "print(len(all_sentences))\n",
    "print(len(test_labeled_sentences))\n",
    "print(len(labeled_sentences_sw))\n",
    "print(len(all_sentences_sw))\n",
    "print(len(test_labeled_sentences_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving sentences to a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'wb') as f:\n",
    "    pickle.dump((labeled_sentences,\n",
    "                 all_sentences,\n",
    "                 test_labeled_sentences,\n",
    "                 labeled_sentences_sw,\n",
    "                 all_sentences_sw,\n",
    "                 test_labeled_sentences_sw,), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading sentences from a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'rb') as f:\n",
    "    (labeled_sentences,\n",
    "     all_sentences,\n",
    "     test_labeled_sentences,\n",
    "     labeled_sentences_sw,\n",
    "     all_sentences_sw,\n",
    "     test_labeled_sentences_sw,) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=4, no_above=0.7):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    print('{} Tokens extracted from {} texts'.format(len(dictionary.keys()), dictionary.num_docs))\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    #stopword_ids = [dictionary.token2id[sw] for sw in stopwords if sw in dictionary.token2id]\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    #low_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 4]\n",
    "    #dictionary.filter_tokens(low_freq_ids)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    print('{} Tokens after cleaning'.format(len(dictionary.keys())))\n",
    "    #print('Building corpus...')\n",
    "    #corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    return dictionary #, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compacting and saving the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "256792 Tokens extracted from 75000 texts\n",
      "56378 Tokens after cleaning\n",
      "dictionary done\n",
      "dictionary saved\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords=set(['n\\'t', 'movie'])\n",
    "\n",
    "dictionary = prep_corpus(all_sentences, additional_stopwords)\n",
    "dictionary.compactify()\n",
    "print('dictionary done')\n",
    "\n",
    "dictionary.save(os.path.join(outputs, 'reviews.dict'))\n",
    "print('dictionary saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n"
     ]
    }
   ],
   "source": [
    "#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list\n",
    "#print(dictionary.token2id['n\\'t'])\n",
    "print(dictionary.token2id['like'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.dictionary.Dictionary.load(os.path.join(outputs, 'reviews.dict'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Corpora (tf and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus tf done\n",
      "corpus tfidf done\n"
     ]
    }
   ],
   "source": [
    "corpus_tf = [dictionary.doc2bow(sentence) for sentence in all_sentences]\n",
    "print('corpus tf done')\n",
    "tfidf = models.TfidfModel(corpus_tf)\n",
    "corpus_tfidf = tfidf[corpus_tf]\n",
    "print('corpus tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tf.mm'), corpus_tf)\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tfidf.mm'), corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tf.mm'))\n",
    "corpus_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lsi - TF done\n",
      "corpus lsi - TFIDF done\n"
     ]
    }
   ],
   "source": [
    "lsi_tf = models.LsiModel(corpus_tf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tf = lsi_tf[corpus_tf]\n",
    "print('corpus lsi - TF done')\n",
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tfidf = lsi_tfidf[corpus_tfidf]\n",
    "print('corpus lsi - TFIDF done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf.save(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tf.mm'), corpus_lsi_tf)\n",
    "lsi_tfidf.save(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tfidf.mm'), corpus_lsi_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf = models.LsiModel.load(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpus_lsi_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tf.mm'))\n",
    "lsi_tfidf = models.LsiModel.load(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpus_lsi_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.508*\"film\" + 0.290*\"one\" + 0.218*\"like\" + 0.147*\"would\" + 0.145*\"good\" + 0.137*\"even\" + 0.125*\"time\" + 0.123*\"really\" + 0.119*\"story\" + 0.117*\"see\"'),\n",
       " (1,\n",
       "  '0.833*\"film\" + -0.214*\"one\" + -0.195*\"like\" + -0.113*\"show\" + -0.105*\"movies\" + -0.105*\"good\" + -0.101*\"would\" + -0.093*\"even\" + -0.092*\"really\" + -0.085*\"get\"'),\n",
       " (2,\n",
       "  '0.727*\"one\" + -0.456*\"like\" + -0.179*\"bad\" + -0.164*\"really\" + -0.153*\"good\" + -0.123*\"would\" + -0.095*\"people\" + -0.079*\"think\" + 0.076*\"two\" + -0.071*\"could\"'),\n",
       " (3,\n",
       "  '-0.493*\"one\" + -0.423*\"like\" + 0.331*\"story\" + -0.243*\"bad\" + 0.139*\"show\" + 0.138*\"great\" + 0.137*\"also\" + 0.137*\"life\" + 0.128*\"love\" + 0.119*\"character\"'),\n",
       " (4,\n",
       "  '-0.622*\"good\" + 0.409*\"like\" + -0.310*\"bad\" + -0.249*\"really\" + 0.157*\"would\" + 0.145*\"people\" + 0.130*\"show\" + -0.128*\"acting\" + 0.117*\"life\" + -0.116*\"great\"'),\n",
       " (5,\n",
       "  '0.523*\"like\" + -0.479*\"would\" + -0.233*\"bad\" + 0.226*\"story\" + -0.222*\"even\" + 0.197*\"good\" + -0.195*\"could\" + 0.183*\"great\" + 0.144*\"also\" + -0.100*\"get\"'),\n",
       " (6,\n",
       "  '-0.688*\"show\" + 0.389*\"story\" + -0.179*\"great\" + 0.172*\"even\" + -0.160*\"good\" + 0.148*\"bad\" + -0.147*\"series\" + -0.114*\"really\" + -0.104*\"episode\" + -0.090*\"shows\"'),\n",
       " (7,\n",
       "  '0.677*\"would\" + 0.277*\"story\" + -0.226*\"bad\" + -0.199*\"get\" + 0.186*\"good\" + -0.169*\"time\" + 0.162*\"one\" + 0.157*\"like\" + -0.138*\"people\" + -0.137*\"really\"'),\n",
       " (8,\n",
       "  '-0.509*\"really\" + -0.342*\"people\" + -0.328*\"see\" + 0.271*\"even\" + -0.236*\"story\" + 0.192*\"good\" + -0.142*\"think\" + 0.136*\"time\" + 0.127*\"first\" + 0.125*\"show\"'),\n",
       " (9,\n",
       "  '0.475*\"story\" + 0.386*\"show\" + -0.350*\"good\" + 0.243*\"characters\" + 0.242*\"even\" + -0.223*\"would\" + 0.186*\"bad\" + -0.164*\"see\" + -0.144*\"man\" + -0.115*\"get\"')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.83285263489722861),\n",
       " ('one', -0.21359328692379906),\n",
       " ('like', -0.19497804548851649),\n",
       " ('show', -0.11343692504385321),\n",
       " ('movies', -0.10548937512566389),\n",
       " ('good', -0.10519322166393422),\n",
       " ('would', -0.10066140173394787),\n",
       " ('even', -0.092791912491167125),\n",
       " ('really', -0.091550520124656601),\n",
       " ('get', -0.08473104758631439)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.146*\"film\" + 0.103*\"bad\" + 0.101*\"good\" + 0.101*\"like\" + 0.101*\"really\" + 0.095*\"one\" + 0.094*\"would\" + 0.091*\"story\" + 0.089*\"see\" + 0.088*\"even\"'),\n",
       " (1,\n",
       "  '-0.336*\"bad\" + -0.199*\"worst\" + -0.132*\"waste\" + -0.129*\"movies\" + -0.129*\"acting\" + -0.114*\"ever\" + -0.114*\"terrible\" + -0.106*\"horror\" + -0.105*\"horrible\" + -0.102*\"stupid\"'),\n",
       " (2,\n",
       "  '-0.673*\"show\" + -0.210*\"episode\" + -0.208*\"series\" + 0.171*\"film\" + -0.149*\"episodes\" + -0.149*\"tv\" + -0.141*\"season\" + 0.136*\"horror\" + -0.125*\"funny\" + -0.119*\"shows\"'),\n",
       " (3,\n",
       "  '0.241*\"show\" + -0.217*\"book\" + -0.212*\"great\" + 0.193*\"horror\" + -0.120*\"movies\" + -0.114*\"love\" + -0.113*\"read\" + -0.111*\"seen\" + 0.107*\"killer\" + 0.106*\"episode\"'),\n",
       " (4,\n",
       "  '0.246*\"horror\" + 0.199*\"series\" + 0.171*\"action\" + 0.156*\"great\" + -0.155*\"people\" + -0.151*\"life\" + 0.127*\"effects\" + 0.126*\"good\" + 0.123*\"show\" + 0.121*\"original\"'),\n",
       " (5,\n",
       "  '-0.371*\"book\" + 0.352*\"funny\" + 0.253*\"comedy\" + -0.203*\"series\" + -0.150*\"read\" + -0.126*\"show\" + 0.121*\"jokes\" + 0.120*\"laugh\" + 0.119*\"fun\" + -0.107*\"episode\"'),\n",
       " (6,\n",
       "  '-0.390*\"horror\" + 0.239*\"bad\" + 0.158*\"worst\" + -0.132*\"scary\" + 0.125*\"show\" + 0.115*\"script\" + -0.114*\"gore\" + 0.108*\"comedy\" + -0.106*\"saw\" + -0.103*\"kids\"'),\n",
       " (7,\n",
       "  '-0.273*\"book\" + 0.254*\"ever\" + 0.228*\"worst\" + 0.193*\"seen\" + -0.179*\"characters\" + 0.148*\"horror\" + -0.143*\"really\" + 0.136*\"dvd\" + -0.133*\"character\" + 0.131*\"years\"'),\n",
       " (8,\n",
       "  '0.506*\"book\" + -0.195*\"film\" + 0.161*\"read\" + 0.140*\"version\" + -0.134*\"show\" + -0.119*\"people\" + -0.116*\"films\" + 0.111*\"bad\" + 0.106*\"original\" + -0.100*\"characters\"'),\n",
       " (9,\n",
       "  '0.351*\"horror\" + -0.326*\"action\" + -0.279*\"game\" + 0.237*\"book\" + 0.186*\"funny\" + 0.176*\"comedy\" + 0.168*\"show\" + 0.130*\"film\" + -0.120*\"series\" + -0.118*\"war\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -0.33603985320557872),\n",
       " ('worst', -0.19902989206791061),\n",
       " ('waste', -0.13155834102620009),\n",
       " ('movies', -0.12928223184148996),\n",
       " ('acting', -0.12899180845059638),\n",
       " ('ever', -0.11426064842612935),\n",
       " ('terrible', -0.1135877765969317),\n",
       " ('horror', -0.10635397252554785),\n",
       " ('horrible', -0.10520595236251849),\n",
       " ('stupid', -0.1021748272445878)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lda tf done\n",
      "corpus lda tfidf done\n"
     ]
    }
   ],
   "source": [
    "lda_tf = models.LdaModel(corpus_tf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf = lda_tf[corpus_tf]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "lda_tfidf = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tfidf = lda_tfidf[corpus_tfidf]\n",
    "print('corpus lda tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf.save(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf.mm'), corpus_lda_tf)\n",
    "\n",
    "lda_tfidf.save(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tfidf.mm'), corpus_lda_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf.mm'))\n",
    "\n",
    "lda_tfidf = models.LdaModel.load(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpus_lda_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*love + 0.007*one + 0.005*young + 0.005*film + 0.005*play + 0.005*beautiful + 0.004*version'),\n",
       " (1,\n",
       "  '0.015*funny + 0.013*comedy + 0.007*one + 0.006*like + 0.005*great + 0.005*get + 0.004*humor'),\n",
       " (2,\n",
       "  '0.011*action + 0.007*one + 0.006*game + 0.005*like + 0.004*fight + 0.004*effects + 0.004*get'),\n",
       " (3,\n",
       "  '0.008*man + 0.007*story + 0.007*one + 0.006*young + 0.006*life + 0.005*woman + 0.005*family'),\n",
       " (4,\n",
       "  '0.027*show + 0.014*series + 0.011*tv + 0.010*years + 0.009*first + 0.009*one + 0.009*time'),\n",
       " (5,\n",
       "  '0.014*bad + 0.013*like + 0.012*one + 0.011*film + 0.010*even + 0.007*horror + 0.007*would'),\n",
       " (6,\n",
       "  '0.045*film + 0.010*films + 0.006*director + 0.006*characters + 0.005*much + 0.004*scenes + 0.004*one'),\n",
       " (7,\n",
       "  '0.022*film + 0.014*good + 0.014*like + 0.014*one + 0.012*really + 0.011*story + 0.010*would'),\n",
       " (8,\n",
       "  '0.010*one + 0.009*film + 0.008*john + 0.008*good + 0.008*great + 0.008*role + 0.007*cast'),\n",
       " (9,\n",
       "  '0.015*film + 0.010*people + 0.008*world + 0.007*war + 0.007*us + 0.006*life + 0.006*would')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.print_topics(num_topics=10, num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.014318590436805199),\n",
       " ('like', 0.012574973145354574),\n",
       " ('one', 0.011741788472266311),\n",
       " ('film', 0.011317820530890475),\n",
       " ('even', 0.010413566757713218),\n",
       " ('horror', 0.0067832591787139514),\n",
       " ('would', 0.006736585738650192),\n",
       " ('could', 0.0061726688883406832),\n",
       " ('really', 0.0059780142429785924),\n",
       " ('get', 0.0056258237045905225)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*film + 0.002*good + 0.002*like + 0.002*bad + 0.002*one + 0.002*really + 0.002*story'),\n",
       " (1,\n",
       "  '0.002*holmes + 0.002*hitchcock + 0.002*bogart + 0.001*godzilla + 0.001*lugosi + 0.001*karloff + 0.001*trouble'),\n",
       " (2,\n",
       "  '0.006*dunne + 0.004*highlander + 0.003*porky + 0.003*courtroom + 0.003*freddie + 0.003*costner + 0.002*troll'),\n",
       " (3,\n",
       "  '0.004*ninja + 0.004*holocaust + 0.003*troma + 0.002*fu + 0.002*melanie + 0.002*reno + 0.002*danish'),\n",
       " (4,\n",
       "  '0.002*moore + 0.001*trouble + 0.001*cold + 0.001*erotic + 0.001*choices + 0.001*creature + 0.001*drunk'),\n",
       " (5,\n",
       "  '0.003*flynn + 0.002*errol + 0.002*cannibal + 0.002*crow + 0.002*coppola + 0.002*berry + 0.002*lance'),\n",
       " (6,\n",
       "  '0.003*khan + 0.002*visitor + 0.002*costello + 0.002*tripe + 0.002*dont + 0.002*bernie + 0.002*bollywood'),\n",
       " (7,\n",
       "  '0.004*shark + 0.003*chaney + 0.003*jaws + 0.003*skip + 0.002*duvall + 0.002*carradine + 0.002*ringu'),\n",
       " (8,\n",
       "  '0.004*seagal + 0.004*predator + 0.003*belle + 0.003*angus + 0.002*it`s + 0.002*emmanuelle + 0.002*conway'),\n",
       " (9,\n",
       "  '0.003*crawford + 0.003*chaplin + 0.002*todd + 0.002*lucy + 0.002*connery + 0.002*russian + 0.002*lenny')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.print_topics(num_topics=10, num_words=7)\n",
    "#lda_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('film', 0.0029020635686037143),\n",
       "   ('good', 0.0019571212047987193),\n",
       "   ('like', 0.0019242537154389794),\n",
       "   ('bad', 0.001894761218790196),\n",
       "   ('one', 0.0018783515758878798),\n",
       "   ('really', 0.0018450363341177718),\n",
       "   ('story', 0.0017929466471770939),\n",
       "   ('would', 0.0017897442710254968),\n",
       "   ('great', 0.0017539712495374885),\n",
       "   ('see', 0.0017109126425232364),\n",
       "   ('even', 0.0016740684548188793),\n",
       "   ('time', 0.0016432230391687122),\n",
       "   ('movies', 0.0016304796211031596),\n",
       "   ('people', 0.0016190551376186124),\n",
       "   ('could', 0.0015909557347443674),\n",
       "   ('well', 0.0015566853657514967),\n",
       "   ('watch', 0.0015080077725109083),\n",
       "   ('much', 0.0014916167440237077),\n",
       "   ('get', 0.0014761880329683572),\n",
       "   ('show', 0.0014720604715476042)]),\n",
       " (1,\n",
       "  [('holmes', 0.0019172181408110666),\n",
       "   ('hitchcock', 0.0018126541642253051),\n",
       "   ('bogart', 0.0015522813526359649),\n",
       "   ('godzilla', 0.0013640725497848197),\n",
       "   ('lugosi', 0.0013599475419175378),\n",
       "   ('karloff', 0.0013308982225233917),\n",
       "   ('trouble', 0.0012769958500153664),\n",
       "   ('candy', 0.0012660267454408509),\n",
       "   ('wilder', 0.0012280215715461758),\n",
       "   ('woo', 0.0012226687777577593),\n",
       "   ('russell', 0.0011705870746057498),\n",
       "   ('tarantino', 0.0011454351007117872),\n",
       "   ('cable', 0.0011439622039833034),\n",
       "   ('unfunny', 0.0011322648680177877),\n",
       "   ('robot', 0.0011305493979621363),\n",
       "   ('hopkins', 0.0011222460029402862),\n",
       "   ('max', 0.001121200211425597),\n",
       "   ('sarah', 0.0010949147162443303),\n",
       "   ('phantom', 0.0010595507651819736),\n",
       "   ('rip', 0.0010445510494020665)])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.show_topics(formatted=False, num_words=20)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing using PyLDAvis  \n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el242291404429332510727991961190\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el242291404429332510727991961190_data = {\"topic.order\": [8, 6, 4, 7, 10, 3, 2, 5, 9, 1], \"lambda.step\": 0.01, \"mdsDat\": {\"y\": [-0.05890460632161621, 0.021163561495977454, 0.04893316094527364, -0.2598580285535393, -0.12442471889025117, 0.15136891644549214, 0.16945403347240395, 0.08772389385121336, -0.009881567477582981, -0.02557464496737108], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"x\": [-0.125841812016171, -0.16798331687555823, -0.03493298900627693, 0.016218569148509384, -0.10237984504629061, -0.06680465097212233, 0.08061479037237289, -0.06444364947543862, 0.2127709331017748, 0.25278197076920034], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.62358361053141, 17.05305241963407, 10.962966944715946, 10.314990621805627, 9.397863124288044, 6.6480174620629855, 6.546907074912528, 5.86744134663766, 5.818216129939511, 4.766961265472216]}, \"R\": 30, \"token.table\": {\"Term\": [\"1/10\", \"13th\", \"1932\", \"2/10\", \"9/11\", \"93\", \"abbott\", \"abc\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"action\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actors\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actress\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"adam\", \"adam\", \"adam\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"aired\", \"akbar\", \"aladdin\", \"album\", \"ali\", \"alien\", \"aliens\", \"allen\", \"allen\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"altman\", \"amelie\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americans\", \"amitabh\", \"angelina\", \"animated\", \"animated\", \"animation\", \"animation\", \"anime\", \"anime\", \"anne\", \"anne\", \"anthony\", \"anthony\", \"anthony\", \"anthony\", \"ants\", \"apartment\", \"apartment\", \"apartment\", \"architect\", \"arkin\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"arthur\", \"arthur\", \"arts\", \"arts\", \"arts\", \"asks\", \"asks\", \"asks\", \"asks\", \"asks\", \"asks\", \"asoka\", \"atrocious\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"avoid\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"awful\", \"awful\", \"bachchan\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bale\", \"ballet\", \"bambi\", \"band\", \"band\", \"band\", \"band\", \"band\", \"band\", \"banderas\", \"bands\", \"baseball\", \"batman\", \"battle\", \"battle\", \"battle\", \"beatles\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"begins\", \"begins\", \"begins\", \"begins\", \"begins\", \"begins\", \"bela\", \"beliefs\", \"belle\", \"ben\", \"ben\", \"ben\", \"ben\", \"bernie\", \"berry\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bette\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"billy\", \"billy\", \"billy\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blah\", \"blood\", \"blood\", \"blood\", \"blood\", \"bloom\", \"bob\", \"bob\", \"bob\", \"bob\", \"bollywood\", \"book\", \"book\", \"book\", \"book\", \"book\", \"boring\", \"boring\", \"boulevard\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"broadway\", \"broadway\", \"brody\", \"bronson\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brothers\", \"brothers\", \"brothers\", \"brothers\", \"brothers\", \"brothers\", \"brothers\", \"brothers\", \"budget\", \"budget\", \"budget\", \"budget\", \"bullets\", \"burton\", \"buscemi\", \"buster\", \"cabaret\", \"cagney\", \"caine\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"canceled\", \"candyman\", \"cannibal\", \"capitalist\", \"captain\", \"captain\", \"car\", \"car\", \"car\", \"car\", \"carrey\", \"cartoon\", \"cartoon\", \"cartoons\", \"cary\", \"cassavetes\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cattle\", \"chainsaw\", \"chan\", \"chaney\", \"channel\", \"channel\", \"channel\", \"channel\", \"chaplin\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"charlie\", \"charlotte\", \"che\", \"cheap\", \"cheap\", \"cheap\", \"cheap\", \"child\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"children\", \"children\", \"cinderella\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinematic\", \"cinematic\", \"cinematic\", \"cinematic\", \"cinematic\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"civil\", \"clerks\", \"clint\", \"coach\", \"coach\", \"columbo\", \"comedian\", \"comedians\", \"comedic\", \"comedic\", \"comedic\", \"comedies\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"communist\", \"computer\", \"computer\", \"computer\", \"computer\", \"conditions\", \"connery\", \"contestants\", \"cop\", \"cop\", \"cop\", \"cop\", \"corman\", \"costello\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"countries\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"crap\", \"crap\", \"crawford\", \"crew\", \"crew\", \"crew\", \"crew\", \"crew\", \"crew\", \"crime\", \"crime\", \"crime\", \"crime\", \"crocodile\", \"crouching\", \"cube\", \"cultural\", \"culture\", \"culture\", \"cultures\", \"daffy\", \"dance\", \"dance\", \"dance\", \"dance\", \"dance\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"davis\", \"davis\", \"davis\", \"de\", \"de\", \"de\", \"de\", \"de\", \"dean\", \"dean\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"death\", \"decides\", \"decides\", \"decides\", \"decides\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delpy\", \"dennis\", \"dennis\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"despite\", \"detective\", \"detective\", \"dialogue\", \"dialogue\", \"dialogue\", \"dialogue\", \"diana\", \"diana\", \"dinosaurs\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"disappointed\", \"disappointed\", \"disappointed\", \"disappointed\", \"disney\", \"documentaries\", \"documentary\", \"documentary\", \"dodge\", \"donald\", \"donald\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"done\", \"donnie\", \"doris\", \"douglas\", \"douglas\", \"douglas\", \"dragon\", \"dunne\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"economic\", \"eddie\", \"eddie\", \"eddie\", \"eddie\", \"educated\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"emily\", \"emily\", \"emily\", \"emmanuelle\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"enid\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"episode\", \"episodes\", \"errol\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"eventually\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"facts\", \"facts\", \"falcon\", \"family\", \"family\", \"family\", \"family\", \"family\", \"farrah\", \"farrow\", \"father\", \"father\", \"father\", \"father\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"fellini\", \"felt\", \"felt\", \"felt\", \"felt\", \"felt\", \"fight\", \"fight\", \"fight\", \"fight\", \"fight\", \"fighting\", \"fighting\", \"fighting\", \"fighting\", \"fights\", \"fights\", \"fights\", \"filipino\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film.it\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finds\", \"finds\", \"finds\", \"finds\", \"finds\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flee\", \"flying\", \"flying\", \"flying\", \"flying\", \"flying\", \"flynn\", \"fonda\", \"football\", \"football\", \"ford\", \"ford\", \"fort\", \"franco\", \"freddy\", \"freedom\", \"freedom\", \"freeman\", \"french\", \"french\", \"french\", \"french\", \"frogs\", \"fry\", \"fu\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funniest\", \"funniest\", \"funniest\", \"funny\", \"funny\", \"funny\", \"funny\", \"gable\", \"gags\", \"game\", \"game\", \"game\", \"game\", \"garbage\", \"garbage\", \"gay\", \"gay\", \"gay\", \"genre\", \"genre\", \"genre\", \"genre\", \"genre\", \"gere\", \"german\", \"german\", \"german\", \"german\", \"german\", \"germans\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"giallo\", \"giant\", \"giant\", \"giant\", \"giant\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"godzilla\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gore\", \"gory\", \"gory\", \"gory\", \"government\", \"government\", \"granny\", \"grant\", \"grant\", \"grant\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gun\", \"gun\", \"gun\", \"guns\", \"guns\", \"guns\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"guys\", \"guys\", \"guys\", \"guys\", \"guys\", \"hai\", \"harding\", \"harry\", \"harry\", \"hayward\", \"heist\", \"helicopter\", \"hellraiser\", \"henry\", \"henry\", \"henry\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"hero\", \"highlander\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"historical\", \"historical\", \"historically\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hitler\", \"holmes\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hooper\", \"hopkins\", \"horrible\", \"horrible\", \"horrid\", \"horror\", \"horror\", \"hospital\", \"hospital\", \"hospital\", \"hospital\", \"house\", \"house\", \"house\", \"house\", \"house\", \"housing\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hudson\", \"human\", \"human\", \"human\", \"human\", \"human\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humphrey\", \"husband\", \"husband\", \"husband\", \"imagery\", \"imagery\", \"imagery\", \"imagery\", \"imo\", \"indigo\", \"iraq\", \"irene\", \"island\", \"island\", \"island\", \"island\", \"island\", \"island\", \"island\", \"isolation\", \"it.i\", \"jack\", \"jack\", \"jack\", \"jack\", \"james\", \"james\", \"james\", \"james\", \"james\", \"james\", \"james\", \"jarring\", \"jarring\", \"jeanne\", \"jet\", \"jews\", \"jim\", \"jim\", \"jim\", \"jim\", \"joan\", \"joan\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"jokes\", \"jokes\", \"jokes\", \"jokes\", \"jolie\", \"julie\", \"julie\", \"kapoor\", \"karloff\", \"keaton\", \"khan\", \"kidman\", \"kids\", \"kids\", \"kids\", \"kids\", \"king\", \"king\", \"king\", \"king\", \"king\", \"king\", \"kinski\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kong\", \"kung\", \"kurosawa\", \"lab\", \"ladd\", \"lame\", \"lame\", \"lame\", \"lame\", \"lancaster\", \"largely\", \"largely\", \"laugh\", \"laugh\", \"laugh\", \"laughable\", \"laughable\", \"laughable\", \"laughs\", \"laughs\", \"laughs\", \"laughton\", \"lenny\", \"leone\", \"leopard\", \"leung\", \"lewton\", \"li\", \"liam\", \"liberal\", \"liberal\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"liked\", \"liked\", \"liked\", \"liked\", \"liked\", \"lime\", \"lithgow\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"liv\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lon\", \"looney\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lou\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loy\", \"lugosi\", \"lynch\", \"lynch\", \"madame\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"maria\", \"maria\", \"marie\", \"maris\", \"marple\", \"martial\", \"mary\", \"mary\", \"mary\", \"mary\", \"massacre\", \"massacre\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"media\", \"media\", \"meg\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"meryl\", \"mesmerizing\", \"message\", \"message\", \"message\", \"message\", \"message\", \"mgm\", \"mgm\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"michael\", \"midler\", \"miike\", \"milla\", \"mimi\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"mitchell\", \"mitchell\", \"mitchum\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"money\", \"monster\", \"monster\", \"mother\", \"mother\", \"mother\", \"mother\", \"mother\", \"movie.i\", \"movie.i\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"mrs\", \"mrs\", \"mrs\", \"ms\", \"ms\", \"ms\", \"ms\", \"mst3k\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"murder\", \"murder\", \"murder\", \"murphy\", \"music\", \"music\", \"music\", \"music\", \"music\", \"musical\", \"musical\", \"musical\", \"muslim\", \"narrative\", \"narrative\", \"narrative\", \"nation\", \"nazi\", \"nazis\", \"nbc\", \"network\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newton\", \"nikita\", \"nikki\", \"ninja\", \"niro\", \"noir\", \"noir\", \"norma\", \"norris\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"novel\", \"novel\", \"novel\", \"novel\", \"nudity\", \"nudity\", \"nudity\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"often\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"oliveira\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"opera\", \"opera\", \"opera\", \"opera\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"orson\", \"pacing\", \"pacing\", \"pacing\", \"pacino\", \"pammy\", \"panther\", \"paris\", \"paris\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"pathetic\", \"pathetic\", \"pathetic\", \"pathetic\", \"pathetic\", \"patients\", \"patients\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performances\", \"performances\", \"performances\", \"performances\", \"performances\", \"performances\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peterson\", \"pianist\", \"plane\", \"plane\", \"planet\", \"planet\", \"planet\", \"planet\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"pointless\", \"pointless\", \"pointless\", \"pokemon\", \"political\", \"politics\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"poorly\", \"poorly\", \"poorly\", \"porky\", \"porn\", \"porn\", \"porn\", \"powell\", \"powell\", \"predator\", \"prison\", \"prison\", \"prison\", \"prison\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"programme\", \"propaganda\", \"prostitute\", \"prostitute\", \"prostitute\", \"pryor\", \"puzzles\", \"quaid\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"rachel\", \"racism\", \"randolph\", \"rani\", \"rathbone\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rats\", \"read\", \"read\", \"read\", \"read\", \"read\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"refuses\", \"refuses\", \"refuses\", \"reilly\", \"religion\", \"religious\", \"religious\", \"religious\", \"remember\", \"remember\", \"remember\", \"remember\", \"remember\", \"remember\", \"remember\", \"reruns\", \"reviews\", \"reviews\", \"reviews\", \"richard\", \"richard\", \"richard\", \"richard\", \"richard\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ridiculous\", \"ringu\", \"robert\", \"robert\", \"robert\", \"robert\", \"robert\", \"robot\", \"robot\", \"robots\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"rukh\", \"sadako\", \"saif\", \"saloon\", \"sandler\", \"satan\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scares\", \"scary\", \"scary\", \"scary\", \"scary\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"sci-fi\", \"sci-fi\", \"sci-fi\", \"science\", \"science\", \"science\", \"science\", \"scientist\", \"scientist\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scott\", \"scott\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"script\", \"script\", \"script\", \"script\", \"script\", \"seagal\", \"season\", \"seasons\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seemed\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"sees\", \"sees\", \"sees\", \"sees\", \"sees\", \"sees\", \"sees\", \"segal\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"shah\", \"shark\", \"sharks\", \"sharpe\", \"shearer\", \"sherlock\", \"shields\", \"ship\", \"ship\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shyamalan\", \"sinatra\", \"singapore\", \"singing\", \"singing\", \"singing\", \"sir\", \"sir\", \"sir\", \"sir\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sizemore\", \"slapstick\", \"slasher\", \"snoopy\", \"snowblood\", \"sobieski\", \"social\", \"social\", \"social\", \"social\", \"society\", \"society\", \"society\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"song\", \"song\", \"song\", \"song\", \"songs\", \"songs\", \"songs\", \"sopranos\", \"soviet\", \"space\", \"space\", \"space\", \"space\", \"space\", \"spaghetti\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stanwyck\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"states\", \"states\", \"states\", \"states\", \"states\", \"steve\", \"steve\", \"steve\", \"stevens\", \"stewart\", \"stewart\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stinker\", \"stinks\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"streep\", \"stuart\", \"stupid\", \"stupid\", \"stupid\", \"stupid\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"superman\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"surprised\", \"surprised\", \"surprised\", \"surprised\", \"surprised\", \"sutherland\", \"takashi\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"tarzan\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"television\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tells\", \"tells\", \"tells\", \"tells\", \"tells\", \"tells\", \"tells\", \"teresa\", \"terminator\", \"terrible\", \"terrible\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"trash\", \"trash\", \"travolta\", \"trevor\", \"troll\", \"troma\", \"troy\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"underlying\", \"underlying\", \"understand\", \"understand\", \"understand\", \"understand\", \"understand\", \"united\", \"united\", \"united\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"val\", \"vampire\", \"vampire\", \"vampires\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"version\", \"vhs\", \"vhs\", \"victoria\", \"vienna\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"violin\", \"virus\", \"visitor\", \"visitor\", \"visual\", \"visual\", \"visual\", \"visual\", \"von\", \"von\", \"walsh\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"war\", \"war\", \"wars\", \"wars\", \"wars\", \"waste\", \"waste\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wayne\", \"wayne\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"welles\", \"werewolf\", \"westerns\", \"westerns\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"william\", \"william\", \"william\", \"william\", \"williams\", \"williams\", \"williams\", \"williams\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"woo\", \"woody\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worse\", \"worse\", \"worse\", \"worse\", \"worst\", \"worst\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"yet\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"zombie\", \"zombies\"], \"Topic\": [2, 2, 10, 2, 5, 1, 7, 8, 1, 2, 4, 9, 10, 1, 4, 6, 9, 1, 2, 4, 7, 9, 10, 1, 2, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 8, 1, 2, 3, 5, 6, 7, 8, 8, 4, 8, 8, 4, 6, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 5, 6, 7, 8, 1, 2, 4, 5, 6, 7, 8, 9, 10, 5, 4, 1, 6, 8, 6, 8, 6, 8, 9, 10, 3, 4, 7, 9, 6, 3, 7, 9, 3, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 4, 6, 10, 2, 3, 5, 6, 7, 10, 5, 2, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 6, 9, 1, 10, 8, 2, 4, 5, 6, 8, 10, 1, 8, 7, 6, 5, 6, 9, 8, 1, 3, 4, 5, 7, 8, 9, 10, 1, 3, 4, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 3, 4, 6, 7, 9, 6, 5, 10, 1, 3, 7, 9, 4, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 3, 6, 9, 10, 7, 8, 9, 10, 4, 1, 5, 8, 9, 10, 1, 2, 1, 1, 2, 3, 6, 7, 8, 10, 8, 10, 7, 9, 2, 3, 6, 7, 8, 9, 1, 3, 4, 5, 6, 7, 8, 10, 1, 2, 4, 6, 6, 9, 7, 7, 10, 10, 9, 1, 2, 3, 4, 5, 6, 10, 8, 2, 2, 3, 6, 9, 2, 3, 6, 7, 7, 6, 8, 8, 9, 7, 1, 2, 4, 7, 8, 9, 10, 3, 2, 6, 4, 1, 2, 6, 8, 7, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 7, 10, 5, 1, 2, 4, 6, 1, 2, 3, 5, 8, 1, 2, 3, 5, 7, 8, 8, 1, 2, 4, 5, 10, 1, 2, 4, 5, 10, 1, 2, 3, 4, 5, 9, 10, 5, 1, 9, 7, 10, 9, 7, 7, 1, 4, 7, 7, 1, 2, 4, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 7, 8, 9, 5, 1, 2, 6, 8, 5, 9, 8, 2, 3, 6, 9, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 10, 1, 2, 4, 5, 6, 8, 1, 3, 5, 9, 2, 1, 1, 5, 5, 8, 5, 7, 2, 4, 7, 8, 10, 3, 7, 8, 9, 10, 1, 3, 4, 7, 8, 9, 10, 7, 9, 10, 3, 4, 5, 9, 10, 7, 9, 2, 3, 5, 6, 8, 9, 10, 2, 3, 6, 7, 1, 2, 3, 4, 6, 7, 8, 9, 10, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 9, 1, 2, 4, 9, 4, 10, 6, 1, 2, 3, 4, 9, 1, 2, 3, 4, 5, 9, 10, 1, 2, 8, 9, 8, 5, 5, 8, 3, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 10, 7, 9, 10, 6, 9, 1, 2, 4, 8, 9, 1, 2, 5, 6, 8, 5, 6, 7, 8, 9, 5, 1, 2, 3, 4, 5, 9, 1, 2, 4, 6, 9, 1, 2, 3, 4, 5, 6, 9, 1, 3, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 7, 8, 9, 8, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 9, 1, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 1, 3, 5, 7, 8, 4, 4, 3, 7, 8, 10, 1, 2, 4, 5, 6, 8, 9, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 4, 1, 2, 3, 5, 8, 1, 3, 5, 6, 9, 1, 3, 5, 6, 1, 3, 6, 1, 1, 2, 3, 4, 5, 7, 9, 10, 1, 1, 2, 4, 5, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 2, 4, 5, 6, 8, 9, 9, 5, 7, 6, 9, 3, 10, 2, 3, 5, 9, 1, 4, 5, 10, 3, 8, 6, 1, 2, 6, 7, 8, 9, 2, 7, 8, 1, 2, 7, 8, 9, 7, 1, 3, 6, 7, 2, 6, 2, 5, 7, 1, 2, 3, 4, 9, 1, 2, 5, 6, 9, 10, 5, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 9, 2, 4, 6, 10, 1, 2, 3, 7, 8, 10, 1, 2, 3, 5, 6, 7, 8, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 3, 6, 5, 6, 4, 7, 9, 10, 1, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 2, 3, 6, 1, 2, 3, 6, 7, 9, 1, 2, 6, 7, 9, 4, 3, 7, 9, 4, 9, 6, 2, 4, 9, 10, 1, 3, 4, 5, 6, 7, 9, 5, 1, 2, 6, 7, 8, 5, 10, 5, 1, 2, 4, 5, 6, 8, 9, 10, 5, 3, 1, 2, 3, 5, 6, 7, 8, 10, 2, 9, 1, 2, 2, 2, 9, 2, 3, 6, 8, 2, 3, 7, 8, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 5, 6, 1, 2, 4, 5, 7, 8, 10, 3, 7, 10, 2, 3, 4, 5, 1, 1, 5, 9, 2, 3, 5, 6, 8, 9, 10, 3, 1, 6, 7, 8, 9, 1, 4, 6, 7, 8, 9, 10, 4, 10, 4, 6, 5, 6, 7, 9, 10, 7, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 8, 1, 7, 10, 4, 6, 7, 4, 1, 1, 2, 7, 8, 2, 4, 6, 8, 9, 10, 4, 1, 2, 3, 5, 6, 7, 8, 6, 6, 3, 6, 10, 1, 2, 6, 7, 4, 4, 5, 1, 2, 7, 2, 6, 9, 1, 2, 7, 8, 7, 3, 4, 3, 3, 6, 9, 3, 5, 1, 2, 3, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 8, 9, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 3, 5, 7, 8, 10, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 1, 2, 3, 4, 5, 7, 8, 10, 9, 6, 1, 4, 3, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 10, 10, 10, 6, 2, 7, 9, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 7, 1, 2, 3, 5, 6, 7, 9, 10, 3, 4, 1, 2, 3, 5, 7, 9, 10, 1, 2, 4, 5, 7, 8, 9, 10, 10, 4, 1, 8, 1, 2, 3, 4, 6, 8, 9, 7, 9, 9, 1, 2, 3, 5, 6, 7, 9, 10, 2, 6, 1, 3, 7, 8, 10, 1, 2, 1, 2, 5, 6, 7, 8, 9, 7, 9, 10, 4, 7, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 9, 7, 1, 2, 4, 8, 10, 4, 8, 10, 5, 3, 4, 5, 5, 5, 5, 8, 2, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 8, 6, 9, 4, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 9, 10, 2, 3, 4, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 8, 10, 1, 2, 5, 8, 9, 1, 2, 4, 6, 8, 9, 10, 1, 2, 4, 9, 8, 7, 5, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 5, 7, 2, 3, 1, 2, 3, 5, 6, 7, 8, 1, 1, 3, 4, 7, 9, 10, 1, 3, 4, 7, 9, 10, 1, 2, 3, 4, 5, 8, 9, 10, 1, 4, 5, 7, 8, 9, 10, 10, 10, 6, 7, 2, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 6, 9, 1, 2, 4, 1, 5, 5, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 4, 7, 1, 2, 4, 9, 10, 6, 3, 5, 6, 9, 1, 2, 4, 5, 8, 9, 10, 8, 5, 3, 7, 9, 7, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 5, 9, 4, 4, 1, 2, 3, 4, 5, 6, 7, 9, 10, 6, 1, 2, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 8, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 5, 7, 8, 3, 6, 7, 10, 5, 2, 3, 5, 1, 2, 5, 6, 7, 8, 9, 8, 1, 2, 5, 4, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 1, 3, 4, 7, 9, 10, 6, 8, 6, 1, 3, 4, 7, 9, 10, 4, 3, 4, 3, 7, 2, 1, 2, 5, 7, 8, 1, 2, 5, 6, 7, 8, 2, 1, 2, 3, 9, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 5, 7, 8, 1, 6, 8, 1, 2, 5, 6, 6, 9, 1, 2, 4, 6, 9, 10, 7, 9, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 7, 9, 6, 8, 8, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 3, 5, 6, 7, 9, 10, 1, 1, 2, 3, 6, 8, 9, 4, 6, 6, 3, 10, 3, 3, 6, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 3, 7, 8, 10, 6, 8, 9, 10, 1, 2, 3, 7, 8, 10, 1, 7, 2, 8, 3, 1, 3, 5, 7, 10, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 8, 2, 3, 6, 7, 8, 9, 10, 4, 7, 8, 10, 4, 8, 10, 8, 5, 1, 2, 5, 6, 8, 3, 1, 2, 4, 6, 8, 9, 1, 2, 4, 7, 8, 10, 10, 1, 2, 4, 6, 7, 8, 9, 10, 3, 5, 6, 8, 9, 6, 7, 9, 9, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 9, 1, 2, 6, 7, 1, 2, 3, 4, 5, 6, 8, 9, 6, 1, 4, 7, 9, 10, 1, 2, 7, 8, 9, 9, 4, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 1, 4, 5, 6, 7, 8, 9, 2, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 10, 3, 6, 1, 2, 1, 2, 3, 5, 6, 7, 8, 10, 1, 2, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 9, 10, 2, 3, 6, 7, 9, 10, 2, 5, 7, 3, 2, 2, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 5, 7, 9, 10, 1, 2, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 10, 1, 2, 3, 5, 8, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 2, 9, 2, 1, 2, 4, 6, 8, 9, 10, 2, 8, 10, 10, 1, 2, 3, 4, 5, 7, 8, 10, 10, 6, 4, 5, 1, 4, 5, 6, 5, 10, 9, 1, 2, 3, 5, 6, 7, 9, 10, 5, 6, 1, 5, 6, 1, 2, 1, 2, 5, 7, 8, 1, 2, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 2, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 7, 9, 10, 6, 7, 9, 10, 4, 7, 9, 10, 1, 2, 3, 7, 10, 1, 4, 5, 7, 8, 9, 10, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 5, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 2], \"Freq\": [0.997788705705679, 0.9980149631562635, 0.9948528835227991, 0.9963288365870687, 0.994609418943029, 0.9924253141030203, 0.9976177659790217, 0.996316427265058, 0.47594470650493975, 0.4331958520763277, 0.06278038569228103, 0.01796123333977024, 0.010127673627720914, 0.27362571277361325, 0.046446410051771005, 0.6341198158950995, 0.045669064276845965, 0.3781166660263059, 0.06584824732544563, 0.097908014519696, 0.09837948168431733, 0.3218549177148273, 0.03787452889124677, 0.5964375329821888, 0.1783488321110636, 0.1391530164043956, 0.0009444774869076624, 0.017079301221580228, 0.0003148258289692208, 0.06304387225108646, 0.004643680977296007, 0.300750341929777, 0.07125469639567025, 0.00030846188915874566, 0.12461860322013324, 0.04318466448222439, 0.04472697392801812, 0.41518970280767165, 0.4123201343001733, 0.40998427106620083, 0.013290256331222619, 0.020297846033140002, 0.040998427106620085, 0.027305435735057382, 0.028755281880281668, 0.027466529751193413, 0.01917018792018778, 0.0005638290564761112, 0.17273876477349043, 0.7920464952966294, 0.03435145890381912, 0.2680642552564255, 0.13585908351905582, 0.027570425261813282, 0.10363822508055112, 0.001660868991675499, 0.001660868991675499, 0.4610572320891185, 0.9970721400903207, 0.9954727333487131, 0.9929759078490725, 0.9966862880090327, 0.9967827779225986, 0.9997094882718653, 0.9993074677291379, 0.9509868976786171, 0.04856769988144503, 0.33199844784530647, 0.03835160860149078, 0.10373795769255705, 0.09848633915696235, 0.08176991959295672, 0.059469032431241256, 0.0668286950268986, 0.06660679565215516, 0.1277770566230961, 0.02496367965863672, 0.9944388224220968, 0.9958321207198992, 0.7606628938730265, 0.07862934539749733, 0.09122741731201348, 0.06950660366629598, 0.0513395450341548, 0.08816344489496225, 0.02628095757700781, 0.6289094266683962, 0.035907122270911834, 0.0640216350277109, 0.018182755532929824, 0.01894673685784284, 0.06814713418224119, 0.9987295263607062, 0.9973556514575823, 0.9955901424128505, 0.1680615264808353, 0.831333473223161, 0.06039038167238674, 0.9389391515671955, 0.19115851496003855, 0.8086649537354439, 0.135397619896383, 0.8639657650531106, 0.030147865248616963, 0.02261089893646272, 0.07213953470204773, 0.8742880922098919, 0.9977098664018299, 0.8700956298802165, 0.11982249140299592, 0.010138826195638117, 0.9973191437991756, 0.9961432448622485, 0.1436060572462632, 0.2868366738199872, 0.25145139043316284, 0.01398516505208707, 0.05687926188969641, 0.119671714371886, 0.06307403298659403, 0.05687926188969641, 0.001689483026426626, 0.00591319059249319, 0.0979207727544505, 0.9011771117558023, 0.00307146689528101, 0.9419165145528431, 0.05323875951820417, 0.023609926603173965, 0.8231278956652014, 0.04936621017027284, 0.0182440341933617, 0.08263474311110887, 0.0032195354458873587, 0.9983980732943093, 0.9974143615515442, 0.2941740183685692, 0.07824121744053907, 0.05864041555584915, 0.3813246912112401, 0.15113676163979906, 0.030130158269027464, 0.005993633634161378, 0.04949331790771479, 0.811240474432816, 0.017997570148259923, 0.0229469019390314, 0.08548845820423463, 0.0031495747759454863, 0.00944872432783646, 0.28309725595979424, 0.22225138498774066, 0.3275475329573424, 0.000242897688511192, 0.025868603826441947, 0.07201916464356842, 0.026111501514953138, 0.024411217695374795, 0.01846022432685059, 0.06773049260757566, 0.9320806180825815, 0.9958256124208017, 0.14537562673246104, 0.19308360104253297, 0.23157366291216328, 0.014847171946054833, 0.14776806202234666, 0.0604441739415218, 0.18632848963579712, 0.01217327368088856, 0.008373523514599645, 0.19935245756876055, 0.7743933454160791, 0.02420443246458694, 0.0020417849471660765, 0.9941332646189576, 0.9934187456707931, 0.9984353401799252, 0.15103587970494115, 0.06651839261616578, 0.03521561962032306, 0.04382388219417981, 0.6737921887355146, 0.029737634346050586, 0.9915152839520287, 0.9974666316584219, 0.9986224370381658, 0.9987565274660083, 0.3336642104271061, 0.6151611062208721, 0.05113429850198685, 0.9966631856711863, 0.351740144403709, 0.14831400434847336, 0.1449432315223717, 0.014831400434847337, 0.005393236521762668, 0.021235868804440505, 0.001853925054355917, 0.31162794777309916, 0.1258779604428259, 0.03263502678147338, 0.21212767407957697, 0.10314999536287121, 0.012238135043052517, 0.5140016718082057, 0.04843991623188421, 0.046442393913043625, 0.6172343965217411, 0.1240960740579714, 0.01672924942028991, 0.03295911826086967, 0.06267226275362339, 0.02297150666666674, 0.028464693043478352, 0.0008475410914248495, 0.8416083037848756, 0.001695082182849699, 0.10890903024809316, 0.030935249837007007, 0.015679510191359717, 0.9966645777835701, 0.9969966414361373, 0.9986867363492908, 0.08251085398858123, 0.004436067418740927, 0.7984921353733667, 0.11356332591976771, 0.996355535974148, 0.9978052166992095, 0.40595444933145464, 0.010819794573244937, 0.0006364585043085257, 0.07786009036040964, 0.015805386190328386, 0.028587594485191277, 0.11031947408014445, 0.12177572715769791, 0.15550802788604978, 0.07271538411724907, 0.9986839196159918, 0.5728874652739178, 0.2502589090357131, 0.015524456577308693, 0.0046690094969349455, 0.021477443685900748, 0.02748879341320449, 0.04120400881045089, 0.06565794605064766, 0.000933801899386989, 0.37761109772172236, 0.14566151527172685, 0.014717357252368248, 0.00252009541992607, 0.13487550687444327, 0.1651166519135561, 0.061691935879790195, 0.07429241297942055, 0.023588093130508015, 0.1658238050866716, 0.7001449548103912, 0.1329794765429347, 0.01633472981880982, 0.21053651766465992, 0.0950384280367117, 0.009569841712029997, 0.31547478195519574, 0.09635840620388825, 0.10493826429053582, 0.0004949918126912067, 0.11632307598243358, 0.03481442415928154, 0.999268641230546, 0.7102947588516194, 0.15320083034054535, 0.08205024707962015, 0.054498319093474634, 0.9970378860071825, 0.9166477568464768, 0.0035597971139669, 0.07564568867179663, 0.0035597971139669, 0.9986154590021331, 0.740271770919739, 0.03825426749166249, 0.00034463304046542783, 0.045663877861669186, 0.17541821759690276, 0.3583194970144317, 0.6416759892826217, 0.9967439912702799, 0.05154814454442972, 0.1012796143912584, 0.5736150357675307, 0.01975633733641139, 0.1351151806340779, 0.10082544571685814, 0.017712578301610212, 0.05725769924375993, 0.9403475991186727, 0.9947272013607864, 0.9973802259187773, 0.050621060371120075, 0.6247887040325916, 0.04854074282162199, 0.17856058966525232, 0.04576698608895788, 0.05166121914586912, 0.06464007654723773, 0.11176477751393361, 0.08966204166229749, 0.057967552516555124, 0.005421425774929616, 0.5087799573395486, 0.0796532556162736, 0.08215545212777957, 0.20218427976954292, 0.6433534449438219, 0.09375392388013475, 0.06067718905326478, 0.9965269117629888, 0.9955693135899826, 0.9961482317819541, 0.9968764593511615, 0.9976081612167358, 0.9982950039869377, 0.9980369124871512, 0.15916435529182035, 0.35011947466970533, 0.005684441260422155, 0.47222969433803313, 0.011789952243838544, 0.0006316045844913506, 0.00042106972299423375, 0.9924095970546012, 0.9971478102346342, 0.9976192992543729, 0.9910223196451993, 0.7323098441650228, 0.26690877826545234, 0.18157305793373923, 0.3464531089284089, 0.3798391228065481, 0.09225082782117397, 0.9969776890895067, 0.07637542114613545, 0.9236247964028415, 0.999216848195022, 0.9993830687890193, 0.995786918046716, 0.2396105733899419, 0.05724576129070174, 0.22460700412810433, 0.14173014534843004, 0.019111689416864544, 0.31293158746118394, 0.004822575834162081, 0.9975142077276113, 0.9980217180740326, 0.9980424830604014, 0.9978752620884935, 0.035312177789620224, 0.29835186948781167, 0.04251874468546109, 0.6240886931798186, 0.9982065649119686, 0.4923507783751977, 0.009175761964935416, 0.15849043393979356, 0.1690401067871792, 9.813649160358734e-05, 0.086801726823373, 9.813649160358734e-05, 0.08169862925998646, 0.002208071061080715, 0.5814451050508395, 0.05459185487281554, 0.08021375609506542, 0.22892308478609105, 0.0007440116507368387, 0.0032085502438026167, 0.05087179661913135, 0.9992653996600138, 0.9985657134579884, 0.9962918001805279, 0.04628235300845541, 0.7999133344961377, 0.09835000014296774, 0.05553882361014649, 0.08350615633587223, 0.03182577286926599, 0.6406031713318311, 0.006131570919766842, 0.23767136803286712, 0.03326563851163316, 0.017665201140660367, 0.36362784166164525, 0.15852797387267942, 0.035330402281320734, 0.3911580252574796, 0.9937037475385329, 0.2405228442834803, 0.0709041301377343, 0.5121132226202435, 0.1708714372930558, 0.00551198184816309, 0.004824696384522799, 0.12544210599759278, 0.7711473054595608, 0.09327746343410745, 0.004824696384522799, 0.23695317152158327, 0.037357481996646014, 0.011029351827581203, 0.6350771939429822, 0.0003557855428252001, 0.05870461456615802, 0.019923990398211206, 0.9975888701041176, 0.9955252018642077, 0.9977055070653601, 0.9655921863850575, 0.0330334169026467, 0.9980313789343784, 0.998793861111511, 0.9967010877571764, 0.02110802700206788, 0.17519662411716339, 0.8031604274286828, 0.9989981600689557, 0.11965921031729819, 0.045492060138648496, 0.04128781761398667, 0.7917990088113109, 0.0016170163556391646, 0.1609038741595587, 0.13739030886181522, 0.35926540466552276, 0.09665168991572473, 0.028708422747244968, 0.04757395769543452, 0.10335032189008189, 0.01872882817320267, 0.03144255824698258, 0.015994692673465056, 0.10295058465921426, 0.2888697839593987, 0.10295058465921426, 0.4882550934892694, 0.00999098500912206, 0.006515859788557865, 0.9961370360414854, 0.006695290789845315, 0.051888503621301194, 0.8369113487306644, 0.10294009589387172, 0.9971631879954045, 0.9976704892873569, 0.994081113221571, 0.1662520193922861, 0.13877234676546193, 0.15755012306045843, 0.537227599854412, 0.9950659487237261, 0.9966559031702621, 0.5335082634591155, 0.3163823982737167, 0.003307378273768836, 0.003835151402561735, 0.062171674571803545, 0.011012865954145166, 0.006333277545514792, 0.033179337363446935, 0.001583319386378698, 0.028710858206333723, 0.9987170775834208, 0.014207172333088708, 0.15777438748851144, 0.7054982682246945, 0.0168242830260261, 0.0785133207881218, 0.025797233973240023, 0.0003738729561339134, 0.00112161886840174, 0.028173838209803688, 0.9714485772341401, 0.9993261813290689, 0.13782945220046267, 0.17647757562153835, 0.03119788276159121, 0.0405106835859468, 0.5690121303681263, 0.04516708399812459, 0.028784496232451797, 0.3632522341447438, 0.06729896302235208, 0.5400133659383914, 0.995587325000739, 0.9924539998400493, 0.9896310817480377, 0.9973494104819706, 0.9235326926408044, 0.07598295732094912, 0.9968999466405016, 0.9977444095361372, 0.048548970903129984, 0.04731988303216467, 0.10508701296753452, 0.14318873696745932, 0.6551038352245134, 0.7776923646372612, 0.07818511473358562, 0.031052244149509186, 0.002218017439250656, 0.11062361978262647, 0.05800249875124169, 0.12545484280465197, 0.16455765095155647, 0.1942106137962924, 0.014989409789646729, 0.40699506146236447, 0.035844240801329134, 0.04480020958623237, 0.1926409012207992, 0.7616035629659502, 0.020653269930305196, 0.09638192634142426, 0.06927450955789868, 0.5090170484906469, 0.3042054550151203, 0.011201447580321983, 0.9879676765843989, 0.2055371695401851, 0.4866742175319326, 0.08454361407194806, 0.11086857174704566, 0.01738122205471188, 0.08302486651376935, 0.01198123073674314, 0.035477388598816655, 0.7540557685821213, 0.0619241691906618, 0.14771494525689116, 0.7118903543900522, 0.08472394755450224, 0.019664323629933856, 0.02468500200353399, 0.010041356747200266, 0.03221601956393419, 0.06213089487330165, 0.054599877312901446, 0.9962106040308596, 0.7921430636975537, 0.20660344430183414, 0.15607605141065523, 0.04392089888943061, 0.14483439276633667, 0.46116943833902146, 0.055946859299631856, 0.035032145542760136, 0.015947469239614688, 0.007581583736865999, 0.06065266989492799, 0.019084676303145445, 0.1644017164657688, 0.8345031127802424, 0.18906619609430492, 0.3737149577004251, 0.43666339915705704, 0.00044174344881846945, 0.0298698996788822, 0.9677847495957833, 0.996154413131285, 0.24151314664201523, 0.13833799012636505, 0.00023133443164943988, 0.5010703789526868, 0.11867456343616266, 0.23105078597091003, 0.1633533204943346, 0.01377178831998961, 0.4513993990907438, 0.05848861906983539, 0.07557891059946105, 0.006305156098308496, 0.8234247862041468, 0.12444414141785835, 0.05117329179799782, 0.0007753529060302701, 0.9994549712600939, 0.9983433059483592, 0.9618309202650325, 0.037880917017240806, 0.9977099597476654, 0.029659482879155024, 0.9688764407190641, 0.6280079184704037, 0.15751975822388836, 0.0062133432754492126, 0.03635956435262872, 0.04763563177844396, 0.03566919287757881, 0.00276148590019965, 0.036129440527612085, 0.04510426970326095, 0.004487414587824431, 0.9928308638029537, 0.9961979835101854, 0.04187342156014453, 0.9115521770400694, 0.046168131463749096, 0.9979696116749502, 0.9987142964016394, 0.18129224315369036, 0.21440031973927984, 0.025232146472717966, 0.5606495435903345, 0.018231319705721075, 0.04570668715960468, 0.11337050834686259, 0.15056320476105073, 0.6627469638142679, 0.0273343913405479, 0.9951838053322966, 0.007087062555772735, 0.8777833194078516, 0.016199000127480538, 0.09921887578081828, 0.9936535163909914, 0.0056140787910392885, 0.031189326616884935, 0.16904615026351635, 0.6643326569396492, 0.015594663308442468, 0.11477672195013656, 0.19728303505670217, 0.4047833179305069, 0.030952250740046188, 0.3667691459051104, 0.00015025364436915624, 0.24883298823892558, 0.04388664004898858, 0.061356079291789864, 0.6046130313545126, 0.0034086710717661037, 0.03323454294971951, 0.005113006607649156, 0.09565508659889416, 0.8487214956410972, 0.05391468517392216, 0.9957099674647754, 0.4851562525723036, 0.16640375918792233, 0.22094757176947302, 0.0014183970174871335, 0.024886420397728797, 0.06453706429566457, 0.01785890790199709, 0.0012249792423752516, 0.007220930270843588, 0.010380087264337659, 0.995850978102261, 0.8035068604648392, 0.03405377208831811, 0.15378251046232544, 0.008378309005856044, 0.9997771543913793, 0.9996188080648868, 0.998787691617101, 0.32053064909106677, 0.41199444864530826, 0.015832361977471145, 0.07693495966067881, 0.062107395956220425, 0.02463113604385305, 0.020123122170336395, 0.03701459584098314, 0.009912199179720357, 0.02096498018286059, 0.025635840714586695, 0.012324923420474374, 0.6783637850629095, 0.0335237917036903, 0.022184862156853873, 0.13261617600430425, 0.02514284377776772, 0.01725489278866412, 0.0004929969368189749, 0.052750672239630314, 0.33102852828267615, 0.4283999738422792, 0.0009169765326389922, 0.036507128205689875, 0.009857497725869167, 0.018683396852519466, 0.12763167113668972, 0.02378407881532386, 0.023153657449134554, 0.0018021082412466875, 0.3238645953554761, 0.1518919803336494, 0.05432069127186444, 0.352955771249887, 0.021625298894960253, 0.09345218451036394, 0.4238163261147368, 0.005422679902307708, 0.13268619885959174, 0.06846133376663481, 0.0040670099267307806, 0.0801539873059858, 0.25012111049394303, 0.0352474193650001, 0.3580985376149528, 0.2504411299650704, 0.055632487550997436, 0.08563225899488001, 0.18484036392797273, 0.01765809331823468, 0.002943015553039114, 0.020411236900109982, 0.016044181563342266, 0.008354366731207806, 0.07452430485186544, 0.9249781366908005, 0.9921468969602921, 0.022516780295257888, 0.5576362404965264, 0.0700661822595455, 0.17145458962255028, 0.1783731534004228, 0.9977070087625866, 0.9968975598105618, 0.8157134370605229, 0.08443579529820883, 0.037256592603865976, 0.06253116547583536, 0.05602244438833904, 0.12605049987376282, 0.5907366680591821, 0.016006412682382582, 0.07703086103396617, 0.09653867649061995, 0.03701482932800972, 0.7235373277133421, 0.06929457430694724, 0.07833299704263601, 0.03256149729139159, 0.0828522084104804, 0.0005793860728005622, 0.012746493601612369, 0.7433063353606612, 0.013641558864920371, 0.13863630753419076, 0.04822132435971852, 0.056152463234672226, 0.9955050781931111, 0.8341708791470768, 0.07063731121468282, 0.02137708102549612, 0.055998657903745266, 0.017891687380034795, 0.1398469831243436, 0.06255520965562035, 0.04541267624037824, 0.7509632620677117, 0.0012029848010696224, 0.10127475060817456, 0.02269951306734947, 0.17635775536940743, 0.699028594715044, 0.09619191374405482, 0.04580567321145468, 0.8577112308844889, 0.9940344680600788, 0.35626477909089604, 0.13998936340055512, 0.00968752205482978, 0.3336549007105177, 0.10528069542496868, 0.0012905375568221967, 0.037136916273291375, 0.016692084452055517, 0.9886444906499896, 0.3040502913263013, 0.15903135502767715, 0.4392317418733642, 0.0270170950152632, 0.00014396320612040781, 0.0682385597010733, 0.0022554235625530557, 0.3417581422703339, 0.1550761490965107, 0.23369169560194716, 0.00015962547513794206, 0.056986294624245315, 0.05052146288115866, 0.04605194957729628, 0.112935023660094, 0.0007183146381207393, 0.0021549439143622178, 0.7939899692127855, 0.03746918955835617, 0.10955277328014613, 0.052813714806063936, 0.006066440214210047, 0.41462442782330927, 0.15937538856417413, 0.08244357654902723, 0.014827422901002073, 0.03136271650264313, 0.04005733097862341, 0.006365699884199843, 0.18239282777960406, 0.05181834966711458, 0.01669055457442642, 0.9956024000822409, 0.06914794425824264, 0.015614051929280596, 0.007807025964640298, 0.8230835945577915, 0.0836467067640032, 0.9982128137034443, 0.9990218267145825, 0.05656834958467695, 0.9412973370890244, 0.054962028729001265, 0.943098447508999, 0.9943013080208352, 0.9975059904946064, 0.9977931601025158, 0.05770784064203038, 0.9416230582809347, 0.9981875103754113, 0.11331780950088383, 0.059640952368886224, 0.2403179551334533, 0.5862354788729934, 0.9939028362047269, 0.9950562098357802, 0.9975609522750201, 0.27538331875010874, 0.1922657517899683, 0.13543653158931412, 0.2819554054399803, 0.0814423291764477, 0.033504755673855066, 0.10538129983899361, 0.8632102995507129, 0.030239851258145992, 0.13179691156024498, 0.1946753441025005, 0.6550352966440143, 0.018429885400316277, 0.9970444652028237, 0.9988463143967071, 0.09937141349787497, 0.005109070102718507, 0.8284357171558059, 0.06692881834561244, 0.9860635077136695, 0.013542503110230654, 0.15684799426718174, 0.5374816369206203, 0.30537829186867965, 0.19672019092592852, 0.23868360121855184, 0.0008018486043176434, 0.4803073139862684, 0.08339225484903491, 0.9896263202675916, 0.04025646080991437, 0.8353215618057231, 0.051039441383998574, 0.012220711317295433, 0.06110355658647717, 0.995097601822552, 0.3428116878929702, 0.29899804128666424, 0.10455446392479009, 0.012403530263233997, 0.08551139687358966, 0.09357369154469176, 0.05891676875036148, 0.003137363537170952, 7.296194272490587e-05, 0.11194043691691863, 0.21421773233470462, 0.2792539318625749, 0.0006514477415145604, 0.14364422700396057, 0.17882240504574684, 0.06481905028069876, 0.006514477415145604, 0.9965368516924054, 0.10635140177603154, 0.02604524125127303, 0.8146372680259287, 0.05209048250254606, 0.05740889696751452, 0.23598721051327246, 0.4970144718315247, 0.1429115094723234, 0.012947538465013914, 0.053866645878029584, 0.37569991519370793, 0.2470280240482883, 0.15397988470047813, 0.026614656265781616, 0.06670856996849656, 0.0966242688718945, 0.03287288293292923, 0.0005501737729360541, 0.9979324386656774, 0.13903552426935334, 0.22536656549879383, 0.3991266580210064, 0.003156778932078111, 0.008921331764568574, 0.07919397581778566, 0.10540896607982563, 0.00713706541165486, 0.02827375913078656, 0.004254788995409628, 0.633154393847966, 0.1739649287167368, 0.0008351430102417021, 0.004523691305475887, 0.0006031588407301182, 0.029090814856752623, 0.04249949985452218, 0.024311940964813997, 0.09061301661122469, 0.00041757150512085106, 0.9996813622851345, 0.9529258879843475, 0.027884599775623402, 0.01818560854931961, 0.7070384121632863, 0.2920376050239661, 0.9929549336062875, 0.14072820897435298, 0.7908528926868569, 0.06805166912844299, 0.556669539479303, 0.011766338662463401, 0.01519665056092452, 0.0059016118683202015, 0.0993683898328414, 0.1284707133584954, 0.14208130572980884, 0.040536696520524385, 0.13565681942195504, 0.259027738217016, 0.6045686933484109, 0.06324609148912999, 0.10277489866983625, 0.8323637397765858, 0.18460953666062876, 0.4825738978353266, 0.012836726768387858, 0.13652620418920985, 0.16970587931089032, 0.013598227508885443, 0.24228121883111425, 0.3794939487037849, 0.3017720527619225, 0.0690861297260999, 0.006956589451586449, 0.993936103578402, 0.9961351402434889, 0.16625971757440822, 0.8335302619334426, 0.9953555789611197, 0.9971006551972779, 0.9961159391869066, 0.9975369626502507, 0.0014136778111623843, 0.4149144375761598, 0.5831420971044835, 0.022319367744146956, 0.10868561684106344, 0.09412950744270673, 0.08992440917207034, 0.6006821145055202, 0.0016173454887063012, 0.08248461992402137, 0.9957982716723086, 0.0220693046228539, 0.1956592081488838, 0.003293926063112522, 0.721040415215331, 0.05764370610446913, 0.83104346877747, 0.16780685427237377, 0.9961803595516436, 0.05964592968425188, 0.07250065590930616, 0.006427363112527142, 0.6833572461238857, 0.0005141890490021713, 0.06118849683125839, 0.04987633775321062, 0.06607329279677902, 0.9962293927255879, 0.9991190611300937, 0.033380693124403706, 0.12158777607324701, 0.5391261666069894, 0.057250685973139315, 0.010816090509583324, 0.12848769588108466, 0.1048041873514798, 0.004475623659137927, 0.9984575202454461, 0.9987325532626726, 0.09550572254777186, 0.9042105168536935, 0.998592976713574, 0.9597526144791486, 0.04020821517854357, 0.14970571422326978, 0.8152648533604571, 0.014429466431158533, 0.019840516342842982, 0.3460551189953988, 0.5633455425506493, 0.062419471265193885, 0.02708769507734829, 0.0009814382274401555, 0.9954891540369419, 0.3851304620622002, 0.06734409886710808, 0.11288836025899729, 0.2608816231189339, 0.06345492598420518, 0.034593169326873154, 0.015556691531611594, 0.009006505623564608, 0.04707946121408772, 0.004093866192529367, 0.9987208393730764, 0.06280647013373868, 0.008638808159659225, 0.18001408354316922, 0.5554520165359269, 0.19285555513185187, 0.2026166297813883, 0.06808861163583863, 0.0779838423926041, 0.0002356007323039399, 0.5732165816954857, 0.07751264092799623, 0.9977346815492785, 0.7518142583957853, 0.025969404435087575, 0.22171379036456018, 0.03230832092340146, 0.02538510929695829, 0.8769401393494681, 0.06230890463798853, 0.992598768535117, 0.9953671635391934, 0.9972084451968048, 0.9982893149914372, 0.03849550502951566, 0.17377970841895643, 0.017048009370214078, 0.6467244844958632, 0.030246468237476593, 0.04564467024928286, 0.047844413393826606, 0.9923990186089192, 0.9971739943310524, 0.06216461860687207, 0.5285709836241774, 0.01167733167200912, 0.39737272777983973, 0.007492492436039633, 0.005351780311456881, 0.10168382591768074, 0.0927641920652526, 0.0021407121245827524, 0.7153546349647364, 0.07492492436039633, 0.9624080570220688, 0.030903011005295786, 0.9947138084301176, 0.9988862354132646, 0.9952481058358876, 0.03851973724892067, 0.8098539880138931, 0.13059130433170665, 0.020669127304298894, 0.059714573473089644, 0.9392144025273605, 0.0010451179567159204, 0.0028367487396574985, 0.038520061833243927, 0.01000327187142381, 0.0839080416677639, 0.22022128373656893, 0.027620974570349326, 0.5928804865884172, 0.02299259504775025, 0.0014347364586275114, 0.2284817810364312, 0.7091184946766476, 0.06061761537701236, 0.9965948861793464, 0.08169851608047846, 0.9184893777532578, 0.9950713828500013, 0.9974510775299604, 0.9978330420189391, 0.9993377410634362, 0.9959032226233898, 0.041016409404526714, 0.2737095027580124, 0.21688676972930224, 0.46838738739510755, 0.11126364415666153, 0.0031903553880046316, 0.18264784596326517, 0.1535358530477229, 0.03987944235005789, 0.5092604788102393, 0.997022995459112, 0.5058199219116454, 0.2852534402641431, 0.023296158749021193, 0.07387261741079167, 0.015991899948853028, 0.022632135221733178, 0.07315325858956298, 0.9989962514522619, 0.9981647722798556, 0.9955962149503991, 0.9969365935979884, 0.9940704021155933, 0.028120179560588248, 0.9420260152797063, 0.015999412508610554, 0.013090428416135908, 0.9979593652706279, 0.8689065536589441, 0.1292001574885031, 0.2087909867074744, 0.37252448453821785, 0.4185668365597953, 0.9432043992125845, 0.038797255909109964, 0.017243224848493315, 0.05766539071692384, 0.2189149092031368, 0.7234870779762204, 0.997216833904274, 0.998024719219005, 0.9959504893440967, 0.9952297533978302, 0.9926818320147404, 0.9954144490762387, 0.9972593106481483, 0.9971881166922968, 0.027659022429633535, 0.9705802416216859, 0.2710853937364079, 0.025586108057119854, 0.3031548594912595, 0.2626338738804362, 0.06749638460317138, 0.018523879136376365, 0.05151953884804677, 0.4601670811769228, 0.31209925065087185, 0.010613998207079707, 0.004617004035503372, 0.037481213572352096, 0.04945816500024461, 0.05887957913911311, 0.05857291466443023, 0.004293302645560331, 0.0038162690182758498, 0.8413809060893774, 0.02286361157851569, 0.02309224769430085, 0.10631579384009797, 0.006401811241984394, 0.9983238582921288, 0.9948898167166893, 0.37292815149257136, 0.13267636158870327, 0.15949226545857045, 0.11479909234212515, 0.0025984402974677493, 0.03398759909087816, 0.09754544876693931, 0.05404755818732918, 0.023385962677209743, 0.008574852981643573, 0.9964163546105776, 0.08962332040881398, 0.5082681028036242, 0.3241900161418554, 0.04565716322713165, 0.03067968111029481, 0.0014494337532422745, 0.9969529615574247, 0.997174921607791, 0.6313360566741508, 0.18009213740987529, 0.009243810745036371, 0.00024325817750095715, 0.012162908875047857, 0.031785735193458405, 0.056760241416890005, 0.04646231190268282, 0.031947907311792374, 0.9984273315276178, 0.41839484575317326, 0.0009219329070387486, 0.1355783686821689, 0.0008677015595658811, 0.0080262394259844, 0.11941742713525437, 0.12402709167044812, 0.19268397757109845, 0.9971596474784622, 0.9985350733374744, 0.14896334694364285, 0.849244648039531, 0.9967868298519208, 0.41084224531206137, 0.29266953098656245, 0.026586702711653514, 0.08934167956675776, 0.0066466756779133785, 0.004100222009102409, 0.08196127995037342, 0.06383398264697329, 0.024040249042842545, 0.410386592372631, 0.33013490470754747, 0.028440961452659938, 0.033934757869964235, 0.0906053809130801, 0.016016529555064066, 0.06605233692497398, 0.007564535066903609, 0.009001374129890887, 0.007860354873989225, 0.030143252755179366, 0.07006604131183376, 0.45734803820948733, 0.00030947898105933644, 0.11413584821468327, 0.08609705253070739, 0.07470822602772381, 0.11097916260787803, 0.05626327875658736, 0.45890208648267916, 0.06041102346209182, 0.01682842521796386, 0.1162370568198881, 0.18511267739760245, 0.011084591460934278, 0.025948020919914334, 0.08550250776911578, 0.03491646310194298, 0.005038450664061036, 0.06971293334076759, 0.9286758620037968, 0.9992054934007073, 0.994462536518418, 0.9976541750159209, 0.9987806009784486, 0.059125799678713016, 0.28354953394307536, 0.10617213490693628, 0.5505692744275857, 0.949296912951275, 0.049636439892877124, 0.32244857469000987, 0.09133345763647395, 0.09001857950881706, 0.23738607427774566, 0.16769753351193112, 0.002629756255313757, 0.011125891849404356, 0.0387383325301988, 0.02862388539437666, 0.009912158193105699, 0.9511478138506068, 0.04851643653093249, 0.9952564320276821, 0.0012307600219832334, 0.015472411704932076, 0.3722169952197864, 0.3062834226135418, 0.13151549949192265, 0.11252663058132419, 0.00017582286028331904, 0.060658886797745074, 0.9960635095830448, 0.9935599091272731, 0.29333754498621345, 0.013777975597837298, 0.042667279270721954, 0.646231500621143, 0.004000057431630183, 0.019448791411688586, 0.9782742080079359, 0.011982835646848395, 0.12167186964492216, 0.011061079058629287, 0.049774855763831796, 0.1828150566634563, 0.015362609803651789, 0.6068230872442456, 0.0006145043921460716, 0.9948113185207788, 0.998359691131824, 0.9876172600535494, 0.9934272119035238, 0.3215143082988771, 0.5836145501678417, 0.0002339136473618604, 0.06689930314549207, 0.019297875907353483, 0.008070020833984184, 0.0002339136473618604, 0.009023184760884283, 0.9880387313168288, 0.9985350670899859, 0.16341720927205194, 0.5519935671173188, 0.1206841247915334, 0.04730110385602225, 0.006483640403940744, 0.10403295739050376, 0.000884132782355556, 0.005304796694133336, 0.3904378298222512, 0.6095221377542969, 0.0018791488260128102, 0.8276606607016422, 0.05240737281435726, 0.035286239066240545, 0.08289134265856507, 0.9237238071882051, 0.0740163307041831, 0.5999435308620383, 0.3175671876114032, 0.0024146195622382106, 0.019674677914533565, 0.019138095789591743, 0.029064865101015497, 0.012207243342426509, 0.14952258111382685, 0.08317193574456617, 0.7663032282083626, 0.08121754348423708, 0.04006732145222363, 0.06605693536717949, 0.8132583354221606, 0.9971786626498669, 0.5158482297556077, 0.15352416049926002, 0.03320775182326129, 0.14400601507221059, 0.042020849440899635, 0.023548596834329662, 0.025029197234092904, 0.03511138090867117, 0.02270253946303638, 0.004970587056348028, 0.037536763848571714, 0.40039214771809833, 0.5621130386323615, 0.9987221417652122, 0.24018538978460488, 0.11070958558264035, 0.20942655808310903, 0.3336833547400544, 0.10582370617879264, 0.13336476355266497, 0.3081807373987258, 0.5582396690599727, 0.9953297181779291, 0.0026780209256179327, 0.8989223573657528, 0.09819410060599086, 0.9983133529640178, 0.999204190133384, 0.9963674055283641, 0.9946235053551237, 0.06982837161456504, 0.05358921542513131, 0.004871746856830119, 0.8704187717536479, 0.1499127903714524, 0.02964782457106873, 0.17716966134808007, 0.04941304095178122, 0.09970276488819081, 0.08758860000968961, 0.14178354815035288, 0.19358754269657513, 0.043276260059382586, 0.02789445860181198, 0.9974759109945007, 0.9923654828571894, 0.998685321466854, 0.9990798495758517, 0.9966514281514803, 0.19115753366417984, 0.8076656003507494, 0.9978703449048117, 0.99738595290633, 0.35072488910490746, 0.5023832780872058, 0.031869666686295844, 0.037736222245464725, 0.05731783336863655, 0.010306111117458855, 0.006421500003955132, 0.00047566666695963945, 0.0026161666682780168, 0.00015855555565321314, 0.15375425347335903, 0.019588883255019302, 0.188866402704054, 0.6375627097152509, 0.8845552686063854, 0.03179773841395503, 0.08325226057471863, 0.2420038190976795, 0.008791725741598299, 0.06408705343217708, 0.4983520328263878, 0.12748002325317534, 0.0159639230571127, 0.030539678891867775, 0.012030782593766094, 0.0006940836111788131, 0.2041390637813833, 0.6743646409167336, 0.05567429012219545, 0.01568289862597055, 0.04966251231557341, 0.09061429539078089, 0.19164795368229248, 0.23597294831736812, 0.000939450753344571, 0.0020497107345699727, 0.03954233625441239, 0.09548235838538457, 0.24109722515379306, 0.02374248267543552, 0.07882845866700354, 0.9898875096962939, 0.33716258166643864, 0.22077412042513012, 0.07932174348958551, 0.0487855825562371, 0.05581948268669986, 0.0508634869984472, 0.049547050643755086, 0.05987204166094812, 0.062479101892789365, 0.03537600047265765, 0.014033729334035332, 0.16723527456392104, 0.10174453767175616, 0.7168896734803049, 0.7304873220957819, 0.04270308711682533, 0.08805149821433896, 0.10845828320822008, 0.029476467213383855, 0.3180080161774916, 0.2057319360661283, 0.06194542350971767, 0.020433386227163813, 0.30133867478164744, 0.09248795871242568, 0.9964575663692572, 0.1934540539280676, 0.07169179645569565, 0.7339874399035506, 0.9980168209732343, 0.9982099395356432, 0.9950255680894302, 0.2579971860667848, 0.7405274880410643, 0.16095447490543208, 0.018120702472796988, 0.06644257573358896, 0.6004703368436649, 0.03375424970422968, 0.003197771024611233, 0.008172081507339817, 0.10232867278755946, 0.006040234157598996, 0.07452255370844264, 0.8610016473100427, 0.010646079101206093, 0.033934377135094416, 0.019961398314761424, 0.006102037446616638, 0.9885300663518953, 0.40427085558499065, 0.26115267463924324, 0.01071395588263562, 0.30180055455883714, 0.0022803352047501488, 0.010460585304330047, 0.009338515600405372, 0.9927314365087843, 0.1760845473109243, 0.00024304285343122744, 0.2916514241174729, 0.07716610596441471, 0.30027944541428153, 0.15457525478226067, 0.20225480023745374, 0.0009185049965370287, 0.434636564361322, 0.06319314376174757, 0.2268707341446461, 0.07182709072919564, 0.28471921732947114, 0.038230848970034995, 0.05634019848215683, 0.43080130339392064, 0.15735012576088087, 0.003219439913266105, 0.0016097199566330524, 0.02756645425734102, 0.004975598971086277, 0.06603976816169059, 0.06377813226574228, 0.1741459639880197, 0.03709082869355225, 0.6513511380331126, 0.0027139630751379694, 0.9973300838366081, 0.9971188587050202, 0.9409862613214327, 0.05869137212741043, 0.05032495788016508, 0.06919681708522699, 0.8034020061583497, 0.07728475674453923, 0.3225414753804667, 0.03995273158595585, 0.003995273158595585, 0.037955095006658064, 0.0023049652838051456, 0.0344208149048235, 0.1611939055141065, 0.025969275530871305, 0.08036645622867274, 0.29134761187297037, 0.2201951356554915, 0.001530904767013846, 0.10078456382841153, 0.011226634958101537, 0.001403329369762692, 0.03355232947705346, 0.1931491514382469, 0.031000821532030383, 0.2832173818975615, 0.12400328612812153, 0.05718674243308987, 0.007478266318173291, 0.12199838385725839, 0.0687707235926132, 0.013490205907292994, 0.25440768749079723, 0.3535313743748197, 0.12331807791340661, 0.4100369010211459, 0.34568556002319056, 0.009686735626217852, 0.1415834223421247, 0.05613070589894885, 0.03680959537962784, 0.09125317442484407, 0.8601097078767217, 0.047891736932187665, 0.9913028251257426, 0.9996677592317706, 0.9980140686352335, 0.13333826617417271, 0.6325481090742426, 0.013477587551297783, 0.08356104281804626, 0.08967088250796791, 0.0025158163429089193, 0.017251312065661162, 0.027673979771998113, 0.11045532226875128, 0.7989436538990843, 0.09014739306238893, 0.996149831572161, 0.00312441669846277, 0.9612788708937122, 0.03541005591591139, 0.2312045356752564, 0.7685988618393658, 0.9989972342273475, 0.7408699543640601, 0.015981786380734733, 0.07648426339351623, 0.16609642274263597, 0.05575378405007757, 0.2621405986915928, 0.4452477631507949, 0.020540867807923316, 0.07414275142098034, 0.026800941806528516, 0.1156157416617398, 0.9945180103083161, 0.998114703621502, 0.8775750331160989, 0.046974819290886066, 0.07473266705368238, 0.9954118257619172, 0.9968382207187076, 0.9974577942327273, 0.4612666443976262, 0.06946132997987782, 0.05851755273044395, 0.28354331964442314, 0.0018088888015593183, 0.016460888094189798, 0.010129777288732184, 9.044444007796592e-05, 0.09071577339819982, 0.008049555166938966, 0.9980238659536089, 0.9972919302169424, 0.9960771178470674, 0.9957166709256366, 0.9949881125870126, 0.2630806670546914, 0.11814270696437532, 0.10815538534470648, 0.4005890342328149, 0.07441772572704466, 0.017782304347215255, 0.0013397626562970397, 0.015468168849974913, 0.0010961694460612144, 0.9968418674243994, 0.7934944143050364, 0.07453917970049816, 0.080701085222406, 0.004770507500831882, 0.046313676987242855, 0.4440554289480854, 0.11411839133526014, 0.11026448693744648, 0.004139378797651712, 0.1941225918898734, 0.010562552794007817, 0.0628043679643708, 0.02198152878752978, 0.02754827958437174, 0.010277078394169768, 0.17096118996392146, 0.0018382923652034565, 0.1650786543952704, 0.5132512283648051, 0.024265459220685626, 0.12426856388775366, 0.672325029785122, 0.2519699924433526, 0.0027485531383762193, 0.0008100998723635174, 0.006943713191687292, 0.031073116532800628, 0.0264439744050091, 0.007667016649154717, 0.7613715516630073, 0.1167904391905535, 0.014891312829396621, 0.01595497803149638, 0.0908370082593194, 0.9312085375104447, 0.016816406997931282, 0.05044922099379385, 0.9937350926779024, 0.9989569509695887, 0.08409395948867149, 0.024581311235150127, 0.8901022173570151, 0.3121098779945258, 0.12865274970892995, 0.01777439305189164, 0.007829196939523698, 0.005713197766679456, 0.5239213951962345, 0.003808798511119637, 0.9934364668005224, 0.7871166576995265, 0.20543937215018204, 0.0067357171196780995, 0.006420506292621769, 0.13253759418340652, 0.0009172151846602528, 0.7323963249512119, 0.127034303075445, 0.12233563871090987, 0.760624605640642, 0.002587158522587218, 0.015892545210178625, 0.06763571566192299, 0.0306763081963913, 0.9958992547086224, 0.00031652238579212445, 0.1022367306108562, 0.03861573106663918, 0.8280225612321975, 0.03070267142183607, 0.9262238468020138, 0.07397954047939406, 0.9976944853270664, 0.14712995124867684, 0.00021431893845400848, 0.17509857271692494, 0.12580521687250298, 0.4110637239547883, 0.14070038309505656, 0.9972205470687434, 0.9961718656993567, 0.9965134497507853, 0.9923311250708362, 0.9989311086681834, 0.9975102186845675, 0.5461571279395308, 0.1535356587435003, 0.017564644674559256, 0.00010332143926211326, 0.2825841363818798, 0.6165687766213566, 0.29846392044301884, 0.041108935259969795, 0.0015008023983798498, 0.0026753434058075583, 0.03967338513978037, 0.9991101020933376, 0.12273792424720903, 0.8510950308758521, 0.002017609713652751, 0.02421131656383301, 0.22269281813624445, 0.3032478160394021, 0.16012761778310602, 0.108982561949775, 6.139862645057746e-05, 0.0512064544597816, 0.06324058524409479, 0.03610239235293955, 0.05433778440876105, 0.3543805585446986, 0.2528755598514173, 0.025228426859689332, 0.25267846276657596, 0.00026279611312176384, 0.06116579532909054, 0.0016424757070110242, 0.03895952377030149, 0.012811310514685988, 0.009387463557466688, 0.32453802584384833, 0.08135801749804462, 0.09052197001842877, 0.3433129529587817, 0.15086995003071463, 0.10622118522605484, 0.8448669655672362, 0.048480438385225026, 0.05065705231442485, 0.04926918786745431, 0.11935634243946677, 0.7799798191974456, 0.9638520192018396, 0.035618271706226994, 0.14961409868879683, 0.06988344312427877, 0.5466791164403807, 0.015564948695862089, 0.19567364074798055, 0.022553293008289964, 0.09779878347972841, 0.9018491845714554, 0.30577834124401365, 0.10031752371754825, 0.30826434401835523, 0.0005849418292568411, 0.013892368444849975, 0.020765434938617856, 0.0016085900304563128, 0.09037351262018195, 0.15837300027128973, 0.27321185433672524, 0.3840007204954548, 0.2725383657886478, 0.014704499966356204, 0.0554505571250379, 0.997487210353781, 0.9997641771440899, 0.9985241196237026, 0.5733804334041228, 0.18692600413034724, 0.02383759823700723, 0.06634104646508746, 0.015842194339557174, 0.03483499393238463, 0.08557162758646361, 0.0021103110658697174, 0.011146009150720339, 0.7965426249464584, 0.1382563774338223, 0.00557672782926342, 0.0025560002550790674, 0.0025560002550790674, 0.0525141870588972, 0.0020912729359737823, 0.2795886154663918, 0.15082110775647928, 0.23110714759542686, 0.24596143192431272, 0.02633673724569336, 0.01312280333349427, 0.020504380208584794, 0.022691514097500507, 0.00975097192141588, 0.5379476253658884, 0.31792088249920336, 0.0002089522724280009, 0.002507427269136011, 0.03421593461008515, 0.00224623692860101, 0.09256585668560441, 0.003447712495062015, 0.008932709646297039, 0.03670805140527583, 0.7293127949010463, 0.02770418973983082, 0.02839679448332659, 0.07410870755404744, 0.06787526486258551, 0.03601544666178006, 0.992037502386866, 0.000577950933046487, 0.007628952316213629, 0.003698885971497517, 0.07975722876041522, 0.8124834216767515, 0.09570867451249825, 0.9949639529802443, 0.9988618350281038, 0.9958939469879755, 0.9964186307598331, 0.9963618515430105, 0.9985294057514492, 0.9984048544772444, 0.8977851736080905, 0.10148231747880232, 0.07311404837392135, 0.05305262035062493, 0.0011596201169535502, 0.03264330629224244, 0.0026671262689931655, 0.0513131901751946, 0.7859905152711164, 0.15048478150748104, 0.015258456915642265, 0.11842802340030602, 0.008959093968817477, 0.1990598691196633, 0.020017975586576548, 0.07195272343706537, 0.3779617768094873, 0.03093687136107285, 0.0069992921631386535, 0.9920803537773465, 0.9971732693170101, 0.9965784678209548, 0.06150969812314868, 0.3635380876252762, 0.574879101689428, 0.04790883681735477, 0.006248978715307144, 0.07082175877348097, 0.8727740272378978, 0.0004663454813187538, 0.08021142278682565, 0.6775999843561492, 0.10399504233408209, 0.07601431345495686, 0.061091258052756746, 0.9944934504947771, 0.9984813168168669, 0.999611900351129, 0.9962437358112018, 0.9915167379666749, 0.9955199011914183, 0.16480518362481952, 0.7807791678112017, 0.021623375156447952, 0.0321428549622875, 0.10713100697242407, 0.8919040771866884, 0.0010251771002145844, 0.5297995096443225, 0.30583159412194105, 0.08543446910390223, 0.0018515210022517512, 0.04741216280766092, 0.007406084009007005, 0.010844623013188828, 0.011373629013832187, 0.0010377342438814819, 0.822144954715104, 0.008042440390081484, 0.09469324975418522, 0.035542397852940756, 0.037617866340703715, 0.0010377342438814819, 0.10738695676823562, 0.030575452968733756, 0.6159835159066849, 0.24609510926053998, 0.15566860670146174, 0.6658912071365644, 0.1781589719756106, 0.9961458868260256, 0.9985076803512236, 0.051367180270317905, 0.12936919475487474, 0.014268661186199419, 0.7495803343150095, 0.05469653454709777, 0.9964226971013947, 0.27357802974040396, 0.2886403748846991, 0.002783259428837144, 0.31221621945837846, 0.11133037715348576, 0.011460480001094122, 0.006880633919294186, 0.031945800339580145, 0.19069185433472458, 0.04324969892127774, 0.08699087169393363, 0.6398989544943593, 0.997679318280743, 0.0876944681836895, 0.05716791280329126, 0.05975804477496142, 0.2673386213616695, 0.10490034485264126, 0.08806448703678525, 0.17298381382225672, 0.16188324822938466, 0.060974239054571236, 0.7936031729256502, 0.06285036948701958, 0.018761304324483457, 0.06378843470324375, 0.09872917472858499, 0.7005070016456745, 0.20059260897236314, 0.9947827206596729, 0.08507028716137337, 0.9140885757731884, 0.38799975872733394, 0.11442961634341293, 0.04635283575941782, 0.03706963840705486, 0.053678352037472954, 0.05746741218129458, 0.03883786647417161, 0.17941199780995373, 0.07496023984527106, 0.009788405371539187, 0.9980577717584586, 0.9978768297536087, 0.6067342869967023, 0.017034726189216347, 0.1918817270747577, 0.09107588598934366, 0.04350721663077726, 2.9219084372583785e-05, 0.0031556611122390485, 0.020131949132710228, 0.026443271357188325, 0.9979158172395238, 0.9950552175833596, 0.09393112839072523, 0.8881867820617605, 0.002840250102527329, 0.015012750541930167, 0.2786769411410391, 0.01674355284221881, 0.0018349099005171296, 0.5965750814056318, 0.008486458289891726, 0.04472592882510504, 0.030046649620968, 0.02270701001889948, 0.9973909202360001, 0.061635680450932366, 0.15208804267113182, 0.19971561392867046, 0.5034914675796943, 0.08324819177788267, 0.824956940495653, 0.07332950582183582, 0.04757688175345301, 0.039283663833126335, 0.014404010072146324, 0.9987035977125486, 0.9967302178083611, 0.11782089956795494, 0.0816132377428585, 0.47201359145377736, 0.07825526910585359, 0.01664384454863304, 0.11563092002208217, 0.07037134274071162, 0.02642575318686474, 0.021169802276770094, 0.9966552161597124, 0.007536065631219905, 0.033912295340489576, 0.007536065631219905, 0.513394471126856, 0.33912295340489573, 0.04286137327756321, 0.056049488132198046, 0.003368013400668296, 0.05430921608577627, 0.000842003350167074, 0.02399709547976161, 0.8344253200155702, 0.08251632831637325, 0.06160556172559999, 0.000774912726108176, 0.6447273881220025, 0.15691982703690563, 0.0193728181527044, 0.07245433989111445, 0.044170025388166036, 0.9952051112834877, 0.9977174585352296, 0.10582141168193003, 0.8939474066352165, 0.4361179515725415, 0.4466222654295131, 0.0012847002558886165, 0.015340832467375831, 0.031361800364339755, 0.024333734258596147, 0.04330195568377513, 0.0015869826690388792, 0.6931012224918626, 0.1916885628750106, 0.04996011288062947, 0.0017161870836857453, 0.0067694046078715505, 0.056681845625065314, 0.46361430166151796, 0.11301848807513605, 0.05939433137781146, 0.20741854584790115, 0.01354067658354251, 0.037929281566400325, 0.00961695780081144, 0.04062202975062753, 0.041083643725066475, 0.013848419233168475, 0.6987365310232249, 0.22155308845080562, 0.014444691717178328, 0.00813785448855117, 0.05716842778207198, 0.3861403551133051, 0.20666106831955922, 0.03976436168875521, 0.016417000754357508, 0.07717126479167707, 0.04933621160954843, 0.028232696799016202, 0.12446245208580381, 0.04152535484925723, 0.030277721114437897, 0.02266234523993286, 0.0038483227765923724, 0.0423315505425161, 0.5575792111862726, 0.3211211561356524, 0.05216615319380771, 0.046101216364188465, 0.7095920177874235, 0.033790096085115406, 0.1458998722434828, 0.06103172393668132, 0.003405203481445739, 0.9794034245344848, 0.019767775541062994, 0.9982255802168779, 0.99448627621101, 0.9979160191115332, 0.997267191555119, 0.9983988808412554, 0.45045216761714446, 0.0006380342317523292, 0.08023280464285541, 0.044821904780601135, 0.2839252331297865, 0.018024467047003303, 0.045300430454415376, 0.024245300806588512, 0.052318807003691, 0.12386502354058324, 0.00043461411768625695, 0.2099186188424621, 0.5419638047547625, 0.02129609176662659, 0.09952663295015285, 0.002607684706117542, 0.08892070233001843, 0.13087719810217324, 0.004060306042466595, 0.015158475891875288, 0.7349153936864538, 0.02598595867178621, 0.21501001412103524, 0.12014239138226175, 0.2166052657908412, 0.09865634545456266, 0.05264330510359685, 0.07467771879279174, 0.09800827446370398, 0.034896130277005485, 0.04691036941523166, 0.04242372409390238, 0.9452663460845996, 0.051201927079582474, 0.6937720248138818, 0.08374863000976246, 0.004878560971442473, 0.21018466851964657, 0.00731784145716371, 0.8925438788012504, 0.051211534029579946, 0.05560109408925822, 0.22050408786717388, 0.06766939052514866, 0.10840062422017657, 0.020550758818854997, 0.49506963136593024, 0.015829638549658576, 0.016847919392034276, 0.05128432606146697, 0.0037954104124912377, 0.9971444644055605, 0.931742677705329, 0.06804427853781657, 0.998690042086538, 0.13366905899626683, 0.06535343189152658, 0.0009256860041292717, 0.03165846134122109, 0.3467619771468252, 0.10052950004843891, 0.3210279062320314, 0.1313964553114997, 0.8683002871615599, 0.9976502642458163, 0.9959863306412905, 0.306195225345332, 0.00806787921236132, 0.07914205322602057, 0.006915325039166845, 0.5424688308501991, 0.0034576625195834225, 0.0476389058253716, 0.005762770865972371, 0.9983837601129799, 0.9977390864599708, 0.9605674616671876, 0.03522080692779688, 0.1701150181041406, 0.7550955151420944, 0.010758262014491104, 0.06320478933513524, 0.028974790110816287, 0.9682409028697776, 0.9984287140908246, 0.0807202847040599, 0.06797497659289255, 0.5435342854908244, 0.012214253606535379, 0.03558065181034219, 0.24003663609365178, 0.0002655272523159865, 0.019383489419067014, 0.9188340384958404, 0.08110767048513094, 0.001051185278488126, 0.17870149734298144, 0.8199245172207383, 0.09535476372508055, 0.9045942184041096, 0.5655210069084936, 0.28830681337421815, 0.0034442837859182773, 0.018133141108216812, 0.12460203107880827, 0.5545213219211766, 0.31805216094300914, 0.0006076945993656731, 7.596182492070913e-05, 7.596182492070913e-05, 0.12662836214282214, 0.455384490635592, 0.17573835979137922, 0.1447761597500479, 0.0205414041825453, 0.0875261084270876, 0.03310639672009389, 0.04991689868159234, 0.014194582013408131, 0.007376036574943198, 0.011492894198167309, 0.06171892028870324, 0.9374288836303039, 0.5346894358344454, 0.0717743828800913, 0.04090405648792344, 0.09984785070898235, 0.04352611139099546, 0.03775759060423702, 0.027514096116235683, 0.04548391238528923, 0.08142354492339632, 0.01709579796802954, 0.997332473257678, 0.9976970608763557, 0.07806564957307377, 0.9200594413969408, 0.0010519510592842652, 0.15726668336299765, 0.06495797791080338, 0.0007889632944631989, 0.3973745126446312, 0.10808797134145826, 0.08389309697792015, 0.026298776482106632, 0.06048718590884525, 0.09967236286718413, 0.005149509236393018, 0.0070805752000404, 0.6552750503310115, 0.09784067549146734, 0.13131248552802197, 0.1034729512187722, 0.005780623133498726, 0.058331742528941695, 0.6227307648359992, 0.3126791604028857, 0.03715814649843663, 0.8050931741327936, 0.1400576291094919, 0.017149913768509214, 0.014898244926700463, 0.10585595079497698, 0.6210215779971983, 0.0373762986757573, 0.2207292603613779, 0.3694950416879, 0.03246326174683197, 0.01668250950878865, 0.09964417841735924, 0.16434526259333684, 0.04801857466718896, 0.26939998463516807, 0.9977283434531437, 0.9983515105566566, 0.3388724046381668, 0.09444797817640202, 0.04528214526742634, 0.29846552683566413, 0.07825217439462179, 0.0016526330389571655, 0.06858427111672237, 0.0004131582597392914, 0.03834108650380624, 0.03569687364147478, 0.12072172247630562, 0.008622980176878974, 0.06476217403676812, 0.0052097171901977135, 0.5643560880346936, 0.12440445359351436, 0.035929084070329055, 0.06143873376026269, 0.014461456338307446, 0.13828286674973322, 0.8362937330111958, 0.005619268436826615, 0.019789597538389384, 0.03715960222055115, 0.9627477223197722, 0.4957682779005051, 0.2463903316612932, 0.005397791044138785, 0.0011548762233971354, 0.11255021977150778, 0.023750280594210655, 0.009088373758038327, 0.08463234237112487, 0.004468868864449785, 0.01682102325382784, 0.17001122388004353, 0.1459292957617287, 0.030371983970113484, 0.01006408936287784, 0.07943441961414295, 0.010603237007317724, 0.10711066536205702, 0.32384801842689054, 0.0274965298664341, 0.09488998542141965, 0.1301962943864093, 0.0569371549794793, 0.09246593968667438, 0.0012146593062288917, 0.13589000988435726, 0.022319364751955884, 0.0252041806042495, 0.38588207834759103, 0.07591620663930573, 0.07386646906004447, 0.26750843162486265, 0.0915624988139653, 0.13757912739527692, 0.27574670272637625, 0.11615962253134159, 0.0274216738093238, 0.03142311977291611, 0.04130904509473242, 0.004707583486579193, 0.00659061688121087, 0.010866246670582318, 0.003685074957849656, 0.5312177285392504, 0.0381735969992631, 0.03722870598442986, 0.07143376072139333, 0.09004811371360825, 0.024567166385664373, 0.19275776702598202, 0.9994480639284278, 0.9992414468134903]}, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"tinfo\": {\"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.4576, -8.6529, -9.0318, -9.1524, -9.1926, -9.2764, -9.3972, -9.4024, -9.4485, -9.4505, -9.4559, -9.4558, -9.4618, -9.4689, -9.4758, -9.5032, -9.5235, -9.5419, -9.5642, -9.5714, -9.5876, -9.5959, -9.6033, -9.6726, -9.678, -9.6973, -9.7356, -9.7401, -9.7503, -9.7788, -9.7182, -6.2639, -6.2885, -6.1823, -6.3348, -6.4771, -6.8135, -6.9302, -4.4209, -4.8898, -6.2916, -5.7351, -4.2601, -6.1089, -5.6397, -7.0743, -4.5334, -6.7154, -6.3419, -8.7318, -4.9701, -4.607, -5.3208, -5.5144, -6.3389, -5.0407, -4.8525, -6.9074, -5.1539, -5.8695, -5.5414, -4.8392, -5.2827, -4.8478, -4.5837, -4.2705, -4.8834, -5.2347, -3.8299, -4.3038, -5.2607, -5.3539, -5.4857, -5.204, -4.957, -5.5788, -5.1535, -5.1981, -5.3575, -5.0983, -5.2934, -5.3134, -5.3262, -6.3016, -6.9498, -7.0708, -7.2268, -7.8429, -7.8523, -7.911, -7.9161, -7.9426, -7.9489, -7.9648, -7.9911, -8.0028, -8.0365, -8.1234, -8.1823, -8.2632, -8.2894, -8.3422, -8.3782, -8.4659, -8.4832, -8.5577, -8.5992, -8.6127, -8.627, -8.633, -8.6598, -8.6713, -8.6966, -7.1316, -4.9933, -5.2882, -6.3076, -7.1981, -5.7921, -6.6196, -7.3646, -5.9233, -6.0716, -5.8882, -5.8075, -7.1938, -7.0977, -4.2462, -7.525, -6.0536, -6.3555, -6.8586, -7.552, -6.9996, -6.5546, -7.0265, -6.6944, -6.5624, -6.8061, -6.0311, -5.6765, -4.5646, -6.0256, -6.4315, -5.4377, -6.2067, -4.376, -5.2374, -5.2725, -6.336, -5.5074, -5.9634, -4.4446, -5.0876, -5.7943, -5.2283, -5.1804, -5.3966, -5.0002, -5.3236, -4.4814, -5.1197, -5.3699, -5.6567, -5.3079, -5.478, -5.2995, -5.2692, -5.5449, -5.4451, -5.6441, -5.6868, -5.7526, -7.0096, -7.8903, -7.9545, -8.0288, -8.0747, -8.0865, -8.1211, -8.1292, -8.3223, -8.3339, -8.3944, -8.5338, -8.563, -8.5878, -8.6095, -8.6107, -8.6109, -8.6137, -8.6439, -8.6456, -8.654, -8.695, -8.722, -8.7368, -8.7444, -8.7485, -8.7959, -8.8002, -8.8244, -8.8349, -8.6645, -5.465, -5.3704, -5.6887, -7.6554, -6.1559, -6.9004, -6.0422, -5.8109, -6.0022, -6.943, -5.8455, -7.108, -5.4382, -5.2837, -6.6857, -6.5811, -7.731, -7.56, -6.0563, -5.3532, -5.1156, -5.937, -6.7901, -6.4683, -4.8423, -6.523, -5.7877, -6.3329, -5.9156, -5.4388, -6.2536, -5.7805, -5.6687, -5.783, -6.1256, -5.1864, -6.0984, -5.7746, -4.9601, -5.3731, -5.8759, -5.8501, -5.6106, -5.0264, -5.6511, -5.7677, -5.6254, -5.6696, -5.7207, -5.8258, -5.7893, -5.8566, -5.8107, -7.0979, -7.321, -7.4902, -7.5647, -7.8546, -8.0087, -8.082, -8.1259, -8.132, -8.2283, -8.2474, -8.2507, -8.2536, -8.2858, -8.2952, -8.3129, -8.3195, -8.3411, -8.3607, -8.4231, -8.4922, -8.4927, -8.4932, -8.5062, -8.5126, -8.5363, -8.5433, -8.5438, -8.582, -8.5937, -6.7741, -7.9852, -7.2722, -8.3026, -7.3739, -6.6655, -6.8229, -7.7483, -8.2097, -5.8252, -6.2017, -3.1101, -4.5672, -5.0873, -6.2563, -6.7184, -6.4314, -7.2195, -6.2386, -6.0665, -6.0086, -6.0139, -6.6145, -5.9733, -5.5906, -5.9591, -5.9199, -6.1953, -6.0202, -6.2134, -6.0996, -5.9251, -5.4971, -5.1873, -5.4343, -5.6387, -5.906, -5.7885, -5.8454, -5.894, -5.3739, -5.5444, -5.9298, -5.7894, -5.8592, -5.9281, -5.6445, -5.4516, -5.7865, -5.7319, -5.74, -5.8019, -5.9455, -6.1636, -6.6108, -7.0847, -7.1722, -7.1885, -7.2632, -7.2809, -7.3431, -7.5657, -7.5771, -7.6267, -7.633, -7.7283, -7.7293, -7.871, -7.9049, -7.9104, -7.9218, -7.9453, -7.9499, -7.97, -7.9922, -7.9941, -8.0044, -8.0358, -8.1076, -8.152, -8.1971, -8.2153, -8.2419, -5.661, -4.9742, -6.8592, -6.2859, -6.1345, -7.0905, -6.858, -7.6395, -6.5384, -7.0627, -6.6486, -7.1822, -6.128, -6.3985, -5.7104, -5.2733, -4.8503, -6.0533, -5.0114, -6.8558, -5.8213, -6.3141, -4.5673, -6.7227, -6.3432, -5.1759, -6.3831, -4.1705, -6.3548, -6.4678, -5.3868, -6.0398, -5.1879, -5.6874, -6.2751, -6.1333, -5.2236, -6.1113, -5.6886, -6.0217, -5.8946, -5.9256, -5.8611, -5.8854, -5.9607, -5.9748, -5.8995, -6.0764, -6.1059, -6.1191, -5.8264, -6.3243, -6.3742, -6.6112, -6.6447, -6.8227, -6.9366, -7.0084, -7.0107, -7.0225, -7.0465, -7.0619, -7.0644, -7.095, -7.1012, -7.1972, -7.2811, -7.2905, -7.0135, -7.3364, -7.365, -7.4259, -7.4315, -7.4434, -7.5231, -7.5334, -7.572, -7.6151, -7.6267, -7.6439, -6.4555, -6.3646, -6.4249, -6.1498, -5.1654, -5.903, -6.8111, -5.4269, -4.4661, -6.3419, -6.2232, -5.887, -6.2253, -6.631, -6.4544, -5.9507, -5.7229, -6.1585, -6.5903, -6.6471, -6.1797, -6.6451, -6.0978, -6.1675, -6.1753, -5.4496, -6.1418, -6.4851, -5.6964, -6.2561, -6.0267, -6.0821, -4.9706, -5.6001, -5.9737, -5.49, -5.2763, -6.1122, -5.7901, -5.8668, -5.9379, -6.016, -6.0508, -6.0121, -6.0619, -6.0994, -6.115, -6.1153, -5.9994, -6.0419, -6.5128, -6.5721, -6.5868, -6.6428, -6.6563, -6.6814, -6.8335, -7.046, -7.0683, -7.1692, -7.1997, -7.2512, -7.2999, -7.3227, -7.4433, -7.4464, -7.5055, -7.517, -7.5638, -7.5785, -7.5793, -7.6463, -7.6611, -7.6857, -7.7528, -7.7567, -7.7678, -7.7863, -6.1327, -6.2975, -4.3327, -7.2955, -6.4695, -6.3859, -4.1914, -5.5431, -7.2027, -5.6449, -6.475, -6.4321, -6.0231, -6.4952, -6.538, -6.5411, -5.4374, -6.5996, -6.4391, -6.461, -5.8954, -6.0611, -5.7963, -6.1278, -5.5437, -5.8975, -6.2098, -5.7757, -5.3356, -4.9814, -5.0866, -5.3846, -5.5371, -5.5943, -5.8278, -5.7505, -5.9382, -5.8334, -5.6508, -5.9119, -5.6973, -5.8817, -5.7564, -5.7351, -5.8649, -5.721, -5.8801, -5.9868, -4.7655, -5.4969, -5.6441, -5.7692, -6.457, -6.7097, -6.8927, -7.0096, -7.0579, -7.1758, -7.2367, -7.2772, -7.4069, -7.424, -7.4582, -7.5272, -7.7234, -7.7509, -7.8173, -7.8407, -7.8678, -7.9259, -7.9531, -7.9742, -8.0238, -8.0245, -8.0539, -8.0672, -8.1477, -8.1563, -5.6358, -5.8618, -3.6102, -4.267, -5.5332, -4.5252, -6.1982, -6.6619, -6.8404, -6.5012, -4.8706, -5.8049, -5.715, -5.3105, -4.5912, -5.3666, -6.3663, -5.224, -5.1167, -6.3614, -5.1867, -5.8892, -5.211, -5.6837, -4.6697, -5.5893, -5.1792, -5.6282, -4.7396, -5.1731, -4.9691, -5.2434, -5.3298, -4.6825, -5.0018, -4.9822, -5.1597, -5.317, -5.3859, -5.3897, -5.4166, -5.5758, -6.4918, -6.5906, -6.6334, -6.7084, -6.9675, -6.9811, -7.035, -7.0512, -7.1268, -7.1773, -7.1904, -7.2164, -7.2336, -7.249, -7.2878, -7.3815, -7.3905, -7.407, -7.4089, -7.4095, -7.5235, -7.5281, -7.5463, -7.5461, -7.5465, -7.564, -7.5906, -7.6078, -7.6211, -7.6242, -6.3343, -7.0338, -6.74, -6.4892, -6.4262, -5.8913, -5.2471, -6.3719, -5.8073, -5.9701, -6.4167, -6.8105, -6.9115, -6.029, -5.513, -5.7404, -4.8296, -6.4226, -5.5283, -5.8437, -5.6227, -4.8642, -6.0392, -5.9216, -4.9548, -5.3288, -6.0494, -5.9794, -5.3042, -5.4916, -5.4114, -4.86, -6.0408, -4.9688, -5.1328, -4.8462, -4.6317, -4.7329, -5.9865, -5.3631, -5.8193, -5.625, -5.699, -5.8173, -5.8563, -5.8288, -5.9194, -6.0349, -6.2269, -6.5566, -6.563, -6.7765, -6.9355, -7.0092, -7.0231, -7.0911, -7.1503, -7.174, -7.1747, -7.2524, -7.2768, -7.2848, -7.3145, -7.3185, -7.4131, -7.4477, -7.4719, -7.5111, -7.5404, -7.5501, -7.5517, -7.5562, -7.5585, -7.5919, -7.5945, -7.6515, -7.6689, -6.6959, -5.7669, -6.9221, -7.1359, -6.3084, -6.8604, -6.5394, -6.9964, -6.6335, -6.2955, -6.1718, -6.2075, -5.4643, -5.7453, -5.4957, -5.9451, -6.8786, -6.386, -6.496, -5.795, -5.765, -6.4995, -6.2018, -6.153, -5.7122, -5.3946, -4.7416, -5.4592, -5.3698, -6.1353, -5.2966, -5.4851, -6.3261, -5.831, -5.0009, -5.7688, -5.7373, -5.9292, -5.694, -5.3335, -5.9916, -6.0377, -5.9153, -5.9452, -6.0576, -6.0364, -6.0892, -6.1052], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Term\": [\"film\", \"show\", \"bad\", \"funny\", \"good\", \"great\", \"comedy\", \"people\", \"series\", \"action\", \"love\", \"really\", \"films\", \"like\", \"tv\", \"watch\", \"horror\", \"would\", \"story\", \"years\", \"role\", \"characters\", \"saw\", \"think\", \"dvd\", \"best\", \"worst\", \"cast\", \"war\", \"first\", \"it.i\", \"ringu\", \"kidman\", \"jolie\", \"puzzles\", \"clerks\", \"nikita\", \"pokemon\", \"falcon\", \"bale\", \"donnie\", \"percent\", \"shyamalan\", \"crouching\", \"imo\", \"93\", \"banderas\", \"sobieski\", \"amelie\", \"angelina\", \"boulevard\", \"oliveira\", \"gere\", \"lithgow\", \"segal\", \"sizemore\", \"cube\", \"filipino\", \"indigo\", \"milla\", \"film.it\", \"liked\", \"felt\", \"read\", \"seemed\", \"enjoyed\", \"disappointed\", \"surprised\", \"really\", \"think\", \"recommend\", \"feel\", \"good\", \"book\", \"thought\", \"reviews\", \"story\", \"feeling\", \"definitely\", \"movie.i\", \"movies\", \"see\", \"say\", \"lot\", \"understand\", \"characters\", \"great\", \"opinion\", \"watch\", \"done\", \"actors\", \"well\", \"better\", \"could\", \"would\", \"like\", \"much\", \"seen\", \"film\", \"one\", \"character\", \"know\", \"something\", \"way\", \"time\", \"watching\", \"people\", \"first\", \"many\", \"even\", \"make\", \"made\", \"get\", \"gore\", \"zombie\", \"slasher\", \"zombies\", \"vampires\", \"blah\", \"godzilla\", \"atrocious\", \"scares\", \"cannibal\", \"freddy\", \"chainsaw\", \"satan\", \"werewolf\", \"13th\", \"candyman\", \"horrid\", \"1/10\", \"mst3k\", \"stinks\", \"stinker\", \"crocodile\", \"hellraiser\", \"corman\", \"troma\", \"2/10\", \"hooper\", \"troll\", \"norris\", \"lime\", \"garbage\", \"horror\", \"worst\", \"crap\", \"trash\", \"awful\", \"lame\", \"porn\", \"waste\", \"horrible\", \"terrible\", \"stupid\", \"laughable\", \"vampire\", \"bad\", \"gory\", \"worse\", \"scary\", \"nudity\", \"massacre\", \"pointless\", \"cheap\", \"pathetic\", \"avoid\", \"ridiculous\", \"poorly\", \"boring\", \"minutes\", \"even\", \"poor\", \"blood\", \"nothing\", \"budget\", \"like\", \"acting\", \"ever\", \"oh\", \"thing\", \"money\", \"one\", \"could\", \"guy\", \"make\", \"get\", \"plot\", \"would\", \"movies\", \"film\", \"really\", \"made\", \"actually\", \"people\", \"seen\", \"time\", \"good\", \"watch\", \"see\", \"know\", \"scene\", \"something\", \"holmes\", \"sherlock\", \"streep\", \"sadako\", \"shields\", \"spaghetti\", \"kurosawa\", \"sharpe\", \"dodge\", \"meryl\", \"madame\", \"frogs\", \"lewton\", \"cattle\", \"leone\", \"val\", \"architect\", \"fort\", \"trevor\", \"harding\", \"saloon\", \"flee\", \"singapore\", \"snowblood\", \"isolation\", \"liv\", \"housing\", \"leung\", \"teresa\", \"capitalist\", \"patients\", \"mother\", \"father\", \"son\", \"refuses\", \"begins\", \"apartment\", \"finds\", \"daughter\", \"husband\", \"hospital\", \"town\", \"asks\", \"wife\", \"woman\", \"decides\", \"prison\", \"prostitute\", \"emily\", \"child\", \"family\", \"young\", \"becomes\", \"sees\", \"sister\", \"man\", \"eventually\", \"house\", \"tells\", \"boy\", \"girl\", \"brother\", \"home\", \"takes\", \"death\", \"wants\", \"life\", \"lives\", \"goes\", \"story\", \"two\", \"comes\", \"away\", \"end\", \"one\", \"back\", \"find\", \"way\", \"character\", \"little\", \"old\", \"get\", \"around\", \"also\", \"khan\", \"bollywood\", \"chaney\", \"miike\", \"berry\", \"ali\", \"fellini\", \"lancaster\", \"bernie\", \"kapoor\", \"farrow\", \"bachchan\", \"altman\", \"akbar\", \"rani\", \"rathbone\", \"lon\", \"shah\", \"leopard\", \"amitabh\", \"takashi\", \"rukh\", \"farrah\", \"hai\", \"jeanne\", \"hayward\", \"saif\", \"kinski\", \"mesmerizing\", \"granny\", \"narrative\", \"visitor\", \"largely\", \"jarring\", \"lynch\", \"visual\", \"cinematic\", \"imagery\", \"underlying\", \"style\", \"cinematography\", \"film\", \"films\", \"director\", \"particularly\", \"effective\", \"elements\", \"pacing\", \"score\", \"cinema\", \"direction\", \"often\", \"feature\", \"camera\", \"rather\", \"production\", \"performances\", \"genre\", \"perhaps\", \"despite\", \"dialogue\", \"audience\", \"work\", \"characters\", \"scenes\", \"quite\", \"performance\", \"seems\", \"however\", \"script\", \"much\", \"character\", \"yet\", \"though\", \"cast\", \"may\", \"story\", \"one\", \"plot\", \"well\", \"even\", \"also\", \"many\", \"political\", \"americans\", \"nazi\", \"propaganda\", \"politics\", \"religion\", \"countries\", \"cultural\", \"documentaries\", \"civil\", \"iraq\", \"nation\", \"nazis\", \"racism\", \"highlander\", \"che\", \"asoka\", \"historically\", \"jews\", \"hitler\", \"soviet\", \"germans\", \"beliefs\", \"communist\", \"conditions\", \"cultures\", \"9/11\", \"muslim\", \"economic\", \"educated\", \"documentary\", \"war\", \"media\", \"culture\", \"society\", \"freedom\", \"facts\", \"liberal\", \"german\", \"religious\", \"historical\", \"united\", \"america\", \"social\", \"history\", \"american\", \"world\", \"country\", \"us\", \"states\", \"human\", \"message\", \"people\", \"government\", \"view\", \"life\", \"gay\", \"film\", \"reality\", \"truth\", \"many\", \"black\", \"would\", \"real\", \"white\", \"men\", \"one\", \"true\", \"time\", \"fact\", \"also\", \"make\", \"even\", \"see\", \"made\", \"way\", \"like\", \"man\", \"years\", \"could\", \"alien\", \"aliens\", \"martial\", \"shark\", \"kong\", \"dragon\", \"batman\", \"ninja\", \"fu\", \"seagal\", \"karloff\", \"lugosi\", \"predator\", \"kung\", \"chan\", \"superman\", \"virus\", \"rats\", \"woo\", \"robots\", \"jet\", \"bullets\", \"dinosaurs\", \"helicopter\", \"li\", \"ants\", \"sharks\", \"lab\", \"terminator\", \"bela\", \"scientist\", \"plane\", \"arts\", \"ship\", \"game\", \"sci-fi\", \"robot\", \"fight\", \"action\", \"computer\", \"giant\", \"space\", \"science\", \"fights\", \"planet\", \"earth\", \"hero\", \"fighting\", \"wars\", \"guns\", \"island\", \"flying\", \"monster\", \"battle\", \"gun\", \"effects\", \"crew\", \"captain\", \"special\", \"team\", \"evil\", \"car\", \"one\", \"back\", \"star\", \"get\", \"like\", \"guys\", \"time\", \"also\", \"two\", \"world\", \"big\", \"man\", \"gets\", \"around\", \"guy\", \"good\", \"charlie\", \"comedies\", \"murphy\", \"woody\", \"sandler\", \"gags\", \"slapstick\", \"chaplin\", \"keaton\", \"rachel\", \"quaid\", \"comedian\", \"baseball\", \"lou\", \"lenny\", \"costello\", \"porky\", \"buscemi\", \"travolta\", \"comedians\", \"arkin\", \"carrey\", \"daffy\", \"cassavetes\", \"pryor\", \"buster\", \"meg\", \"panther\", \"abbott\", \"brody\", \"allen\", \"bob\", \"comedy\", \"coach\", \"eddie\", \"funniest\", \"funny\", \"hilarious\", \"football\", \"jokes\", \"jim\", \"ben\", \"laughs\", \"williams\", \"dennis\", \"adam\", \"humor\", \"comedic\", \"steve\", \"billy\", \"jack\", \"tom\", \"laugh\", \"brothers\", \"fun\", \"school\", \"comic\", \"plays\", \"great\", \"one\", \"like\", \"get\", \"love\", \"best\", \"gets\", \"new\", \"john\", \"big\", \"two\", \"played\", \"little\", \"guy\", \"character\", \"also\", \"cast\", \"good\", \"make\", \"go\", \"episode\", \"episodes\", \"season\", \"disney\", \"cartoons\", \"bambi\", \"seasons\", \"nikki\", \"aired\", \"beatles\", \"pammy\", \"snoopy\", \"sopranos\", \"tarzan\", \"album\", \"bands\", \"looney\", \"laughton\", \"abc\", \"programme\", \"enid\", \"nbc\", \"cinderella\", \"newton\", \"mimi\", \"contestants\", \"canceled\", \"aladdin\", \"reruns\", \"fry\", \"animation\", \"cartoon\", \"show\", \"series\", \"television\", \"tv\", \"animated\", \"vhs\", \"network\", \"anime\", \"dvd\", \"songs\", \"song\", \"remember\", \"years\", \"kids\", \"band\", \"shows\", \"music\", \"channel\", \"original\", \"ago\", \"saw\", \"children\", \"first\", \"version\", \"old\", \"year\", \"time\", \"still\", \"great\", \"back\", \"new\", \"one\", \"would\", \"like\", \"see\", \"watch\", \"best\", \"love\", \"ever\", \"made\", \"flynn\", \"cary\", \"dunne\", \"fonda\", \"connery\", \"irene\", \"errol\", \"hopkins\", \"heist\", \"hudson\", \"mitchum\", \"bronson\", \"pacino\", \"giallo\", \"clint\", \"sutherland\", \"caine\", \"columbo\", \"niro\", \"loy\", \"walsh\", \"burton\", \"stevens\", \"troy\", \"stuart\", \"gable\", \"randolph\", \"sinatra\", \"freeman\", \"liam\", \"dean\", \"mitchell\", \"donald\", \"ford\", \"wayne\", \"scott\", \"robert\", \"douglas\", \"harry\", \"detective\", \"anthony\", \"stewart\", \"westerns\", \"grant\", \"james\", \"richard\", \"john\", \"noir\", \"michael\", \"peter\", \"murder\", \"role\", \"william\", \"crime\", \"cast\", \"plays\", \"cop\", \"supporting\", \"performance\", \"actor\", \"played\", \"great\", \"de\", \"also\", \"best\", \"good\", \"one\", \"film\", \"david\", \"well\", \"excellent\", \"man\", \"character\", \"made\", \"films\", \"time\", \"first\", \"crawford\", \"cagney\", \"marie\", \"bette\", \"welles\", \"franco\", \"stanwyck\", \"norma\", \"peterson\", \"belle\", \"bloom\", \"humphrey\", \"charlotte\", \"violin\", \"emmanuelle\", \"doris\", \"orson\", \"cabaret\", \"1932\", \"victoria\", \"pianist\", \"maris\", \"midler\", \"shearer\", \"delpy\", \"reilly\", \"ballet\", \"vienna\", \"marple\", \"ladd\", \"mgm\", \"joan\", \"von\", \"diana\", \"julie\", \"broadway\", \"arthur\", \"maria\", \"anne\", \"ms\", \"davis\", \"mrs\", \"novel\", \"stage\", \"french\", \"dance\", \"sir\", \"paris\", \"powell\", \"musical\", \"king\", \"opera\", \"henry\", \"mary\", \"actress\", \"beautiful\", \"love\", \"version\", \"play\", \"beauty\", \"young\", \"woman\", \"singing\", \"wonderful\", \"one\", \"performance\", \"role\", \"screen\", \"best\", \"film\", \"book\", \"played\", \"great\", \"time\", \"music\", \"years\", \"old\", \"man\"], \"Total\": [117780.0, 17247.0, 26937.0, 12913.0, 43106.0, 27111.0, 9276.0, 27627.0, 8651.0, 10291.0, 18439.0, 34563.0, 20838.0, 58696.0, 7388.0, 19742.0, 10296.0, 39831.0, 34224.0, 13172.0, 9331.0, 21505.0, 9678.0, 20976.0, 6856.0, 18854.0, 7642.0, 11197.0, 6041.0, 25763.0, 411.1619459902239, 338.3876415276549, 231.9502485306796, 205.7004333886461, 197.6248461440067, 181.81357906466027, 161.23092022440434, 160.39498321800053, 153.2031198864732, 152.89700627637708, 152.09035647986144, 152.10558913197454, 151.19743015661476, 150.13290290936794, 149.10354988493413, 145.09907995459682, 142.206582472431, 139.6255361983699, 136.56920395546584, 135.597967726782, 133.43446377890965, 132.33826946679218, 131.36271473140332, 122.62664462947401, 121.97119535186029, 119.65890769898536, 115.19444174958184, 114.6841519715893, 113.5259471471911, 110.36664141946034, 117.33236881109323, 4373.762196606212, 4303.674570455761, 5030.91128057442, 4303.5989445391215, 3700.0306360546574, 2579.47056681557, 2291.0286673440733, 34563.639564968995, 20976.734029885076, 4700.7272496360765, 8629.82428250586, 43106.38963449038, 5803.274106565624, 9830.60094187588, 2078.4720841526464, 34224.20727660783, 3152.1324231188587, 4780.23052147643, 337.76329847957595, 22363.771438157808, 33644.329098345006, 15325.13542410981, 12332.576157643029, 4919.483458439544, 21505.0395839289, 27111.237331427797, 2646.1787104726013, 19742.856345929864, 8690.973217811776, 12705.437838745636, 28603.52005296668, 17134.255146089858, 28421.302983551614, 39831.10836301429, 58696.0717201223, 28366.870633505216, 19143.12753587443, 117780.37701923442, 77481.90760339168, 20379.7788908004, 18071.648829989565, 15122.701803515833, 23318.75638798938, 35207.40533843147, 13164.50731724554, 27627.517160093474, 25763.07444324553, 19847.371080418423, 36823.311697243094, 23663.053765612334, 23169.47711345921, 27411.55080725793, 2671.8513526094557, 1397.7714804999027, 1238.4806539069148, 1059.8039176388004, 572.7502787600994, 567.4149839244127, 535.1063652305005, 532.3765332333838, 518.4613776946958, 515.226600351624, 507.1191307304735, 493.9772262184675, 488.21554995417955, 472.08718805514343, 432.8592415426139, 408.1641616444314, 376.5297861771843, 366.81112735300917, 347.9817739760843, 335.7127753760118, 307.5974244047042, 302.33410213391033, 280.6913532868973, 269.32888251651804, 265.7261787453011, 261.9617042241368, 260.40166429522753, 253.5283482324009, 250.6552245612776, 244.40966523370506, 1181.465484612873, 10296.403313642335, 7642.708291504089, 2733.0319506557757, 1112.9223899927472, 4768.90079437976, 2062.575734092741, 960.1792236855015, 4310.21989824193, 3717.0547536816907, 4517.044257893063, 4929.143383374032, 1159.8758454830197, 1293.2755242763396, 26937.214948292185, 824.8280479215091, 4093.059489606596, 2973.81597610243, 1729.6827618363645, 805.8595678160237, 1545.1517263777746, 2592.780880826802, 1502.9007250366346, 2222.5222444190535, 2705.67108234243, 2018.915842347676, 5456.024626874435, 8550.163800002803, 36823.311697243094, 5564.794123172148, 3302.85416126811, 12613.875255019926, 4565.142260575695, 58696.0717201223, 17871.823940354545, 17448.647190515505, 3825.8233653720918, 13232.658686007066, 6786.310970185343, 77481.90760339168, 28421.302983551614, 9192.374514863917, 23663.053765612334, 27411.55080725793, 19098.281107134182, 39831.10836301429, 22363.771438157808, 117780.37701923442, 34563.639564968995, 23169.47711345921, 12415.110430366021, 27627.517160093474, 19143.12753587443, 35207.40533843147, 43106.38963449038, 19742.856345929864, 33644.329098345006, 18071.648829989565, 16287.009299873269, 15122.701803515833, 846.7459314039087, 351.5169387884505, 329.6871282290058, 306.1720677946235, 292.46652667057447, 289.03396203016587, 279.22966743485466, 276.992010666774, 228.5233276188446, 225.88921071326635, 212.68338791303452, 185.1287603752236, 179.82459483696968, 175.43609769595062, 171.6952818735151, 171.48969492794632, 171.45965868918813, 170.9743300432606, 165.91480842616505, 165.6401760504789, 164.25968699547187, 157.69347280302975, 153.5252917259376, 151.28337652434217, 150.14122062399716, 149.53588357974374, 142.64344259719627, 142.03946869242822, 138.6648826813453, 137.2320252572011, 163.8796891609481, 4789.4024546721275, 5341.336555274529, 3854.5514167853116, 475.7258789576241, 2359.767591489501, 1084.9382155039184, 2802.302404658856, 3606.824661713549, 3080.5481196138267, 1108.8421097435805, 3817.6866876926315, 931.8114524355389, 6214.184406903686, 7651.908030837278, 1550.283213399605, 1751.994384917611, 468.33602198164033, 574.9824913193465, 3424.8971878153766, 7949.626796229744, 10583.231127205507, 4004.961508837312, 1443.8249367925484, 2144.332989294016, 16156.186061118477, 2028.4101691430856, 5094.564140874451, 2580.9358042737895, 4403.6502575635295, 8186.884347664001, 2884.175063296279, 5362.381162455193, 6849.37904021477, 5925.935453547567, 3766.091771288191, 17274.999347820096, 4139.547589931896, 7285.907722673211, 34224.20727660783, 20059.530797352025, 7314.926418942777, 8233.919442620987, 15510.466906491198, 77481.90760339168, 14211.460658409536, 12529.328406206332, 23318.75638798938, 20379.7788908004, 19242.312416693338, 11708.969268307595, 27411.55080725793, 10654.146693661227, 27039.283039608676, 729.4831067064887, 583.8083065352934, 493.047596921356, 457.7508527832355, 342.7522669518139, 293.9456885587883, 273.2281391207897, 261.5336947403882, 259.9473688343316, 236.16396175210306, 231.718894009427, 230.9641338114227, 230.28063148443664, 223.00962403380427, 220.94638607936926, 217.08802071854964, 215.65711552140863, 211.0629228033648, 206.98738085019266, 194.51436377432594, 181.59377208206647, 181.5044831677769, 181.4159852645383, 179.0859587041445, 177.94062824898938, 173.80723397417927, 172.60178479579469, 172.51357369224664, 166.06950268850252, 164.1564933949263, 1120.2302309522743, 312.315388530142, 704.3335067768601, 226.51514439160715, 651.166894341451, 1487.2290690121145, 1243.6015702972463, 433.3249020644575, 253.89669376690202, 4359.8870973149, 2810.681940753582, 117780.37701923442, 20838.657882423533, 12053.627033974426, 2814.4604259444027, 1603.1125203239094, 2346.9556996166934, 878.7616312409324, 3148.0990369744395, 3991.3048711021547, 4322.7460472264775, 4322.245838516314, 1999.1987358429612, 4749.807195297651, 8210.40946939275, 5111.760660837217, 5443.628525540013, 3741.3546445627826, 4969.808547775655, 3825.0583264002485, 4527.514794728472, 6173.216826119369, 12101.900136657245, 21505.0395839289, 15220.92527352808, 11056.511590297523, 8229.001477576585, 10973.265112680498, 9770.71504510661, 8908.837451101846, 28366.870633505216, 20379.7788908004, 8496.928437708144, 12997.873401238487, 11197.335585160883, 9886.847858033874, 34224.20727660783, 77481.90760339168, 19098.281107134182, 28603.52005296668, 36823.311697243094, 27039.283039608676, 19847.371080418423, 1690.5616735091457, 1081.373856979514, 673.536006599573, 617.1635361797006, 607.2058691804758, 563.587849760244, 553.7103674426845, 520.3793119496531, 416.69032838841736, 411.99336953017956, 392.0945534339452, 389.6572141853548, 354.2869809282927, 353.95854443864937, 307.2911539463855, 297.1017125167194, 295.4733266127202, 292.1157772383426, 285.35598142281765, 284.07112063391264, 278.4154848986378, 272.33509507374464, 271.8163619986065, 269.03928907712935, 260.7396694242971, 242.75254584527443, 232.25197308656456, 222.03697524933457, 218.05017207604448, 212.34766094962814, 2903.8367774976386, 6041.352156573517, 886.2975740723375, 1618.7840581204634, 1950.8824373675252, 710.475379841859, 912.4539992042323, 397.70024511837903, 1391.0810556453166, 772.944934395155, 1251.4387502857358, 683.4397887746126, 2301.9395504946046, 1711.1112271928018, 3889.619982924907, 6544.662594428299, 11133.041944988736, 2674.705360721039, 10802.520819636038, 1066.0239636910558, 4282.998223386818, 2249.9676951718516, 27627.517160093474, 1366.262402978055, 2602.9145265120655, 17274.999347820096, 2524.7374175881155, 117780.37701923442, 2719.9155556774645, 2300.8916629852524, 19847.371080418423, 6060.7063047960055, 39831.10836301429, 14011.764285236146, 3802.4582652367417, 5687.54255498182, 77481.90760339168, 6269.2561009057445, 35207.40533843147, 10533.413582537054, 27039.283039608676, 23663.053765612334, 36823.311697243094, 33644.329098345006, 23169.47711345921, 23318.75638798938, 58696.0717201223, 16156.186061118477, 13172.417910067816, 28421.302983551614, 1675.4867485507882, 1018.7054864238528, 969.1818193622357, 764.8705488667551, 739.7425154756088, 619.257332858838, 552.6872514170244, 514.4733929106998, 513.2518457467103, 507.2746745499984, 495.2623854227695, 487.7144659248327, 486.4878333480933, 471.8659815294961, 468.91791476142674, 426.11175956909415, 391.88602040969243, 388.22606939645834, 512.1634594757787, 370.8550116709383, 360.4014023189126, 339.1779951050524, 337.29710531907057, 333.2945362474569, 307.8437039614818, 304.6977986660135, 293.20391080108215, 280.8603895153127, 277.63371045613417, 272.91027098092167, 926.4907705847711, 1039.335046854551, 976.73199884042, 1349.9888788863798, 3914.606689259972, 1835.7919805263925, 675.8625381557591, 3325.062790854414, 10291.43047798963, 1194.8696854412246, 1382.2102722216252, 2102.5098016214624, 1441.0629253927766, 873.2542760663357, 1112.767945744703, 2231.6209364249667, 3091.4854215838886, 1718.0985285581667, 951.307082076203, 885.4302089106112, 1818.3941202051744, 896.6282463648139, 2095.0838712847026, 1936.0781882272875, 1953.4587433878148, 6655.412613774031, 2147.5816327666316, 1183.925092511286, 6107.946612473226, 2123.1237601907656, 3884.3393752849397, 3414.6035048121194, 77481.90760339168, 14211.460658409536, 5405.129990721129, 27411.55080725793, 58696.0717201223, 4168.709423176691, 35207.40533843147, 27039.283039608676, 20059.530797352025, 11133.041944988736, 9920.259289520634, 16156.186061118477, 9210.255278574628, 10654.146693661227, 9192.374514863917, 43106.38963449038, 1388.0196396992305, 1330.332780501083, 831.0619793939876, 783.2912473523236, 771.8249970490251, 729.8420082175589, 720.0935940315376, 702.2594567506385, 603.3073416590407, 487.96428283272854, 477.21317408337325, 431.5204736244176, 418.5766156423989, 397.62533282475397, 378.74813390974964, 370.23811209691144, 328.263871192867, 327.26053171508096, 308.5474927752131, 305.00618864987405, 291.12278931340103, 286.86700126779215, 286.6465572410119, 268.12965219881914, 264.2122518473117, 257.8052652254162, 241.14388239726003, 240.19483284123945, 237.56593765891668, 233.22977363303636, 1276.5685867632924, 1123.659543490824, 9276.344019458535, 393.54088129340363, 987.7152832943716, 1091.2752089384196, 12913.807917433694, 3035.890851341916, 441.9432453580353, 2787.96846343924, 1064.3893995187839, 1127.1244388389239, 1872.8738097027024, 1049.5679595224392, 1021.2801665191156, 1018.8795794087446, 4244.468980299843, 947.5068417356426, 1276.2185073093628, 1248.3129300513078, 2911.6240726037545, 2338.681166440346, 4061.478004259285, 2397.89319999844, 7760.092403923635, 4474.051988898924, 2302.075318799931, 6819.762473029675, 27111.237331427797, 77481.90760339168, 58696.0717201223, 27411.55080725793, 18439.519698460546, 18854.33208726983, 9210.255278574628, 12547.294966221878, 6697.808563156006, 9920.259289520634, 20059.530797352025, 7838.501949018667, 19242.312416693338, 9192.374514863917, 20379.7788908004, 27039.283039608676, 11197.335585160883, 43106.38963449038, 23663.053765612334, 14540.86034909888, 4270.951762844981, 2055.7836481470113, 1774.4184484260873, 1565.8534351246237, 787.616823536984, 611.9575053201677, 509.7523334657339, 453.596333362185, 432.26561315910146, 384.282278613587, 361.64737066025754, 347.3045677102962, 305.176183549401, 300.00344668048746, 289.9608467347359, 270.68574670120927, 222.62894421979564, 216.60284168521622, 202.74683270504818, 198.08590488866756, 192.79993113616652, 181.97840592493856, 177.11516177327815, 173.4377723743885, 165.08506917759647, 164.97647708899385, 160.21610479372663, 158.11058330718657, 145.9580001799102, 144.7154427826395, 1904.2767542663717, 1544.9996638869038, 17247.02745977036, 8651.253444032123, 2375.2874612709693, 7388.605609092289, 1225.7415740151032, 738.2238719457346, 615.7955427823683, 931.1643796626621, 6856.33305858678, 2267.637691888491, 2681.889948902887, 4725.8997679845, 13172.417910067816, 4997.999653703853, 1277.8420622771134, 7143.579498412991, 9005.543600881594, 1387.623280895556, 9298.507740602341, 3010.4722437836335, 9678.533391923896, 4358.8521515765515, 25763.07444324553, 5401.399586572718, 11708.969268307595, 5564.338509012078, 35207.40533843147, 15835.061393215154, 27111.237331427797, 14211.460658409536, 12547.294966221878, 77481.90760339168, 39831.10836301429, 58696.0717201223, 33644.329098345006, 19742.856345929864, 18854.33208726983, 18439.519698460546, 17448.647190515505, 23169.47711345921, 754.3481606956273, 683.421624130185, 654.8419326291387, 607.594332544466, 469.092756601727, 462.7916908075519, 438.53163557797757, 431.5469627899917, 400.16020240309365, 380.4867036103263, 375.55015578256683, 365.95872919353843, 359.71337602297586, 354.2267397342153, 340.7819217116203, 310.402406389674, 307.6038532832846, 302.59569626202784, 302.0113065590784, 301.8573813743314, 269.42334109947257, 268.1882580703586, 263.37409623104355, 263.42176964220454, 263.30197095625124, 258.7647883362116, 251.9885212728451, 247.7001817038043, 244.44305049282104, 243.68521438666795, 892.7417575534959, 443.30245982993654, 606.8885311770077, 800.5526909668631, 858.7318078813004, 1523.536333464562, 3159.3342047432575, 931.3783910393538, 1792.3764357811594, 1520.6653882598252, 928.7556438605385, 599.5042652583971, 538.0087174024687, 1513.5558219092961, 2802.805632340436, 2180.513399089466, 6697.808563156006, 999.1758961252524, 3254.655337800397, 2210.7891057784486, 3196.8658908396023, 9331.886460557416, 1902.9090369609066, 2466.6056138913495, 11197.335585160883, 6819.762473029675, 2183.4321250767484, 2498.552768028546, 8229.001477576585, 6363.115451336949, 7838.501949018667, 27111.237331427797, 2324.0871862894737, 27039.283039608676, 18854.33208726983, 43106.38963449038, 77481.90760339168, 117780.37701923442, 3068.833306016656, 28603.52005296668, 5901.141239478637, 16156.186061118477, 20379.7788908004, 23169.47711345921, 20838.657882423533, 35207.40533843147, 25763.07444324553, 975.6574161834568, 805.3731580234573, 579.4603850999906, 575.7577434721245, 465.24104292362455, 396.9900970756538, 368.8559973701352, 363.7747146746074, 339.90752459397197, 320.42079698561247, 312.9269252239352, 312.7083840721339, 289.41510418899315, 282.456517489916, 280.2020760225743, 272.03427881384505, 270.9598573111202, 246.58980305450325, 238.22617788549502, 232.54642264379365, 223.64435097498298, 217.20275230901024, 215.11616928344202, 214.78140664316888, 213.8102115538231, 213.33653361149356, 206.35809510678843, 205.82611798297037, 194.45616011871456, 191.1333438714598, 514.170767135179, 1356.4527934960906, 414.15312946547976, 334.7851886851139, 807.8482103026893, 454.08740384959737, 653.5896133141085, 401.6471357349385, 620.3949527641879, 923.4458071802701, 1116.0662073189405, 1070.0724854274488, 2705.616206397049, 2034.6962451733107, 2850.390432207223, 1627.2229571586413, 480.0784474831655, 926.3666927674661, 800.1573129164126, 2219.4768101779096, 2507.557631378334, 855.0827591420746, 1414.7495166211297, 1572.917415161535, 3241.8915760622986, 5933.357432197589, 18439.519698460546, 5401.399586572718, 6507.690204876879, 1715.9477261955462, 10583.231127205507, 7651.908030837278, 1268.0927135073246, 4435.783474963131, 77481.90760339168, 8229.001477576585, 9331.886460557416, 6838.286817480524, 18854.33208726983, 117780.37701923442, 5803.274106565624, 7838.501949018667, 27111.237331427797, 35207.40533843147, 9005.543600881594, 13172.417910067816, 11708.969268307595, 16156.186061118477], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.484, 1.4835, 1.4823, 1.4818, 1.4816, 1.4812, 1.4806, 1.4806, 1.4803, 1.4803, 1.4803, 1.4803, 1.4802, 1.4802, 1.4801, 1.48, 1.4798, 1.4797, 1.4796, 1.4795, 1.4794, 1.4794, 1.4793, 1.4788, 1.4788, 1.4786, 1.4784, 1.4783, 1.4782, 1.478, 1.4774, 1.3133, 1.3048, 1.255, 1.2586, 1.2674, 1.2918, 1.2936, 1.0892, 1.1196, 1.2135, 1.1626, 1.0291, 1.1855, 1.1276, 1.247, 0.9865, 1.1893, 1.1464, 1.4065, 0.9753, 0.93, 1.0026, 1.0262, 1.1207, 0.9439, 0.9004, 1.1724, 0.9161, 1.021, 0.9694, 0.8601, 0.9291, 0.8579, 0.7845, 0.71, 0.8242, 0.8662, 0.4541, 0.399, 0.7776, 0.8046, 0.851, 0.6996, 0.5346, 0.8965, 0.5805, 0.6058, 0.7073, 0.3484, 0.5955, 0.5966, 0.4156, 1.7685, 1.7682, 1.7681, 1.768, 1.7673, 1.7673, 1.7672, 1.7672, 1.7671, 1.7671, 1.7671, 1.767, 1.767, 1.7669, 1.7668, 1.7666, 1.7665, 1.7664, 1.7663, 1.7662, 1.7659, 1.7659, 1.7656, 1.7655, 1.7655, 1.7654, 1.7654, 1.7653, 1.7653, 1.7652, 1.7545, 1.7278, 1.7309, 1.7399, 1.7478, 1.6986, 1.7093, 1.7289, 1.6685, 1.6683, 1.6568, 1.6502, 1.7107, 1.698, 1.5131, 1.7204, 1.5899, 1.6075, 1.6463, 1.7167, 1.6181, 1.5456, 1.6189, 1.5598, 1.4951, 1.5442, 1.3251, 1.2304, 0.8821, 1.3108, 1.4266, 1.0804, 1.3277, 0.6044, 0.9322, 0.9211, 1.375, 0.9628, 1.1745, 0.2582, 0.6181, 1.0402, 0.6606, 0.5615, 0.7067, 0.368, 0.6218, -0.1973, 0.3904, 0.5401, 0.8772, 0.4262, 0.6229, 0.1921, 0.0199, 0.5251, 0.0919, 0.5144, 0.5757, 0.5841, 2.2096, 2.2081, 2.2079, 2.2077, 2.2076, 2.2075, 2.2074, 2.2074, 2.2067, 2.2067, 2.2064, 2.2058, 2.2056, 2.2055, 2.2054, 2.2054, 2.2054, 2.2054, 2.2052, 2.2052, 2.2052, 2.2049, 2.2048, 2.2047, 2.2046, 2.2046, 2.2043, 2.2043, 2.2041, 2.2041, 2.197, 2.0214, 2.0069, 2.0149, 2.1403, 2.0384, 2.071, 1.9802, 1.9592, 1.9255, 2.0065, 1.8677, 2.0155, 1.7878, 1.7342, 1.9287, 1.911, 2.0804, 2.0463, 1.7655, 1.6265, 1.578, 1.7283, 1.8954, 1.8217, 1.4283, 1.8226, 1.6369, 1.7718, 1.6548, 1.5115, 1.74, 1.593, 1.46, 1.4905, 1.6012, 1.0172, 1.5339, 1.2923, 0.5598, 0.681, 1.1871, 1.0945, 0.7007, -0.3235, 0.7477, 0.757, 0.2782, 0.3687, 0.375, 0.7667, -0.0474, 0.8303, -0.0551, 2.2703, 2.27, 2.2698, 2.2696, 2.269, 2.2685, 2.2683, 2.2681, 2.2681, 2.2678, 2.2677, 2.2677, 2.2677, 2.2675, 2.2675, 2.2674, 2.2674, 2.2673, 2.2672, 2.267, 2.2666, 2.2666, 2.2666, 2.2666, 2.2665, 2.2664, 2.2664, 2.2664, 2.2662, 2.2661, 2.1653, 2.2314, 2.1312, 2.2352, 2.108, 1.9905, 2.0119, 2.1408, 2.214, 1.7552, 1.8177, 1.1739, 1.4489, 1.4761, 1.7617, 1.8625, 1.7683, 1.9626, 1.6674, 1.6023, 1.5804, 1.5752, 1.7456, 1.5215, 1.3569, 1.4623, 1.4385, 1.5381, 1.4293, 1.4978, 1.4431, 1.3076, 1.0624, 0.7972, 0.8958, 1.0111, 1.0392, 0.8688, 0.928, 0.9718, 0.3337, 0.4939, 0.9833, 0.6987, 0.778, 0.8336, -0.1245, -0.7488, 0.3168, -0.0326, -0.2933, -0.0464, 0.1192, 2.3642, 2.3639, 2.3634, 2.3632, 2.3632, 2.3631, 2.3631, 2.363, 2.3625, 2.3625, 2.3624, 2.3624, 2.3622, 2.3622, 2.3618, 2.3617, 2.3616, 2.3616, 2.3615, 2.3615, 2.3615, 2.3614, 2.3614, 2.3613, 2.3612, 2.361, 2.3608, 2.3606, 2.3606, 2.3605, 2.3259, 2.28, 2.3143, 2.2853, 2.25, 2.3042, 2.2865, 2.3354, 2.1844, 2.2477, 2.1799, 2.2513, 2.0911, 2.1172, 1.9841, 1.9009, 1.7926, 2.0157, 1.6617, 2.1331, 1.7769, 1.9278, 1.1667, 2.0181, 1.753, 1.0277, 1.7437, 0.1135, 1.6975, 1.7518, 0.6779, 1.2113, 0.1803, 0.7255, 1.4421, 1.1813, -0.5208, 1.1059, -0.197, 0.6766, -0.139, -0.0366, -0.4144, -0.3483, -0.0507, -0.0712, -0.919, 0.1941, 0.3689, -0.4134, 2.7103, 2.71, 2.7099, 2.7097, 2.7096, 2.7094, 2.7092, 2.7091, 2.7091, 2.7091, 2.709, 2.709, 2.709, 2.7089, 2.7089, 2.7087, 2.7086, 2.7085, 2.7084, 2.7084, 2.7084, 2.7082, 2.7082, 2.7081, 2.7079, 2.7079, 2.7078, 2.7076, 2.7076, 2.7075, 2.6737, 2.6497, 2.6515, 2.603, 2.5228, 2.5423, 2.6335, 2.4245, 2.2554, 2.5329, 2.5059, 2.4228, 2.4622, 2.5574, 2.4916, 2.2994, 2.2013, 2.3531, 2.5125, 2.5274, 2.2752, 2.5168, 2.2154, 2.2247, 2.208, 1.7078, 2.1467, 2.3989, 1.5469, 2.0439, 1.6692, 1.7427, -0.2678, 0.7987, 1.3918, 0.2519, -0.2958, 1.5131, -0.2985, -0.1112, 0.1162, 0.6269, 0.7074, 0.2585, 0.7706, 0.5875, 0.7195, -0.8261, 2.7255, 2.7255, 2.7251, 2.725, 2.725, 2.7249, 2.7249, 2.7249, 2.7247, 2.7243, 2.7243, 2.7241, 2.724, 2.7239, 2.7238, 2.7238, 2.7234, 2.7234, 2.7233, 2.7232, 2.7231, 2.723, 2.723, 2.7228, 2.7228, 2.7227, 2.7224, 2.7224, 2.7224, 2.7223, 2.6759, 2.6387, 2.4927, 2.69, 2.5957, 2.5796, 2.3032, 2.3992, 2.6668, 2.3827, 2.5154, 2.5011, 2.4023, 2.5093, 2.4938, 2.4931, 2.1698, 2.5071, 2.3699, 2.3701, 2.0887, 2.1421, 1.855, 2.0505, 1.4602, 1.657, 2.0092, 1.3573, 0.4173, -0.2786, -0.1062, 0.3573, 0.6013, 0.5218, 1.0047, 0.7728, 1.2128, 0.9249, 0.4033, 1.0819, 0.3985, 0.9527, 0.2819, 0.0204, 0.7723, -0.4319, 0.0088, 0.389, 2.8355, 2.8353, 2.8352, 2.8352, 2.8346, 2.8343, 2.834, 2.8338, 2.8337, 2.8334, 2.8333, 2.8332, 2.8328, 2.8328, 2.8327, 2.8324, 2.8317, 2.8316, 2.8313, 2.8312, 2.8311, 2.8308, 2.8307, 2.8306, 2.8303, 2.8303, 2.8302, 2.8301, 2.8296, 2.8296, 2.7729, 2.7561, 2.595, 2.6281, 2.6545, 2.5277, 2.6511, 2.6944, 2.6973, 2.623, 2.2571, 2.4292, 2.3514, 2.1893, 1.8835, 2.0772, 2.4414, 1.8626, 1.7383, 2.3638, 1.6363, 2.0615, 1.5719, 1.897, 1.1342, 1.7769, 1.4133, 1.7082, 0.752, 1.1175, 0.7838, 1.1554, 1.1935, 0.0203, 0.3664, -0.0017, 0.3773, 0.753, 0.7302, 0.7486, 0.7769, 0.3342, 2.843, 2.8429, 2.8428, 2.8427, 2.8423, 2.8422, 2.8421, 2.842, 2.8419, 2.8418, 2.8418, 2.8417, 2.8417, 2.8416, 2.8415, 2.8413, 2.8413, 2.8412, 2.8412, 2.8412, 2.8408, 2.8408, 2.8408, 2.8408, 2.8408, 2.8407, 2.8406, 2.8405, 2.8405, 2.8405, 2.832, 2.8326, 2.8122, 2.7861, 2.7789, 2.7405, 2.6554, 2.752, 2.662, 2.6636, 2.7101, 2.754, 2.7613, 2.6094, 2.5092, 2.5329, 2.3215, 2.631, 2.3444, 2.4158, 2.268, 1.9552, 2.3703, 2.2284, 1.6824, 1.8043, 2.2225, 2.1578, 1.641, 1.7107, 1.5824, 0.8929, 2.1687, 0.7867, 0.9833, 0.443, 0.0712, -0.4489, 1.945, 0.3362, 1.4584, 0.6455, 0.3393, 0.0928, 0.1597, -0.3372, -0.1155, 3.0425, 3.0423, 3.0419, 3.0419, 3.0415, 3.0412, 3.041, 3.041, 3.0408, 3.0407, 3.0406, 3.0406, 3.0404, 3.0403, 3.0403, 3.0402, 3.0402, 3.0398, 3.0397, 3.0396, 3.0395, 3.0393, 3.0393, 3.0393, 3.0393, 3.0393, 3.0391, 3.0391, 3.0388, 3.0388, 3.0222, 2.981, 3.0123, 3.0112, 2.9578, 2.9819, 2.9387, 2.9686, 2.8967, 2.837, 2.7713, 2.7776, 2.5932, 2.5972, 2.5097, 2.6209, 2.908, 2.7433, 2.7798, 2.4606, 2.3685, 2.7099, 2.5041, 2.4469, 2.1645, 1.8777, 1.3968, 1.907, 1.8101, 2.3776, 1.397, 1.5328, 2.4893, 1.7321, -0.2981, 1.1763, 1.0821, 1.2011, 0.4222, -1.0495, 1.3029, 0.9561, -0.1624, -0.4536, 0.7975, 0.4383, 0.5033, 0.1654], \"Freq\": [117780.0, 17247.0, 26937.0, 12913.0, 43106.0, 27111.0, 9276.0, 27627.0, 8651.0, 10291.0, 18439.0, 34563.0, 20838.0, 58696.0, 7388.0, 19742.0, 10296.0, 39831.0, 34224.0, 13172.0, 9331.0, 21505.0, 9678.0, 20976.0, 6856.0, 18854.0, 7642.0, 11197.0, 6041.0, 25763.0, 410.2647660786223, 337.49045845539774, 231.0529729958325, 204.8031010556745, 196.7275102051663, 180.9163682085454, 160.33363326499403, 159.4971112851057, 152.3053184079944, 151.9997307027577, 151.19294841073528, 151.20788695035503, 150.30016459880787, 149.23549405880007, 148.20627691664134, 144.20156608950714, 141.30937849844074, 138.72757203472372, 135.67195755130103, 134.70076972694358, 132.53651147369797, 131.43866258454736, 130.46537170026372, 121.72875439219771, 121.07378878514832, 118.76143339736959, 114.29713523485299, 113.78627808637916, 112.6287262803979, 109.46933186512126, 116.30464407463046, 3679.5397173701676, 3589.939161096272, 3992.4279514707446, 3427.5253353054213, 2973.012814290495, 2123.7908466732974, 1889.7660787119191, 23238.083621211397, 14539.470989944291, 3578.8344734561238, 6243.925809538521, 27292.52826149752, 4296.287293259279, 6868.527287721658, 1636.2555983894104, 20764.52297530211, 2342.559017383211, 3403.262498574073, 311.88303061985863, 13417.120227939206, 19290.9005220385, 9448.758277706544, 7785.555719204706, 3413.4602059433314, 12503.55121008422, 15092.189320233836, 1933.4415538694773, 11164.650702674591, 5458.36160475613, 7578.2259531203035, 15293.802448531249, 9816.016707574385, 15162.653745337995, 19746.723643761958, 27009.673377274637, 14632.542819580567, 10298.253310358527, 41960.91032507624, 26123.683103749005, 10034.201693616335, 9141.408962519934, 8012.329412276301, 10618.858959619387, 13594.703314862043, 7300.095218905629, 11168.858018441295, 10681.7659468901, 9108.099909419958, 11803.4652985212, 9711.03875606851, 9518.56280934782, 9397.394719272368, 2670.953583889528, 1396.8737501451035, 1237.582905675674, 1058.906194102068, 571.8524491363577, 566.5171928238237, 534.2085749594996, 531.4787396662394, 517.5635540194911, 514.32880879217, 506.221316514734, 493.0794557014494, 487.31768978571455, 471.1894463826972, 431.96131124092847, 407.2663883578704, 375.63190823103616, 365.9133697700087, 347.08401761375586, 334.814996890338, 306.6996713313522, 301.43612988992663, 279.79356957790293, 268.43109122491273, 264.82846881079814, 261.0639402720869, 259.50390110675545, 252.6306128398863, 249.75722538659198, 243.51038561243317, 1164.6103222887127, 9881.962861352411, 7358.203312059584, 2655.0154404728164, 1089.701329612348, 4445.370945521688, 1943.2724349199057, 922.5973491601534, 3898.8380671695923, 3361.465766525359, 4038.4138060399073, 4377.708549447204, 1094.4149511728838, 1204.800550419729, 20859.55662249237, 785.826491676936, 3422.5502539106274, 2530.6480207451323, 1530.215867816341, 764.9286572438035, 1328.958921736681, 2073.953573149937, 1293.6841820944258, 1803.3021028499857, 2057.6806493390154, 1612.681172760395, 3500.603113026274, 4990.330782164286, 15170.654289144568, 3519.738512158944, 2345.627504680908, 6336.6872825638375, 2936.7184800104014, 18319.426448401973, 7741.553367378189, 7474.7968186212065, 2580.4686051397603, 5909.976872081489, 3745.554031104622, 17105.62939610251, 8992.44494466933, 4435.925431197662, 7812.177258735843, 8195.791941392363, 6602.43169988126, 9813.968230870214, 7102.096962547611, 16487.985968260742, 8708.868875175462, 6780.990896623139, 5089.933328664565, 7214.745930794354, 6085.970188816328, 7275.622352007906, 7499.0641277857985, 5692.124482199252, 6289.463716045408, 5154.890959415277, 4939.315763261962, 4624.8989209286865, 845.8465900867983, 350.6175558928793, 328.7878607563526, 305.2728535903829, 291.5671383255223, 288.1345440522646, 278.33029703268426, 276.0926574135836, 227.62357953986637, 224.98995191884086, 211.7839831874477, 184.22930938823217, 178.92535630004943, 174.5367359934462, 170.79587260245341, 170.5903466997222, 170.5603735715871, 170.07499506452115, 165.01545746063803, 164.74079191244175, 163.36026758871287, 156.7940412491919, 152.6259913887577, 150.38414496612899, 149.24192112853623, 148.63659854998835, 141.74409383468446, 141.14010579870182, 137.7654837315513, 136.3322148497318, 161.65083475243105, 3963.6274031762773, 4356.909038750766, 3169.4308847777606, 443.43138889280635, 1986.4540597764278, 943.5074577473041, 2225.4975321578904, 2804.843701533651, 2316.3841753075494, 904.1239685719388, 2709.3553386647977, 766.6295781123265, 4071.5266671014983, 4751.757645117434, 1169.4314334282992, 1298.3986283881063, 411.1405778661555, 487.8243674245016, 2194.45901813036, 4432.511758331965, 5621.618741318531, 2472.310133503205, 1053.4705475909914, 1453.3344699232086, 7388.7040742811005, 1376.02325257506, 2870.4163028472844, 1664.128338096411, 2525.789824652427, 4068.9516508308566, 1801.5371670344225, 2891.4201327087853, 3233.209850775036, 2883.967647506365, 2047.3673081693214, 5237.48062164304, 2103.9925737462963, 2908.4064213533557, 6567.150320206316, 4345.28161585591, 2628.305389438317, 2696.9475846198134, 3426.60780700972, 6146.306167927444, 3290.6571289271633, 2928.446890417956, 3376.420563424241, 3230.467063153621, 3069.399731487133, 2763.2998106868995, 2865.921764207554, 2679.374224772458, 2805.3705549690085, 728.586828768259, 582.9120462668575, 492.1512494357497, 456.8545393842595, 341.8555794866905, 293.0493039903105, 272.33179322467237, 260.63720020597634, 259.05082382468856, 235.26771562460996, 230.82233084081807, 230.06786156605926, 229.38427611126681, 222.1133390315984, 220.05012317716472, 216.19143326378108, 214.76079517088831, 210.16665232885472, 206.0909450033958, 193.61809245442842, 180.69747541104016, 180.60821497421782, 180.5195131633444, 178.18967967483874, 177.04387465760348, 172.910732720869, 171.70555974572306, 171.61713498588404, 165.1730693943571, 163.25980155007477, 1007.2500210772637, 300.02589783343296, 612.0897401179737, 218.43308282446466, 552.8916815045859, 1122.776003453359, 959.2230631861192, 380.221048076037, 239.69775266219816, 2601.4526981208874, 1785.2179493834306, 39297.742775460574, 9153.16957428426, 5440.856492761366, 1690.36208200353, 1064.852049309171, 1418.8324534786204, 645.1653783240558, 1720.5851686791298, 2043.8185045941773, 2165.6440052050284, 2154.1818188784073, 1181.4854475178402, 2243.3827014915364, 3289.45710393599, 2275.555252132913, 2366.360942093286, 1796.8063811400093, 2140.6249055804924, 1764.4594136967291, 1977.236377606061, 2354.2435164060003, 3611.613498583189, 4923.060188056328, 3845.624666192916, 3134.7085253902824, 2399.659360469021, 2698.5953785776614, 2549.3402438773605, 2428.433984389318, 4085.306111150272, 3444.71581977068, 2343.0751167139424, 2696.344287119479, 2514.588595137116, 2347.1396179620674, 3116.820034615897, 3779.7530267232987, 2704.139225709064, 2855.742642155916, 2832.87711394797, 2662.687237266537, 2306.5018597033486, 1689.664544179821, 1080.4766836530314, 672.6387910234253, 616.2664272533091, 606.3086720205858, 562.6906817308175, 552.8131692253276, 519.4820466727454, 415.7932017077671, 411.09623879348203, 391.19740695190904, 388.7600657851882, 353.38982077239336, 353.0613917751165, 306.39384619073553, 296.20467521772787, 294.5761293799709, 291.21857927958416, 284.45891673595725, 283.1739492415339, 277.51840406162387, 271.4379701906033, 270.91921056868455, 268.14220181444466, 259.84245318979555, 241.85541852780233, 231.35485843611033, 221.13983597293517, 217.1530170602659, 211.45039687672508, 2793.2403174077717, 5550.961880350152, 842.7689591167966, 1495.2558745836418, 1739.5289579940675, 668.7754125272668, 843.8009498343878, 386.2233044135895, 1161.5749766455779, 687.6225335937713, 1040.3176147157474, 610.154695476072, 1750.9282830644233, 1336.0001504746906, 2658.39034993321, 4116.023782173072, 6283.1258362621675, 1886.7230226395593, 5348.183070701837, 845.6750254588205, 2379.4681022342984, 1453.5736092525672, 8338.243432224108, 966.0780545263881, 1411.952972221506, 4536.929423400374, 1356.764572650725, 12399.785136444874, 1395.658803758057, 1246.5436577370494, 3674.133136934188, 1912.4833240428818, 4482.816041028201, 2720.341276589, 1511.4304150704659, 1741.6916501809671, 4325.49445717817, 1780.4209616866492, 2717.025621419117, 1947.3495396831527, 2211.266429096733, 2143.8238011649237, 2286.585313868292, 2231.800959919511, 2069.8583106497294, 2040.9097393014583, 2200.4286767888552, 1843.6471989002562, 1790.1379073071587, 1766.663150856387, 1674.5871984311227, 1017.8059085242434, 968.2822976691264, 763.9710000753745, 738.8429438007062, 618.3577637691825, 551.7877079710519, 513.5738978063483, 512.3523300470531, 506.37516619767194, 494.3627573946351, 486.8148864750911, 485.5883027392523, 470.9664677328412, 468.0183098149226, 425.21217543471766, 390.9864714557902, 387.32640226559215, 510.92949617771336, 369.9554318985391, 359.5018407976119, 338.2784102143807, 336.3975527315244, 332.3949929991578, 306.94417591027684, 303.7982764719173, 292.3043878927857, 279.96083450997406, 276.73416255973206, 272.01066433410193, 892.6939902144181, 977.6648895598223, 920.4667209535663, 1211.9486485456407, 3243.376217318254, 1551.1089562432046, 625.5539864427925, 2497.066128475106, 6526.404107009623, 1000.0925531799974, 1126.1198211984024, 1576.2214202679202, 1123.8055753522412, 748.9929354016964, 893.6831524822437, 1478.9091647925145, 1857.3084000502008, 1201.3767823197365, 780.1277441760701, 737.0540874308341, 1176.2310621135061, 738.4917813018299, 1276.5727102044768, 1190.6715872107152, 1181.4441013768974, 2440.9409646067184, 1221.6637763754932, 866.6525271147781, 1907.2077423193166, 1089.7231652239325, 1370.682623109151, 1296.8392279254633, 3940.7838789002935, 2099.9378076036915, 1445.325538484574, 2344.278848998489, 2902.799364746451, 1258.3959454261426, 1736.5357824804341, 1608.3733486342787, 1497.9514569137705, 1385.3662024220102, 1337.9699707991836, 1390.8358817462474, 1323.2251665882554, 1274.5214669619204, 1254.8901318662156, 1254.4303923203522, 1387.1180820608738, 1329.4351934735514, 830.1643394296497, 782.3936339662675, 770.927502840404, 728.9444224115457, 719.1959909765675, 701.3618731864597, 602.4097941910228, 487.06665637287136, 476.3155057794006, 430.62289677861776, 417.67900692707434, 396.7277272304034, 377.8505857839698, 369.3405068682331, 327.366278652202, 326.3629127328697, 307.6497797412012, 304.10860508497336, 290.2251075078891, 285.96949514738475, 285.748973617526, 267.2319887620852, 263.31468596537877, 256.907666937134, 240.2462706210883, 239.2972533159611, 236.6683767258979, 232.33218323511971, 1214.0047298768823, 1029.5542713151654, 7344.9423514291875, 379.53984921931084, 866.8707232634987, 942.4550847930598, 8459.435251827399, 2189.1771856948344, 416.44684532423474, 1977.422709008906, 862.1236791998042, 899.975389187393, 1354.7235769398412, 844.8963185458384, 809.4741409260401, 807.0134799528485, 2433.3973389276975, 761.1302565217774, 893.6734634999225, 874.3099321960875, 1539.2126398109012, 1304.1146410885913, 1699.573327605228, 1220.0415113548993, 2187.9586015967766, 1535.956162353376, 1123.921673841462, 1734.931199043367, 2694.177223516976, 3839.098332358603, 3455.7087752451503, 2565.3309028164294, 2202.459774332169, 2079.97392739252, 1646.8796360555802, 1779.1919952482137, 1474.6943013983905, 1637.696972004462, 1965.7253413534584, 1514.0706037717625, 1876.5085783023255, 1560.3785057235568, 1768.7186812895393, 1806.788396419063, 1586.890310779996, 1832.4309840681392, 1562.897302317053, 1404.7094669456428, 4270.057639041881, 2054.889569015396, 1773.5243513273915, 1564.959352999047, 786.7226967499541, 611.0634680116644, 508.8582799665302, 452.7017260159574, 431.37149106072275, 383.3881987702283, 360.75295050804425, 346.41047239841174, 304.2820378915091, 299.10930724852335, 289.06670296822335, 269.7916395311133, 221.73378797635903, 215.7083802075277, 201.85271835969064, 197.19175933609253, 191.90580908625617, 181.08432783499086, 176.22109920120565, 172.5433053533625, 164.19055238361017, 164.0823789548816, 159.3220311544551, 157.2165329248435, 145.06391955159495, 143.8212894399369, 1788.3174944778896, 1426.6466245742454, 13556.3088873005, 7029.3477300471, 1981.5850893664674, 5429.915785997432, 1019.0913515686136, 640.9250715012721, 536.1914774960794, 752.7035341582292, 3844.005337457947, 1510.1060287380271, 1652.2232740867723, 2475.9080772432444, 5083.0838656330625, 2340.7885896971907, 861.3827039939871, 2699.5883252931526, 3005.228279864588, 865.5973555039775, 2802.2328209481084, 1388.0258411768484, 2734.8835212045806, 1704.7791306135434, 4699.229006595679, 1873.49207823614, 2823.212272115953, 1801.9516100731628, 4381.940972192974, 2840.595158379011, 3483.3334323937647, 2647.691397550384, 2428.531812899743, 4639.247061598942, 3371.384750884385, 3438.087503922483, 2878.77437253213, 2459.7531564683595, 2295.9843094913026, 2287.3123822369976, 2226.5416858324897, 1898.8543508907037, 753.4507537331187, 682.522525326979, 653.9445356808583, 606.6968361282518, 468.19531661850505, 461.8942608385623, 437.6342008000507, 430.6262185486002, 399.26269557068355, 379.58915964742806, 374.65274938346727, 365.06129064042665, 358.81594222773987, 353.32931693303743, 339.88446600658773, 309.50494684794484, 306.7064407447247, 301.6983504870852, 301.1139252406115, 300.95984163305684, 268.5258647130874, 267.2902148099733, 262.4766406544966, 262.52409812641446, 262.4043843060658, 257.86740729185055, 251.09113912823443, 246.80275485642818, 243.54558375858971, 242.78770764116766, 881.977607224723, 438.19303377102113, 587.8156130337935, 755.4209188471161, 804.5041802044126, 1373.553399038933, 2615.880284874125, 849.4016943169602, 1493.8178455940629, 1269.4838491695857, 812.2018844985221, 547.7837778147631, 495.2064043499679, 1196.8006778786516, 2004.967153846711, 1597.291283935621, 3971.3567001602196, 807.3740819628204, 1974.5591777717807, 1440.4684632772512, 1796.785660869391, 3835.9808613246546, 1184.6944493082372, 1332.4739575001358, 3503.8823656851887, 2410.65973894204, 1172.6156186279154, 1257.705460984303, 2470.5884287249078, 2048.449203245601, 2219.531210625866, 3852.1960448747195, 1182.7680465366132, 3455.0774295621904, 2932.4477934878514, 3905.864801269428, 4840.517315298656, 4374.489514535041, 1248.7607947713227, 2329.2053176642908, 1476.0368996387826, 1792.5598507282418, 1664.808519621891, 1479.0726678618798, 1422.4248400667047, 1462.121116716543, 1335.4371477194813, 974.7627920019361, 804.4785038361796, 578.565725654953, 574.8631389754302, 464.3463963714265, 396.09540072887324, 367.9613622394828, 362.8801083705685, 339.01215470439547, 319.5260943747745, 312.03217938991946, 311.81368387976335, 288.5204262092298, 281.56178425264073, 279.3073570130354, 271.13959062790207, 270.0651891117511, 245.69516806238244, 237.33148513268307, 231.65171490797024, 222.7496640459579, 216.30804383644087, 214.22156604634196, 213.88677604207282, 212.9154760500279, 212.44167239796948, 205.46335148479204, 204.93145579055553, 193.5615398776419, 190.23853747335596, 503.33684809995617, 1274.3725296786097, 401.42837931324186, 324.161783190246, 741.5235731006472, 426.9706231221803, 588.5722877767868, 372.68261580545317, 535.7355777070044, 751.1754929888967, 850.1164721867127, 820.2590798538419, 1724.6965474492754, 1302.229628582212, 1671.4830141748348, 1066.3847648555823, 419.2795561730094, 686.1784011093852, 614.684680977566, 1239.1637170798133, 1276.8304543811425, 612.5834355325004, 824.9611580644021, 866.2119467163078, 1346.0753069492987, 1849.324859012234, 3552.979696339683, 1733.5104532207217, 1895.7848369795254, 881.6745145970738, 2039.7700794401503, 1689.2430972726506, 728.5751759899114, 1195.2603234940914, 2741.3948635547617, 1271.9395889973957, 1312.6958043966742, 1083.4502928895943, 1370.8529335275737, 1965.7770656039215, 1017.9977160419719, 972.0845805199551, 1098.6886530324043, 1066.3059543449597, 952.9720575069624, 973.3701171056761, 923.2509589591416, 908.6447171084145]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el242291404429332510727991961190\", ldavis_el242291404429332510727991961190_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el242291404429332510727991961190\", ldavis_el242291404429332510727991961190_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el242291404429332510727991961190\", ldavis_el242291404429332510727991961190_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "7      22.623584        1       1 -0.125842 -0.058905\n",
       "5      17.053052        1       2 -0.167983  0.021164\n",
       "3      10.962967        1       3 -0.034933  0.048933\n",
       "6      10.314991        1       4  0.016219 -0.259858\n",
       "9       9.397863        1       5 -0.102380 -0.124425\n",
       "2       6.648017        1       6 -0.066805  0.151369\n",
       "1       6.546907        1       7  0.080615  0.169454\n",
       "4       5.867441        1       8 -0.064444  0.087724\n",
       "8       5.818216        1       9  0.212771 -0.009882\n",
       "0       4.766961        1      10  0.252782 -0.025575, topic_info=      Category           Freq         Term          Total  loglift  logprob\n",
       "term                                                                       \n",
       "50947  Default  117780.000000         film  117780.000000  30.0000  30.0000\n",
       "52143  Default   17247.000000         show   17247.000000  29.0000  29.0000\n",
       "4216   Default   26937.000000          bad   26937.000000  28.0000  28.0000\n",
       "56099  Default   12913.000000        funny   12913.000000  27.0000  27.0000\n",
       "30360  Default   43106.000000         good   43106.000000  26.0000  26.0000\n",
       "55570  Default   27111.000000        great   27111.000000  25.0000  25.0000\n",
       "8282   Default    9276.000000       comedy    9276.000000  24.0000  24.0000\n",
       "20655  Default   27627.000000       people   27627.000000  23.0000  23.0000\n",
       "33160  Default    8651.000000       series    8651.000000  22.0000  22.0000\n",
       "11184  Default   10291.000000       action   10291.000000  21.0000  21.0000\n",
       "38316  Default   18439.000000         love   18439.000000  20.0000  20.0000\n",
       "37231  Default   34563.000000       really   34563.000000  19.0000  19.0000\n",
       "4921   Default   20838.000000        films   20838.000000  18.0000  18.0000\n",
       "3124   Default   58696.000000         like   58696.000000  17.0000  17.0000\n",
       "45005  Default    7388.000000           tv    7388.000000  16.0000  16.0000\n",
       "2163   Default   19742.000000        watch   19742.000000  15.0000  15.0000\n",
       "4838   Default   10296.000000       horror   10296.000000  14.0000  14.0000\n",
       "6149   Default   39831.000000        would   39831.000000  13.0000  13.0000\n",
       "8332   Default   34224.000000        story   34224.000000  12.0000  12.0000\n",
       "9222   Default   13172.000000        years   13172.000000  11.0000  11.0000\n",
       "11934  Default    9331.000000         role    9331.000000  10.0000  10.0000\n",
       "4540   Default   21505.000000   characters   21505.000000   9.0000   9.0000\n",
       "48307  Default    9678.000000          saw    9678.000000   8.0000   8.0000\n",
       "24142  Default   20976.000000        think   20976.000000   7.0000   7.0000\n",
       "8861   Default    6856.000000          dvd    6856.000000   6.0000   6.0000\n",
       "47519  Default   18854.000000         best   18854.000000   5.0000   5.0000\n",
       "44570  Default    7642.000000        worst    7642.000000   4.0000   4.0000\n",
       "35391  Default   11197.000000         cast   11197.000000   3.0000   3.0000\n",
       "49101  Default    6041.000000          war    6041.000000   2.0000   2.0000\n",
       "1611   Default   25763.000000        first   25763.000000   1.0000   1.0000\n",
       "...        ...            ...          ...            ...      ...      ...\n",
       "4776   Topic10     614.684681       powell     800.157313   2.7798  -6.4960\n",
       "31874  Topic10    1239.163717      musical    2219.476810   2.4606  -5.7950\n",
       "23570  Topic10    1276.830454         king    2507.557631   2.3685  -5.7650\n",
       "16337  Topic10     612.583436        opera     855.082759   2.7099  -6.4995\n",
       "1961   Topic10     824.961158        henry    1414.749517   2.5041  -6.2018\n",
       "50776  Topic10     866.211947         mary    1572.917415   2.4469  -6.1530\n",
       "12598  Topic10    1346.075307      actress    3241.891576   2.1645  -5.7122\n",
       "40572  Topic10    1849.324859    beautiful    5933.357432   1.8777  -5.3946\n",
       "38316  Topic10    3552.979696         love   18439.519698   1.3968  -4.7416\n",
       "46155  Topic10    1733.510453      version    5401.399587   1.9070  -5.4592\n",
       "26184  Topic10    1895.784837         play    6507.690205   1.8101  -5.3698\n",
       "8260   Topic10     881.674515       beauty    1715.947726   2.3776  -6.1353\n",
       "21133  Topic10    2039.770079        young   10583.231127   1.3970  -5.2966\n",
       "44580  Topic10    1689.243097        woman    7651.908031   1.5328  -5.4851\n",
       "26892  Topic10     728.575176      singing    1268.092714   2.4893  -6.3261\n",
       "1703   Topic10    1195.260323    wonderful    4435.783475   1.7321  -5.8310\n",
       "38066  Topic10    2741.394864          one   77481.907603  -0.2981  -5.0009\n",
       "9533   Topic10    1271.939589  performance    8229.001478   1.1763  -5.7688\n",
       "11934  Topic10    1312.695804         role    9331.886461   1.0821  -5.7373\n",
       "49079  Topic10    1083.450293       screen    6838.286817   1.2011  -5.9292\n",
       "47519  Topic10    1370.852934         best   18854.332087   0.4222  -5.6940\n",
       "50947  Topic10    1965.777066         film  117780.377019  -1.0495  -5.3335\n",
       "15184  Topic10    1017.997716         book    5803.274107   1.3029  -5.9916\n",
       "43060  Topic10     972.084581       played    7838.501949   0.9561  -6.0377\n",
       "55570  Topic10    1098.688653        great   27111.237331  -0.1624  -5.9153\n",
       "19420  Topic10    1066.305954         time   35207.405338  -0.4536  -5.9452\n",
       "54146  Topic10     952.972058        music    9005.543601   0.7975  -6.0576\n",
       "9222   Topic10     973.370117        years   13172.417910   0.4383  -6.0364\n",
       "42605  Topic10     923.250959          old   11708.969268   0.5033  -6.0892\n",
       "19551  Topic10     908.644717          man   16156.186061   0.1654  -6.1052\n",
       "\n",
       "[833 rows x 6 columns], token_table=       Topic      Freq     Term\n",
       "term                           \n",
       "31612      2  0.997789     1/10\n",
       "39705      2  0.998015     13th\n",
       "20936     10  0.994853     1932\n",
       "39597      2  0.996329     2/10\n",
       "37796      5  0.994609     9/11\n",
       "5103       1  0.992425       93\n",
       "55863      7  0.997618   abbott\n",
       "11944      8  0.996316      abc\n",
       "27735      1  0.475945   acting\n",
       "27735      2  0.433196   acting\n",
       "27735      4  0.062780   acting\n",
       "27735      9  0.017961   acting\n",
       "27735     10  0.010128   acting\n",
       "11184      1  0.273626   action\n",
       "11184      4  0.046446   action\n",
       "11184      6  0.634120   action\n",
       "11184      9  0.045669   action\n",
       "2854       1  0.378117    actor\n",
       "2854       2  0.065848    actor\n",
       "2854       4  0.097908    actor\n",
       "2854       7  0.098379    actor\n",
       "2854       9  0.321855    actor\n",
       "2854      10  0.037875    actor\n",
       "36000      1  0.596438   actors\n",
       "36000      2  0.178349   actors\n",
       "36000      4  0.139153   actors\n",
       "36000      5  0.000944   actors\n",
       "36000      7  0.017079   actors\n",
       "36000      8  0.000315   actors\n",
       "36000      9  0.063044   actors\n",
       "...      ...       ...      ...\n",
       "9222       2  0.056937    years\n",
       "9222       3  0.092466    years\n",
       "9222       4  0.001215    years\n",
       "9222       5  0.135890    years\n",
       "9222       6  0.022319    years\n",
       "9222       7  0.025204    years\n",
       "9222       8  0.385882    years\n",
       "9222       9  0.075916    years\n",
       "9222      10  0.073866    years\n",
       "43955      1  0.267508      yet\n",
       "43955      2  0.091562      yet\n",
       "43955      3  0.137579      yet\n",
       "43955      4  0.275747      yet\n",
       "43955      5  0.116160      yet\n",
       "43955      6  0.027422      yet\n",
       "43955      7  0.031423      yet\n",
       "43955      8  0.041309      yet\n",
       "43955      9  0.004708      yet\n",
       "43955     10  0.006591      yet\n",
       "21133      1  0.010866    young\n",
       "21133      2  0.003685    young\n",
       "21133      3  0.531218    young\n",
       "21133      4  0.038174    young\n",
       "21133      5  0.037229    young\n",
       "21133      7  0.071434    young\n",
       "21133      8  0.090048    young\n",
       "21133      9  0.024567    young\n",
       "21133     10  0.192758    young\n",
       "17361      2  0.999448   zombie\n",
       "17561      2  0.999241  zombies\n",
       "\n",
       "[2371 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 6, 4, 7, 10, 3, 2, 5, 9, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_tf, corpus_tf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with LDA vectors (we'll have to use only the labeled sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Corpora for predictions\n",
      "corpus tf done\n",
      "corpus lda tf done\n",
      "Creating lda vectors\n",
      "lda vectors done\n"
     ]
    }
   ],
   "source": [
    "#For predictions we need only the labeled set:\n",
    "print(\"Creating Corpora for predictions\")\n",
    "corpus_tf2 = [dictionary.doc2bow(sentence) for sentence in labeled_sentences]\n",
    "print('corpus tf done')\n",
    "\n",
    "lda_tf2 = models.LdaModel(corpus_tf2, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf2 = lda_tf2[corpus_tf2]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "print('Creating lda vectors')\n",
    "X = gensim.matutils.corpus2csc(corpus_lda_tf2)\n",
    "X = X.T\n",
    "print('lda vectors done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.33803619,  0.02897068,  0.        ,  0.37506854,  0.08637989,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.1691858 ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Sentences, Model and the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf2.save(os.path.join(outputs, 'model_tf2.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf2.mm'), corpus_lda_tf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Sentences, Model and the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf2 = models.LdaModel.load(os.path.join(outputs, 'model_tf2.lda'))\n",
    "corpus_lda_tf2 = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf2.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(X,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf_lda = LR(penalty='l2',\n",
    "                   dual=False,\n",
    "                   tol=0.0001,\n",
    "                   C=1.0,\n",
    "                   fit_intercept=True,\n",
    "                   intercept_scaling=1,\n",
    "                   class_weight=None,\n",
    "                   random_state=0,\n",
    "                   solver='liblinear',\n",
    "                   max_iter=100,\n",
    "                   multi_class='ovr',\n",
    "                   verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7182\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_lda = clf_LR_tf_lda.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Word Vectors\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors  \n",
    "\n",
    "Introducing Distributed Word Vectors: This part of the tutorial will focus on using distributed word vectors created by the Word2Vec algorithm, using the Gensim implementation.  \n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "\n",
    "Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "Distributed word vectors are powerful and can be used for many applications, particularly word prediction and translation. Here, we will try to apply them to sentiment analysis.\n",
    "\n",
    "Using word2vec in Python: In Python, we will use the excellent implementation of word2vec from the gensim package. If you don't already have gensim installed, you'll need to install it. There is an excellent tutorial that accompanies the Python Word2Vec implementation, here.\n",
    "\n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). ln order to train your model in a reasonable amount of time, you will need to install cython (instructions here). Word2Vec will run without cython installed, but it will take days to run instead of minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing to Train a Model\n",
    "\n",
    "We are going to use the whole set of sentences for training, the labeled and the unlabeled, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model in Part 1, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used.  \n",
    "\n",
    "To train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Saving Your Model  \n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 5   # Minimum word count\n",
    "num_workers = -1       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-5   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/gensim/models/word2vec.py:879: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"supplied example count (%i) did not equal expected count (%i)\", example_count, total_examples)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "model = Word2Vec(all_sentences_sw,\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/gensim/models/word2vec.py:879: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"supplied example count (%i) did not equal expected count (%i)\", example_count, total_examples)\n"
     ]
    }
   ],
   "source": [
    "##Optionally converting the model for Bigrams (to capture more context):\n",
    "bigram_transformer = gensim.models.Phrases(all_sentences_sw)\n",
    "model = Word2Vec(bigram_transformer[all_sentences_sw],\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"{}features_{}minwords_{}context\".format(num_features, min_word_count, context)\n",
    "model.save(os.path.join(outputs,model_name))\n",
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "\n",
    "Congratulations on making it successfully through everything so far! Let's take a look at the model we created out of our 75,000 training reviews.\n",
    "\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alien\n",
      "son\n",
      "england\n",
      "[('monster_suit', 0.24731558561325073), ('envision', 0.23457247018814087), ('clooney', 0.2264252007007599), ('weather_conditions', 0.22578999400138855), ('mancha', 0.21998344361782074), ('imagined', 0.2181268185377121), ('conclusive', 0.21489593386650085), ('lanisha', 0.21364736557006836), ('rusted', 0.21336263418197632), ('prominently_featured', 0.21218660473823547)]\n",
      "[('fearlessly', 0.2223716527223587), ('pennant', 0.21564170718193054), ('vinson', 0.21473494172096252), ('sprays', 0.21357902884483337), ('sunshine_boys', 0.21285869181156158), ('javier_bardem', 0.21221990883350372), ('wouldnt', 0.21032893657684326), ('dissenting', 0.21006566286087036), ('coeds', 0.208118736743927), ('retire', 0.20568057894706726)]\n",
      "[('vigilantism', 0.23548850417137146), ('shoe', 0.23481306433677673), ('loosen', 0.22034554183483124), ('fast_forward', 0.21811792254447937), ('subway_system', 0.21725550293922424), ('infect', 0.2144487202167511), ('real.the', 0.2138945609331131), ('moreland', 0.21223381161689758), ('ankle', 0.21179741621017456), ('bacall', 0.21128150820732117)]\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"captain onion starship alien\".split()))\n",
    "print(model.doesnt_match(\"father mother son daughter film\".split()))\n",
    "print(model.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(model.most_similar(\"man\"))\n",
    "print(model.most_similar(\"queen\"))\n",
    "print(model.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a reasonably good model for semantic meaning - at least as good as Bag of Words. But how can we use these fancy distributed word vectors for supervised learning? The next section takes a stab at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: More Fun With Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors  \n",
    "\n",
    "Numeric Representations of Words\n",
    "\n",
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76357, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04021143, -0.02611026, -0.08398159,  0.07180156,  0.02742493,\n",
       "       -0.02245309, -0.09519622,  0.08570237,  0.00053754, -0.00655125], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'of', 'to', 'is', 'it', 'in', 'this', 'that', 'was']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words To Paragraphs, \n",
    "--\n",
    "Attempt 1:  Vector Averaging  \n",
    "--\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 2000th review\n",
    "        if counter%2000. == 0.:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call these functions to create average vectors for each paragraph. The following operations will take a few minutes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions \n",
    "# we defined above. Notice that we now use stop word removal.\n",
    "trainDataVecs = getAvgFeatureVecs(labeled_sentences_sw, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00259682,  0.00602138, -0.00191652,  0.00655052, -0.00224005,\n",
       "       -0.00394285,  0.00093025,  0.00457959,  0.00094036,  0.00217633], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[100][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Nan values:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(trainDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(trainDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If there are Nan values we can use inputer  \n",
    "from sklearn.preprocessing import Imputer\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(trainDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(trainDataVecs).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the average paragraph vectors with the classifiers from Part 1.  \n",
    "Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = train_test_split(trainDataVecs,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6998\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_WV = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_RF_WV_tts = clf_RF_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_RF_WV_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_WV = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=None,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_LR_WV_tts = clf_LR_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_LR_WV_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(test_labeled_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(testDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "\n",
    "print(np.isnan(testDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01402438,  0.00019457,  0.00113494,  0.00693492,  0.00524175,\n",
       "       -0.01146466, -0.01194785,  0.0042229 , -0.00121657, -0.00398368], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataVecs[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 0 1 0 0 0]\n",
      "[[ 0.52        0.48      ]\n",
      " [ 0.55666667  0.44333333]\n",
      " [ 0.49        0.51      ]\n",
      " [ 0.44        0.56      ]\n",
      " [ 0.54666667  0.45333333]\n",
      " [ 0.55        0.45      ]\n",
      " [ 0.49333333  0.50666667]\n",
      " [ 0.58666667  0.41333333]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.52666667  0.47333333]]\n"
     ]
    }
   ],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = clf_RF_WV.predict(testDataVecs)\n",
    "result_prob = clf_RF_WV.predict_proba(testDataVecs)\n",
    "print(result[0:10])\n",
    "print(result_prob[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          0\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          1\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points.\n",
    "\n",
    "Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? A standard way of weighting word vectors is to apply \"tf-idf\" weights, which measure how important a given word is within a given set of documents. One way to extract tf-idf weights in Python is by using scikit-learn's TfidfVectorizer, which has an interface similar to the CountVectorizer that we used in Part 1. However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering \n",
    "--\n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  26442.060588359833 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original Word2Vec model is still stored in model.index2word. For convenience, we zip these into one dictionary as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n",
    "Run k-means on the word vectors and print a few clusters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['fruits']\n",
      "\n",
      "Cluster 1\n",
      "['overproduced', 'spunk', 'e-mail_me']\n",
      "\n",
      "Cluster 2\n",
      "['hmm', 'doctor_mordrid', 'novelists', 'swimmingly']\n",
      "\n",
      "Cluster 3\n",
      "['am_huge', 'freak_show', 'godfather', 'eventhough', 'hench', 'macha']\n",
      "\n",
      "Cluster 4\n",
      "['kicking_ass', 'minimally', 'positive_side', 'couched']\n",
      "\n",
      "Cluster 5\n",
      "['surrounded', 'suppress', 'influences', 'malibu', 'warren', 'insistence']\n",
      "\n",
      "Cluster 6\n",
      "['don´t', 'bowman', 'leak']\n",
      "\n",
      "Cluster 7\n",
      "['graphic', 'reliance', 'shepard']\n",
      "\n",
      "Cluster 8\n",
      "['charm', 'heats']\n",
      "\n",
      "Cluster 9\n",
      "['more_accessible', 'hug', 'rosa', 'emmanuelle_seigner', 'jaco_van']\n"
     ]
    }
   ],
   "source": [
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    # Print the cluster number\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        #print(len(word_centroid_map.values()))\n",
    "        #print(cluster)\n",
    "        #print(word_centroid_map.keys())\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense, some cointain mostly names, and some contain related adjectives. On the other hand, some are a little mystifying. Perhaps our algorithm works best on adjectives.\n",
    "\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function above will give us a numpy array for each review, each with a number of features equal to the number of clusters. Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****** Create bags of centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = train_test_split(train_centroids,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "clf_RF_CT = RandomForestClassifier(n_estimators=100, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=True, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvCT, y_traincvCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4904\n"
     ]
    }
   ],
   "source": [
    "eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT)\n",
    "print(eval_RF_CT_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote BagOfCentroids.csv\n"
     ]
    }
   ],
   "source": [
    "result = clf_RF_CT.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(os.path.join(outputs,\"BagOfCentroids.csv\"), index=False, quoting=3)\n",
    "print(\"Wrote BagOfCentroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Comparing deep and non-deep learning methods\n",
    "--\n",
    "\n",
    "You may ask: Why is Bag of Words better?\n",
    "\n",
    "The biggest reason is, in our tutorial, averaging the vectors and using the centroids lose the order of words, making it very similar to the concept of Bag of Words. The fact that the performance is similar (within range of standard error) makes all three methods practically equivalent.  \n",
    "\n",
    "A few things to try:\n",
    "\n",
    "First, training Word2Vec on a lot more text should greatly improve performance. Google's results are based on word vectors that were learned out of more than a billion-word corpus; our labeled and unlabeled training sets together are only a measly 18 million words or so. Conveniently, Word2Vec provides functions to load any pre-trained model that is output by Google's original C tool, so it's also possible to train a model in C and then import it into Python.\n",
    "\n",
    "Second, in published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. In part, it does better than the approaches we try here because vector averaging and clustering lose the word order, whereas Paragraph Vectors preserves word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Deep Learning?\n",
    "\n",
    "The term \"deep learning\" was coined in 2006, and refers to machine learning algorithms that have multiple non-linear layers and can learn feature hierarchies [1].\n",
    "\n",
    "Most modern machine learning relies on feature engineering or some level of domain knowledge to obtain good results. In deep learning systems, this is not the case -- instead, algorithms can automatically learn feature hierarchies, which represent objects in increasing levels of abstraction. Although the basic ingredients of many deep learning algorithms have been around for many years, they are currently increasing in popularity for many reasons, including advances in compute power, the falling cost of computing hardware, and advances in machine learning research.\n",
    "\n",
    "Deep learning algorithms can be categorized by their architecture (feed-forward, feed-back, or bi-directional) and training protocols (purely supervised, hybrid, or unsupervised) [2]. \n",
    "\n",
    "Some good background materials include:\n",
    "\n",
    "[1] \"Deep Learning for Signal and Information Processing\", by Li Deng and Dong Yu (out of Microsoft)\n",
    "\n",
    "[2] \"Deep Learning Tutorial\" (2013 Presentation by Yann LeCun and Marc'Aurelio Ranzato)\n",
    "\n",
    "Where Does Word2Vec Fit In?\n",
    "\n",
    "Word2Vec works in a way that is similar to deep approaches such as recurrent neural nets or deep neural nets, but it implements certain algorithms, such as hierarchical softmax, that make it computationally more efficient.  \n",
    "\n",
    "See Part 2 of this tutorial for more on Word2Vec, as well as this paper: Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "In this tutorial, we use a hybrid approach to training -- consisting of an unsupervised piece (Word2Vec) followed by supervised learning (the Random Forest). \n",
    "\n",
    "Libraries and Packages \n",
    "\n",
    "The lists below should in no way be considered exhaustive.\n",
    "\n",
    "In Python:\n",
    "\n",
    "Theano offers very low-level, nuts and bolts functionality for building deep learning systems. You can also find some good tutorials on their site.  \n",
    "Caffe is a deep learning framework out of the Berkeley Vision and Learning Center.  \n",
    "Pylearn2 wraps Theano and seems slightly more user friendly.  \n",
    "OverFeat was used to win the Kaggle Cats and Dogs competition.  \n",
    "\n",
    "\n",
    "More Tutorials  \n",
    "The O'Reilly Blog has a series of deep learning articles and tutorials:  \n",
    "\n",
    "http://radar.oreilly.com/2014/07/what-is-deep-learning-and-why-should-you-care.html  \n",
    "http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html  \n",
    "Webcast: How to Get Started with Deep Learning in Computer Vision  \n",
    "There are several tutorials using Theano as well.  \n",
    "\n",
    "If you want to dive into the weeds of creating a neural network from scratch, check out Geoffrey Hinton's Coursera course.\n",
    "\n",
    "For NLP, check out this recent lecture at Stanford: http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/  \n",
    "\n",
    "This free, online book also introduces neural nets for deep learning: http://neuralnetworksanddeeplearning.com/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
