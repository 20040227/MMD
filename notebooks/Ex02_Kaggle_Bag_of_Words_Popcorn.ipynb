{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Google's Word2Vec for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial  \n",
    "https://github.com/wendykan/DeepLearningMovies  \n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/  \n",
    "\n",
    "\n",
    "In this tutorial competition, we dig a little \"deeper\" into sentiment analysis. Google's Word2Vec is a deep-learning inspired method that focuses on the meaning of words. Word2Vec attempts to understand meaning and semantic relationships among words. It works in a way that is similar to deep approaches, such as recurrent neural nets or deep neural nets, but is computationally more efficient. This tutorial focuses on Word2Vec for sentiment analysis.\n",
    "\n",
    "Sentiment analysis is a challenging subject in machine learning. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers. There's another Kaggle competition for movie review sentiment analysis. In this tutorial we explore how Word2Vec can be applied to a similar problem.\n",
    "\n",
    "Deep learning has been in the news a lot over the past few years, even making it to the front page of the New York Times. These machine learning techniques, inspired by the architecture of the human brain and made possible by recent advances in computing power, have been making waves via breakthrough results in image recognition, speech processing, and natural language tasks. Recently, deep learning approaches won several Kaggle competitions, including a drug discovery task, and cat and dog image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's charge the batteries for our analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K4200 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set  \n",
    "--\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "Data fields\n",
    "\n",
    "id - Unique ID of each review\n",
    "sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "review - Text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = \"../datasets/Kaggle/\"\n",
    "outputs = \"../outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(datapath, 'BOW_labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(os.path.join(datapath, 'BOW_testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(os.path.join(datapath, \"BOW_unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} labeled train reviews, \\\n",
    "{} labeled test reviews, and \\\n",
    "{} unlabeled reviews\\n\".format(train[\"review\"].size,\n",
    "                               test[\"review\"].size,\n",
    "                               unlabeled_train[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: For Beginners - Bag of Words\n",
    "--\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words  \n",
    "\n",
    "What is NLP?\n",
    "\n",
    "NLP (Natural Language Processing) is a set of techniques for approaching text problems. This page will help you get started with loading and cleaning the IMDB movie reviews, then applying a simple Bag of Words model to get surprisingly accurate predictions of whether a review is thumbs-up or thumbs-down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text_Cleaning_Utilities(object):\n",
    "    \"\"\"Tools for processing text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_wordlist(text, \n",
    "                         remove_stopwords=False, \n",
    "                         remove_html=False, \n",
    "                         remove_non_letters=False, \n",
    "                         steeming=False):\n",
    "        '''Split a text into a list of words'''\n",
    "        #text = text.replace('-\\n','')\n",
    "        text = text.lower()\n",
    "        if remove_html:\n",
    "            text = BeautifulSoup(text, \"html5lib\").get_text()\n",
    "        if remove_non_letters:\n",
    "            text = re.sub(\"[^-A-Za-z0-9_]\", \" \", text)\n",
    "        list_words = word_tokenize(text)\n",
    "        list_words = [w.strip(string.punctuation) for w in list_words if w not in string.punctuation]\n",
    "        list_words = [w for w in list_words if len(w) > 1]\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            list_words = [w for w in list_words if w not in stops]\n",
    "        if steeming:\n",
    "            stemmer = PorterStemmer()\n",
    "            list_words = [stemmer.stem(item) for item in list_words]\n",
    "        return list_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_texts(dataframe, column, \n",
    "                            remove_stopwords=False, \n",
    "                            remove_html=False, \n",
    "                            remove_non_letters=False, \n",
    "                            steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(' '.join(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                                 remove_stopwords=remove_stopwords,\n",
    "                                                                                 remove_html=remove_html,\n",
    "                                                                                 remove_non_letters=remove_non_letters,\n",
    "                                                                                 steeming=steeming)))\n",
    "            \n",
    "        return clean_texts\n",
    "\n",
    "    @staticmethod\n",
    "    def df_to_list_of_tokens(dataframe, column, \n",
    "                             remove_stopwords=False, \n",
    "                             remove_html=False, \n",
    "                             remove_non_letters=False, \n",
    "                             steeming=False):\n",
    "        clean_texts = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            clean_texts.append(Text_Cleaning_Utilities.text_to_wordlist(dataframe[column][txt_id],\n",
    "                                                                        remove_stopwords=remove_stopwords,\n",
    "                                                                        remove_html=remove_html,\n",
    "                                                                        remove_non_letters=remove_non_letters,\n",
    "                                                                        steeming=steeming))\n",
    "            \n",
    "        return clean_texts\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def text_to_sentences(text,\n",
    "                          remove_stopwords=False, \n",
    "                          remove_html=False, \n",
    "                          remove_non_letters=False, \n",
    "                          steeming=False,\n",
    "                          tokenizer=None,):\n",
    "        '''Split a text into parsed sentences. Returns a list of sentences, \n",
    "        where each sentence is a list of words'''\n",
    "        \n",
    "        # Load the punkt tokenizer (english) if no tokenizer passed\n",
    "        if not tokenizer:\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "        raw_sentences = tokenizer.tokenize(text.strip())\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            if len(raw_sentence) > 0:\n",
    "                sentences.append(Text_Cleaning_Utilities.text_to_wordlist(raw_sentence, \n",
    "                                                                          remove_stopwords=remove_stopwords,\n",
    "                                                                          remove_html=remove_html,\n",
    "                                                                          remove_non_letters=remove_non_letters,\n",
    "                                                                          steeming=steeming))\n",
    "        return sentences\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def df_to_list_of_sentences(dataframe, column, \n",
    "                                remove_stopwords=False, \n",
    "                                remove_html=False, \n",
    "                                remove_non_letters=False, \n",
    "                                steeming=False,\n",
    "                                tokenizer=None):\n",
    "        sentences = []\n",
    "        for txt_id in range(len(dataframe[column])):\n",
    "            sentences.append(Text_Cleaning_Utilities.text_to_sentences(dataframe[column][txt_id],\n",
    "                                                                       remove_stopwords=remove_stopwords,\n",
    "                                                                       remove_html=remove_html,\n",
    "                                                                       remove_non_letters=remove_non_letters,\n",
    "                                                                       steeming=steeming,\n",
    "                                                                       tokenizer=tokenizer))\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all the datasets and getting word lists\n",
    "--\n",
    "first set is without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                  'review', \n",
    "                                                                  remove_stopwords=True,\n",
    "                                                                  remove_html=True,)\n",
    "clean_test_reviews = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                 'review', \n",
    "                                                                 remove_stopwords=True,\n",
    "                                                                 remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuff going moment mj started listening music watching odd documentary watched w'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second set mantains stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(train, \n",
    "                                                                     'review',\n",
    "                                                                     remove_html=True,)\n",
    "\n",
    "clean_test_reviews_sw = Text_Cleaning_Utilities.df_to_list_of_texts(test, \n",
    "                                                                    'review',\n",
    "                                                                    remove_html=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with all this stuff going down at the moment with mj ve started listening to his'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally in film who main themes are of mortality nostalgia and loss of innocen'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews_sw[0][0:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'wb') as f:\n",
    "    pickle.dump((clean_train_reviews, \n",
    "                 clean_test_reviews,\n",
    "                 clean_train_reviews_sw, \n",
    "                 clean_test_reviews_sw),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    (clean_train_reviews, \n",
    "     clean_test_reviews,\n",
    "     clean_train_reviews_sw,\n",
    "     clean_test_reviews_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Features from a Bag of Words (Using scikit-learn)\n",
    "--\n",
    "\n",
    "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
    "\n",
    "Sentence 1: \"The cat sat on the hat\"  \n",
    "Sentence 2: \"The dog ate the cat and the hat\"  \n",
    "\n",
    "From these two sentences, our vocabulary is as follows:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
    "\n",
    "{ the, cat, sat, on, hat, dog, ate, and }\n",
    "\n",
    "Sentence 1: [ 2, 1, 1, 1, 1, 0, 0, 0 ]\n",
    "\n",
    "Similarly, the features for Sentence 2 are: [ 3, 1, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
    "\n",
    "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features.  \n",
    "We will test two strategies: CountVectorizer (term frequecies - TF) and TFIDF Vectorizer:  \n",
    "First we'll start with plain word counts (TF):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer_tf = CountVectorizer(input='content', \n",
    "                               encoding='utf-8', \n",
    "                               decode_error='strict', \n",
    "                               strip_accents=None, \n",
    "                               lowercase=True, \n",
    "                               preprocessor=None, \n",
    "                               tokenizer=None, \n",
    "                               stop_words=None, \n",
    "                               #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                               ngram_range=(1, 2),\n",
    "                               analyzer='word', \n",
    "                               max_df=1.0, \n",
    "                               min_df=1, \n",
    "                               max_features=5000, \n",
    "                               vocabulary=None, \n",
    "                               binary=False, \n",
    "                               dtype=np.int64,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit_transform() does two functions: First, it fits the model and learns the vocabulary; \n",
    "second, it transforms our training data into feature vectors. \n",
    "The input to fit_transform should be a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tf = vectorizer_tf.fit_transform(clean_train_reviews)\n",
    "train_data_features_tf = train_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tf = vectorizer_tf.fit_transform(clean_test_reviews)\n",
    "test_data_features_tf = test_data_features_tf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use TfIDf vectors and the train/test cleaned reviews with stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "#Another approach using TfIDf vectorizer and using the texts with stopwords in:\n",
    "#https://github.com/zygmuntz/classifying-text/blob/master/bow_predict.py \n",
    "vectorizer_tfidf = TfidfVectorizer(input='content',\n",
    "                                  #encoding='utf-8',\n",
    "                                  decode_error='strict',\n",
    "                                  strip_accents=None,\n",
    "                                  lowercase=True,\n",
    "                                  preprocessor=None,\n",
    "                                  tokenizer=None,\n",
    "                                  analyzer='word',\n",
    "                                  stop_words=None,\n",
    "                                  #token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  max_df=1.0,\n",
    "                                  min_df=1,\n",
    "                                  max_features=5000,\n",
    "                                  vocabulary=None, \n",
    "                                  binary=False, \n",
    "                                  dtype=np.int64,\n",
    "                                  norm='l2',\n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "train_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_train_reviews_sw)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(train_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "test_data_features_tfidf = vectorizer_tfidf.fit_transform(clean_test_reviews_sw)\n",
    "test_data_features_tfidf = test_data_features_tfidf.toarray() # Numpy arrays are easy to work with\n",
    "print(test_data_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'wb') as f:\n",
    "    pickle.dump((train_data_features_tf, \n",
    "                 test_data_features_tf,\n",
    "                 train_data_features_tfidf,\n",
    "                 test_data_features_tfidf),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'train_test_data_features.pkl'),'rb') as f:\n",
    "    (train_data_features_tf, \n",
    "    test_data_features_tf,\n",
    "    train_data_features_tfidf,\n",
    "    test_data_features_tfidf) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing Train set for Cross Validation  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://github.com/zygmuntz/classifying-text/blob/master/bow_validate.py  \n",
    "Alternatively, we can use the indexes to divide the train samples  \n",
    "\n",
    "train_i, test_i = train_test_split(np.arange(len(train)), train_size = 0.8, random_state = 44)  \n",
    "\n",
    "After generating indexes, we can divide ou datasets:  \n",
    "traincv = train_data_features1[train_i]  \n",
    "testcv = train_data_features1[test_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plain Word Counts\n",
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(train_data_features_tf,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "X_traincv_tfidf, X_testcv_tfidf, y_traincv_tfidf, y_testcv_tfidf = train_test_split(train_data_features_tfidf,\n",
    "                                                                                    train[\"sentiment\"],\n",
    "                                                                                    test_size=0.2,\n",
    "                                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training some Classifiers  \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! Here, we'll use some classifiers implementations included in  the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF_tf = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=0, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8632\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tf_tts = clf_RF_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_RF_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44666667,  0.55333333],\n",
       "       [ 0.38666667,  0.61333333],\n",
       "       [ 0.31333333,  0.68666667],\n",
       "       ..., \n",
       "       [ 0.39333333,  0.60666667],\n",
       "       [ 0.8       ,  0.2       ],\n",
       "       [ 0.22      ,  0.78      ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tf.predict_proba(X_testcv_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train on the TfIdf samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_tfidf = RandomForestClassifier(n_estimators=300, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth=None, \n",
    "                                      min_samples_split=2, \n",
    "                                      min_samples_leaf=1, \n",
    "                                      min_weight_fraction_leaf=0.0, \n",
    "                                      max_features='auto', \n",
    "                                      max_leaf_nodes=None, \n",
    "                                      bootstrap=False, \n",
    "                                      oob_score=False, \n",
    "                                      n_jobs=-1, \n",
    "                                      random_state=0, \n",
    "                                      verbose=0, \n",
    "                                      warm_start=False, \n",
    "                                      class_weight=None).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8536\n"
     ]
    }
   ],
   "source": [
    "eval_RF_tfidf_tts = clf_RF_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_RF_tfidf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42333333,  0.57666667],\n",
       "       [ 0.36666667,  0.63333333],\n",
       "       [ 0.33      ,  0.67      ],\n",
       "       ..., \n",
       "       [ 0.25666667,  0.74333333],\n",
       "       [ 0.75333333,  0.24666667],\n",
       "       [ 0.33      ,  0.67      ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_RF_tfidf.predict_proba(X_testcv_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=0,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_tts = clf_LR_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_LR_tfidf = LR(penalty='l2',\n",
    "                  dual=False,\n",
    "                  tol=0.0001,\n",
    "                  C=1.0,\n",
    "                  fit_intercept=True,\n",
    "                  intercept_scaling=1,\n",
    "                  class_weight=None,\n",
    "                  random_state=0,\n",
    "                  solver='liblinear',\n",
    "                  max_iter=100,\n",
    "                  multi_class='ovr',\n",
    "                  verbose=0).fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tfidf_tts = clf_LR_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_LR_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GBC_tf = GradientBoostingClassifier(loss='deviance',\n",
    "                                        learning_rate=0.1,\n",
    "                                        n_estimators=100,\n",
    "                                        subsample=1.0,\n",
    "                                        min_samples_split=2,\n",
    "                                        min_samples_leaf=1,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        max_depth=3,\n",
    "                                        init=None,\n",
    "                                        random_state=0,\n",
    "                                        max_features=None,\n",
    "                                        verbose=0,\n",
    "                                        max_leaf_nodes=None,\n",
    "                                        warm_start=False,\n",
    "                                        presort='auto').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tf_tts = clf_GBC_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_GBC_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_GBC_tfidf = GradientBoostingClassifier(loss='deviance',\n",
    "                                           learning_rate=0.1,\n",
    "                                           n_estimators=100,\n",
    "                                           subsample=1.0,\n",
    "                                           min_samples_split=2,\n",
    "                                           min_samples_leaf=1,\n",
    "                                           min_weight_fraction_leaf=0.0,\n",
    "                                           max_depth=3,\n",
    "                                           init=None,\n",
    "                                           random_state=0,\n",
    "                                           max_features=None,\n",
    "                                           verbose=0,\n",
    "                                           max_leaf_nodes=None,\n",
    "                                           warm_start=False,\n",
    "                                           presort='auto').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128\n"
     ]
    }
   ],
   "source": [
    "eval_GBC_tfidf_tts = clf_GBC_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_GBC_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do some voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf_vot_tf = VotingClassifier(estimators=[('rf', clf_RF_tf),\n",
    "                                          ('lr', clf_LR_tf),\n",
    "                                          ('gbc', clf_GBC_tf)], voting='soft').fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tf_tts = clf_vot_tf.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_vot_tf_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_vot_tfidf = VotingClassifier(estimators=[('rf', clf_RF_tfidf),\n",
    "                                             ('lr', clf_LR_tfidf),\n",
    "                                             ('gbc', clf_GBC_tfidf)], voting='soft').fit(X_traincv_tfidf, y_traincv_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8788\n"
     ]
    }
   ],
   "source": [
    "eval_vot_tfidf_tts = clf_vot_tfidf.score(X_testcv_tfidf, y_testcv_tfidf)\n",
    "print(eval_vot_tfidf_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'wb') as f:\n",
    "    pickle.dump((clf_RF_tf, eval_RF_tf_tts,\n",
    "                 clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "                 clf_LR_tf, eval_LR_tf_tts,\n",
    "                 clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "                 clf_GBC_tf, eval_GBC_tf_tts,\n",
    "                 clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "                 clf_vot_tf, eval_vot_tf_tts,\n",
    "                 clf_vot_tfidf, eval_vot_tfidf_tts),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the classifiers from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'classifiers.pkl'),'rb') as f:\n",
    "    (clf_RF_tf, eval_RF_tf_tts,\n",
    "     clf_RF_tfidf, eval_RF_tfidf_tts,\n",
    "     clf_LR_tf, eval_LR_tf_tts,\n",
    "     clf_LR_tfidf, eval_LR_tfidf_tts,                \n",
    "     clf_GBC_tf, eval_GBC_tf_tts,\n",
    "     clf_GBC_tfidf, eval_GBC_tfidf_tts,\n",
    "     clf_vot_tf, eval_vot_tf_tts,\n",
    "     clf_vot_tfidf, eval_vot_tfidf_tts) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAIyCAYAAAAE8jZRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e48622f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic_results = {'RandomForest_TF': eval_RF_tf_tts,\n",
    "               'RandomForest_TFIDF': eval_RF_tfidf_tts,\n",
    "               'LogiReg_TF': eval_LR_tf_tts,\n",
    "               'LogiReg_TFIDF': eval_LR_tfidf_tts,\n",
    "               'GradBoost_TF': eval_GBC_tf_tts,\n",
    "               'GradBoost_TFIDF': eval_GBC_tfidf_tts,\n",
    "               'Voting_TF': eval_vot_tf_tts,\n",
    "               'Voting_TFIDF': eval_vot_tfidf_tts,\n",
    "              }\n",
    "\n",
    "import operator\n",
    "tup_results = sorted(dic_results.items(), key=operator.itemgetter(1))\n",
    "\n",
    "N = len(dic_results)\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.40       # the width of the bars\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(111)\n",
    "rects = ax.bar(ind, list(zip(*tup_results))[1], width,)\n",
    "for rect in rects:\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x()+rect.get_width()/2., \n",
    "            1.005*height, \n",
    "            '{0:.4f}'.format(height), \n",
    "            ha='center', \n",
    "            va='bottom',)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(ymin=0.78,ymax = 0.92)\n",
    "ax.set_title(\"Classificators' performance\")\n",
    "ax.set_xticks(ind + width/2.)\n",
    "ax.set_xticklabels(list(zip(*tup_results))[0], rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--\n",
    "\n",
    "All that remains is to run the best classifier on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
    "\n",
    "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "id        25000 non-null object\n",
      "review    25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the logistic regression with tfidf vectors to make sentiment label predictions\n",
    "result = clf_LR_tfidf.predict(test_data_features_tfidf)\n",
    "result_prob = clf_LR_tfidf.predict_proba(test_data_features_tfidf)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'LR_tfidf_model.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          0\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          0\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Alternative Vectors\n",
    "--\n",
    "\n",
    "In the subsequent sections, we are going to explore alternate ways to codify text into vectors. We are going to explore three techniques, namely Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA) and Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the models, the more we have texts, the better. The size of the Corpus is essential for having good results. We don't need labels in order to create the models, so we will use the train examples and also some unlabeled reviews. The list of cleaned sentences will be used for all the subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Parsing sentences with stopwords from training set\n",
      "Parsing sentences with stopwords from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "labeled_sentences = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                                 'review', \n",
    "                                                                 remove_html=True,\n",
    "                                                                 remove_stopwords=True,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "all_sentences = labeled_sentences + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                 'review', \n",
    "                                                                                 remove_html=True,\n",
    "                                                                                 remove_stopwords=True,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from training set\")\n",
    "labeled_sentences_sw = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                         'review', \n",
    "                                                         remove_html=True,\n",
    "                                                         remove_stopwords=False,)\n",
    "\n",
    "print(\"Parsing sentences with stopwords from unlabeled set\")\n",
    "all_sentences_sw = labeled_sentences_sw + Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                                                       'review', \n",
    "                                                                                       remove_html=True,\n",
    "                                                                                       remove_stopwords=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "75000\n",
      "25000\n",
      "75000\n"
     ]
    }
   ],
   "source": [
    "print(len(labeled_sentences))\n",
    "print(len(all_sentences))\n",
    "print(len(labeled_sentences_sw))\n",
    "print(len(all_sentences_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving sentences to a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'wb') as f:\n",
    "    pickle.dump((labeled_sentences,\n",
    "                 all_sentences,\n",
    "                 labeled_sentences_sw,\n",
    "                 all_sentences_sw), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading sentences from a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'sentences.pkl'),'rb') as f:\n",
    "    (labeled_sentences,\n",
    "     all_sentences,\n",
    "     labeled_sentences_sw,\n",
    "     all_sentences_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(docs, additional_stopwords=set(), no_below=4, no_above=0.7):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    print('{} Tokens extracted from {} texts'.format(len(dictionary.keys()), dictionary.num_docs))\n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    #stopword_ids = [dictionary.token2id[sw] for sw in stopwords if sw in dictionary.token2id]\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    #low_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < 4]\n",
    "    #dictionary.filter_tokens(low_freq_ids)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "    print('{} Tokens after cleaning'.format(len(dictionary.keys())))\n",
    "    #print('Building corpus...')\n",
    "    #corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    return dictionary #, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compacting and saving the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "256792 Tokens extracted from 75000 texts\n",
      "56378 Tokens after cleaning\n",
      "dictionary done\n",
      "dictionary saved\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords=set(['n\\'t', 'movie'])\n",
    "\n",
    "dictionary = prep_corpus(all_sentences, additional_stopwords)\n",
    "dictionary.compactify()\n",
    "print('dictionary done')\n",
    "\n",
    "dictionary.save(os.path.join(outputs, 'reviews.dict'))\n",
    "print('dictionary saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18606\n"
     ]
    }
   ],
   "source": [
    "#print(dictionary.token2id['movie']) #verify if these words were in the stopwords list\n",
    "#print(dictionary.token2id['n\\'t'])\n",
    "print(dictionary.token2id['like'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.dictionary.Dictionary.load(os.path.join(outputs, 'reviews.dict'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Corpora (tf and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus tf done\n",
      "corpus tfidf done\n"
     ]
    }
   ],
   "source": [
    "corpus_tf = [dictionary.doc2bow(sentence) for sentence in all_sentences]\n",
    "print('corpus tf done')\n",
    "tfidf = models.TfidfModel(corpus_tf)\n",
    "corpus_tfidf = tfidf[corpus_tf]\n",
    "print('corpus tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tf.mm'), corpus_tf)\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_tfidf.mm'), corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tf.mm'))\n",
    "corpus_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "https://en.wikipedia.org/wiki/Latent_semantic_analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lsi - TF done\n",
      "corpus lsi - TFIDF done\n"
     ]
    }
   ],
   "source": [
    "lsi_tf = models.LsiModel(corpus_tf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tf = lsi_tf[corpus_tf]\n",
    "print('corpus lsi - TF done')\n",
    "lsi_tfidf = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi_tfidf = lsi_tfidf[corpus_tfidf]\n",
    "print('corpus lsi - TFIDF done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf.save(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tf.mm'), corpus_lsi_tf)\n",
    "lsi_tfidf.save(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lsi_tfidf.mm'), corpus_lsi_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_tf = models.LsiModel.load(os.path.join(outputs, 'model_tf.lsi'))\n",
    "corpus_lsi_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tf.mm'))\n",
    "lsi_tfidf = models.LsiModel.load(os.path.join(outputs, 'model_tfidf.lsi'))\n",
    "corpus_lsi_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lsi_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.508*\"film\" + 0.290*\"one\" + 0.218*\"like\" + 0.147*\"would\" + 0.145*\"good\" + 0.137*\"even\" + 0.125*\"time\" + 0.123*\"really\" + 0.119*\"story\" + 0.117*\"see\"'),\n",
       " (1,\n",
       "  '0.833*\"film\" + -0.214*\"one\" + -0.195*\"like\" + -0.113*\"show\" + -0.105*\"movies\" + -0.105*\"good\" + -0.101*\"would\" + -0.093*\"even\" + -0.092*\"really\" + -0.085*\"get\"'),\n",
       " (2,\n",
       "  '-0.728*\"one\" + 0.456*\"like\" + 0.179*\"bad\" + 0.164*\"really\" + 0.153*\"good\" + 0.124*\"would\" + 0.095*\"people\" + 0.079*\"think\" + -0.076*\"two\" + 0.072*\"could\"'),\n",
       " (3,\n",
       "  '-0.493*\"one\" + -0.424*\"like\" + 0.332*\"story\" + -0.243*\"bad\" + 0.139*\"show\" + 0.138*\"great\" + 0.137*\"also\" + 0.136*\"life\" + 0.128*\"love\" + 0.119*\"character\"'),\n",
       " (4,\n",
       "  '-0.622*\"good\" + 0.409*\"like\" + -0.310*\"bad\" + -0.249*\"really\" + 0.157*\"would\" + 0.144*\"people\" + 0.130*\"show\" + -0.129*\"acting\" + 0.117*\"life\" + -0.117*\"great\"'),\n",
       " (5,\n",
       "  '-0.523*\"like\" + 0.477*\"would\" + 0.235*\"bad\" + -0.226*\"story\" + 0.223*\"even\" + -0.197*\"good\" + 0.195*\"could\" + -0.184*\"great\" + -0.143*\"also\" + 0.100*\"get\"'),\n",
       " (6,\n",
       "  '-0.690*\"show\" + 0.388*\"story\" + -0.179*\"great\" + 0.173*\"even\" + -0.159*\"good\" + -0.148*\"series\" + 0.147*\"bad\" + -0.113*\"really\" + -0.103*\"episode\" + -0.089*\"one\"'),\n",
       " (7,\n",
       "  '0.680*\"would\" + 0.274*\"story\" + -0.225*\"bad\" + -0.199*\"get\" + 0.187*\"good\" + -0.167*\"time\" + 0.161*\"one\" + 0.157*\"like\" + -0.138*\"really\" + -0.138*\"people\"'),\n",
       " (8,\n",
       "  '0.509*\"really\" + 0.341*\"people\" + 0.326*\"see\" + -0.269*\"even\" + 0.241*\"story\" + -0.194*\"good\" + 0.143*\"think\" + -0.136*\"time\" + -0.128*\"first\" + 0.123*\"love\"'),\n",
       " (9,\n",
       "  '0.474*\"story\" + 0.385*\"show\" + -0.349*\"good\" + 0.245*\"characters\" + 0.245*\"even\" + -0.219*\"would\" + 0.189*\"bad\" + -0.169*\"see\" + -0.145*\"man\" + -0.116*\"get\"')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.83285428152342944),\n",
       " ('one', -0.21358596390341866),\n",
       " ('like', -0.19497327981629703),\n",
       " ('show', -0.11341435476865828),\n",
       " ('movies', -0.10546302441842066),\n",
       " ('good', -0.1051895649305724),\n",
       " ('would', -0.10066215985892872),\n",
       " ('even', -0.092800068385879064),\n",
       " ('really', -0.091542218585148397),\n",
       " ('get', -0.084738409991278546)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tf.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.146*\"film\" + 0.103*\"bad\" + 0.101*\"good\" + 0.101*\"like\" + 0.101*\"really\" + 0.095*\"one\" + 0.094*\"would\" + 0.091*\"story\" + 0.089*\"see\" + 0.088*\"even\"'),\n",
       " (1,\n",
       "  '-0.333*\"bad\" + -0.199*\"worst\" + -0.133*\"waste\" + -0.131*\"acting\" + -0.130*\"movies\" + -0.116*\"ever\" + -0.114*\"terrible\" + -0.106*\"horrible\" + -0.106*\"horror\" + -0.102*\"stupid\"'),\n",
       " (2,\n",
       "  '-0.670*\"show\" + -0.215*\"episode\" + -0.212*\"series\" + 0.171*\"film\" + -0.151*\"episodes\" + -0.149*\"tv\" + -0.143*\"season\" + 0.135*\"horror\" + -0.125*\"funny\" + -0.116*\"shows\"'),\n",
       " (3,\n",
       "  '0.228*\"book\" + 0.212*\"great\" + -0.211*\"show\" + -0.156*\"horror\" + 0.116*\"movies\" + 0.110*\"seen\" + 0.107*\"best\" + 0.106*\"read\" + 0.104*\"film\" + 0.101*\"love\"'),\n",
       " (4,\n",
       "  '-0.246*\"horror\" + -0.218*\"series\" + -0.170*\"show\" + -0.166*\"action\" + 0.156*\"life\" + 0.150*\"people\" + -0.132*\"effects\" + 0.126*\"book\" + -0.123*\"original\" + -0.116*\"great\"'),\n",
       " (5,\n",
       "  '-0.369*\"book\" + 0.368*\"funny\" + 0.269*\"comedy\" + -0.197*\"series\" + -0.149*\"read\" + 0.123*\"jokes\" + 0.119*\"fun\" + 0.118*\"great\" + 0.118*\"laugh\" + -0.116*\"show\"'),\n",
       " (6,\n",
       "  '-0.403*\"horror\" + 0.233*\"bad\" + 0.148*\"worst\" + -0.130*\"scary\" + 0.126*\"show\" + -0.115*\"gore\" + -0.111*\"saw\" + 0.110*\"script\" + -0.108*\"really\" + 0.105*\"comedy\"'),\n",
       " (7,\n",
       "  '0.345*\"book\" + -0.254*\"ever\" + -0.225*\"worst\" + -0.212*\"seen\" + -0.152*\"horror\" + 0.149*\"characters\" + -0.138*\"films\" + 0.137*\"really\" + 0.132*\"good\" + -0.126*\"movies\"'),\n",
       " (8,\n",
       "  '-0.486*\"book\" + 0.177*\"film\" + -0.151*\"read\" + 0.149*\"show\" + 0.122*\"people\" + -0.117*\"version\" + 0.117*\"characters\" + -0.114*\"bad\" + -0.108*\"series\" + 0.099*\"films\"'),\n",
       " (9,\n",
       "  '0.362*\"horror\" + -0.321*\"action\" + -0.294*\"game\" + 0.223*\"book\" + 0.187*\"show\" + 0.184*\"funny\" + 0.162*\"comedy\" + 0.125*\"film\" + -0.115*\"series\" + -0.108*\"war\"')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', -0.33334074908872263),\n",
       " ('worst', -0.19851027592625292),\n",
       " ('waste', -0.13302279193058195),\n",
       " ('acting', -0.13089173380229058),\n",
       " ('movies', -0.12970661090794308),\n",
       " ('ever', -0.11571648146197057),\n",
       " ('terrible', -0.11417433836761989),\n",
       " ('horrible', -0.10649801308715751),\n",
       " ('horror', -0.10637344074158102),\n",
       " ('stupid', -0.10186137006242267)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_tfidf.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf = models.LdaModel(corpus_tf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf = lda_tf[corpus_tf]\n",
    "print('corpus lda tf done')\n",
    "lda_tfidf = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tfidf = lda_tfidf[corpus_tfidf]\n",
    "print('corpus lda tfidf done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_tf.save(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf.mm'), corpus_lda_tf)\n",
    "lda_tfidf.save(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tfidf.mm'), corpus_lda_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Models and the Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf.lda'))\n",
    "corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf.mm'))\n",
    "lda_tfidf = models.LdaModel.load(os.path.join(outputs, 'model_tfidf.lda'))\n",
    "corpus_lda_tfidf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tfidf.mm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*film + 0.010*films + 0.008*one + 0.005*great + 0.004*director + 0.004*best + 0.004*action'),\n",
       " (1,\n",
       "  '0.046*show + 0.019*series + 0.015*tv + 0.013*episode + 0.013*funny + 0.010*comedy + 0.009*shows'),\n",
       " (2,\n",
       "  '0.029*film + 0.008*characters + 0.008*one + 0.007*character + 0.006*story + 0.006*much + 0.006*plot'),\n",
       " (3,\n",
       "  '0.009*film + 0.008*one + 0.007*role + 0.007*comedy + 0.006*best + 0.006*great + 0.005*cast'),\n",
       " (4,\n",
       "  '0.012*horror + 0.008*effects + 0.007*one + 0.007*original + 0.006*game + 0.005*special + 0.005*like'),\n",
       " (5,\n",
       "  '0.016*like + 0.015*bad + 0.012*one + 0.011*even + 0.010*film + 0.010*would + 0.010*good'),\n",
       " (6,\n",
       "  '0.012*film + 0.011*people + 0.008*world + 0.007*us + 0.007*war + 0.006*life + 0.006*would'),\n",
       " (7,\n",
       "  '0.009*man + 0.009*young + 0.008*life + 0.008*woman + 0.008*wife + 0.007*father + 0.007*mother'),\n",
       " (8,\n",
       "  '0.009*one + 0.008*get + 0.006*back + 0.006*scene + 0.006*man + 0.005*gets + 0.005*two'),\n",
       " (9,\n",
       "  '0.019*film + 0.015*one + 0.013*great + 0.012*good + 0.011*see + 0.011*like + 0.011*story')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.print_topics(num_topics=10, num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 0.016194900422460239),\n",
       " ('bad', 0.014990831908964681),\n",
       " ('one', 0.012421506659957882),\n",
       " ('even', 0.010601281394441957),\n",
       " ('film', 0.010491370697278778),\n",
       " ('would', 0.010393662994182387),\n",
       " ('good', 0.009910498925612226),\n",
       " ('really', 0.0089886140899520829),\n",
       " ('could', 0.0088082659444946674),\n",
       " ('see', 0.0073837516262547252)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tf.show_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*travolta + 0.002*frustrating + 0.002*pryor + 0.002*baseball + 0.001*loy + 0.001*bunuel + 0.001*nicole'),\n",
       " (1,\n",
       "  '0.003*khan + 0.003*bollywood + 0.003*spider + 0.003*stores + 0.003*holden + 0.002*it`s + 0.002*hoskins'),\n",
       " (2,\n",
       "  '0.001*lady + 0.001*grant + 0.001*company + 0.001*office + 0.001*indian + 0.001*charlie + 0.001*dennis'),\n",
       " (3,\n",
       "  '0.002*murphy + 0.002*bambi + 0.002*bugs + 0.002*massacre + 0.001*thinks + 0.001*dogs + 0.001*seagal'),\n",
       " (4,\n",
       "  '0.002*karloff + 0.002*batman + 0.001*cannibal + 0.001*beatles + 0.001*spy + 0.001*korean + 0.001*13th'),\n",
       " (5,\n",
       "  '0.002*ninja + 0.002*holocaust + 0.002*pink + 0.002*werewolf + 0.001*buscemi + 0.001*heist + 0.001*shark'),\n",
       " (6,\n",
       "  '0.003*film + 0.002*good + 0.002*like + 0.002*bad + 0.002*one + 0.002*really + 0.002*story'),\n",
       " (7,\n",
       "  '0.003*cagney + 0.002*terminator + 0.002*violin + 0.002*hooper + 0.002*edwards + 0.002*sucker + 0.002*axe'),\n",
       " (8,\n",
       "  '0.004*chaney + 0.003*turkish + 0.003*tripe + 0.002*sherlock + 0.002*stalin + 0.002*campus + 0.002*garbo'),\n",
       " (9,\n",
       "  '0.006*sandler + 0.005*crawford + 0.004*fonda + 0.003*godzilla + 0.002*welles + 0.002*shore + 0.002*adam')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.print_topics(num_topics=10, num_words=7)\n",
    "#lda_tfidf.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('travolta', 0.0017811503022605269),\n",
       "   ('frustrating', 0.001669392150998225),\n",
       "   ('pryor', 0.0015205201876349712),\n",
       "   ('baseball', 0.0015160212097134198),\n",
       "   ('loy', 0.0014463250433967964),\n",
       "   ('bunuel', 0.0013830315699520385),\n",
       "   ('nicole', 0.0013410222590946766),\n",
       "   ('bernie', 0.0013149751732040823),\n",
       "   (\"o'hara\", 0.0012980745350681165),\n",
       "   ('gorilla', 0.0012841433280566222),\n",
       "   ('gable', 0.0012453228275161416),\n",
       "   ('doris', 0.0012415407356415175),\n",
       "   ('brody', 0.001197401889687323),\n",
       "   ('bacon', 0.0011962201871007013),\n",
       "   ('emmanuelle', 0.0011867371602922627),\n",
       "   ('jill', 0.0011824009580979646),\n",
       "   ('angels', 0.0011673850860900328),\n",
       "   ('kidman', 0.0011401617178576457),\n",
       "   ('lately', 0.0011326578774540281),\n",
       "   ('lou', 0.0011010131357451075)]),\n",
       " (1,\n",
       "  [('khan', 0.0034869199308461391),\n",
       "   ('bollywood', 0.0029767388018837257),\n",
       "   ('spider', 0.0026504394310253431),\n",
       "   ('stores', 0.0026496629445836328),\n",
       "   ('holden', 0.0025006689765431907),\n",
       "   ('it`s', 0.0022306847938198),\n",
       "   ('hoskins', 0.0019640103788521879),\n",
       "   ('rani', 0.0019623757590351812),\n",
       "   ('delpy', 0.0019127590118775324),\n",
       "   ('stop-motion', 0.0019039541335621169),\n",
       "   ('cate', 0.0018841476770045914),\n",
       "   ('bullock', 0.0018704019406768208),\n",
       "   ('laws', 0.0018332764266882969),\n",
       "   ('tucker', 0.0017260579434878222),\n",
       "   ('juliette', 0.0016422250779896072),\n",
       "   ('vivek', 0.0015695803361244274),\n",
       "   ('nero', 0.0015666729236917317),\n",
       "   ('nbc', 0.001565964582286788),\n",
       "   ('binoche', 0.0015192587615219116),\n",
       "   ('sitcoms', 0.0015008428170480692)]),\n",
       " (2,\n",
       "  [('lady', 0.0013610223743522715),\n",
       "   ('grant', 0.0013399031847400181),\n",
       "   ('company', 0.0013232967316000961),\n",
       "   ('office', 0.0011960226683595944),\n",
       "   ('indian', 0.00116720860836574),\n",
       "   ('charlie', 0.001153203874562679),\n",
       "   ('dennis', 0.0011275238732474397),\n",
       "   ('building', 0.0010844558123717226),\n",
       "   ('color', 0.0010756252572377111),\n",
       "   ('cute', 0.0010324528322603875),\n",
       "   ('purpose', 0.00099812372131889053),\n",
       "   ('ahead', 0.00099707853353537624),\n",
       "   ('witty', 0.0009958946807907946),\n",
       "   ('chuck', 0.00099199193045163936),\n",
       "   ('patrick', 0.00096337310449619285),\n",
       "   ('martial', 0.00094407041697680647),\n",
       "   ('disturbing', 0.0009317199624456192),\n",
       "   ('arts', 0.00092675231126706256),\n",
       "   ('holmes', 0.00090413445661091034),\n",
       "   ('feelings', 0.00090341059553543889)]),\n",
       " (3,\n",
       "  [('murphy', 0.0019856857017599479),\n",
       "   ('bambi', 0.0017838938828537013),\n",
       "   ('bugs', 0.0017677174590324644),\n",
       "   ('massacre', 0.001566526708935133),\n",
       "   ('thinks', 0.0014897109640176639),\n",
       "   ('dogs', 0.0014231692862285417),\n",
       "   ('seagal', 0.0013975878934520665),\n",
       "   ('chainsaw', 0.0013659274616143623),\n",
       "   ('lucky', 0.0013251633110747791),\n",
       "   ('drunk', 0.0013015353607848733),\n",
       "   ('hopefully', 0.0012873870024690415),\n",
       "   ('halloween', 0.0012856117764271082),\n",
       "   ('candyman', 0.0012715917961276121),\n",
       "   ('predator', 0.0012475863133263208),\n",
       "   ('doll', 0.0012258140746117009),\n",
       "   ('punk', 0.0011515686508109808),\n",
       "   ('regret', 0.0010910057370719845),\n",
       "   ('renting', 0.0010761216755093995),\n",
       "   ('texas', 0.001024091402586568),\n",
       "   ('stopped', 0.00099348876156346534)]),\n",
       " (4,\n",
       "  [('karloff', 0.0017980061150867483),\n",
       "   ('batman', 0.0016347648004712144),\n",
       "   ('cannibal', 0.0014939580071043543),\n",
       "   ('beatles', 0.0014902519160193801),\n",
       "   ('spy', 0.0014601004789690261),\n",
       "   ('korean', 0.001449637793653728),\n",
       "   ('13th', 0.001447995898297705),\n",
       "   ('kevin', 0.0014454837398919724),\n",
       "   ('robots', 0.0012201416956901514),\n",
       "   ('coppola', 0.0011642635611351748),\n",
       "   ('daffy', 0.0011344037897245347),\n",
       "   ('conspiracy', 0.0011325306095014219),\n",
       "   ('rats', 0.0011219826488305096),\n",
       "   ('knowledge', 0.0010701403488617991),\n",
       "   ('core', 0.0010659092529665235),\n",
       "   ('inspiring', 0.0010643202961723114),\n",
       "   ('casual', 0.0010433468948161878),\n",
       "   ('display', 0.00099925819449523117),\n",
       "   ('francis', 0.00099070350945338562),\n",
       "   ('virus', 0.0009893278758547105)]),\n",
       " (5,\n",
       "  [('ninja', 0.0020409986712016104),\n",
       "   ('holocaust', 0.0020062211304448306),\n",
       "   ('pink', 0.0018518755668384137),\n",
       "   ('werewolf', 0.001850578753024925),\n",
       "   ('buscemi', 0.001467694856139296),\n",
       "   ('heist', 0.0014522978379372484),\n",
       "   ('shark', 0.0013943568794835357),\n",
       "   ('agreed', 0.0013488175737795928),\n",
       "   ('berry', 0.0013326740549823291),\n",
       "   ('baldwin', 0.0013088829940475519),\n",
       "   ('lance', 0.0013036507442982649),\n",
       "   ('franco', 0.0012828128773405521),\n",
       "   ('bronson', 0.0012542658885569386),\n",
       "   ('grandfather', 0.0012092463455122418),\n",
       "   ('tcm', 0.001191091207230199),\n",
       "   ('finger', 0.0011639704789272068),\n",
       "   ('hogan', 0.001163387673915493),\n",
       "   ('midler', 0.0011610591721701818),\n",
       "   ('guest', 0.0011548076161874558),\n",
       "   ('karen', 0.0011501423684064311)]),\n",
       " (6,\n",
       "  [('film', 0.0032870860213968667),\n",
       "   ('good', 0.0022167763005669391),\n",
       "   ('like', 0.0021795482166254783),\n",
       "   ('bad', 0.0021461428933118153),\n",
       "   ('one', 0.0021275561456660609),\n",
       "   ('really', 0.0020898209117671453),\n",
       "   ('story', 0.0020308203838850939),\n",
       "   ('would', 0.0020271931405971355),\n",
       "   ('great', 0.0019866740428378009),\n",
       "   ('see', 0.0019379027704795999),\n",
       "   ('even', 0.0018961703829841884),\n",
       "   ('time', 0.0018612326427007646),\n",
       "   ('movies', 0.0018467985372286806),\n",
       "   ('people', 0.0018338583430823311),\n",
       "   ('could', 0.0018020309385263509),\n",
       "   ('well', 0.001763213848031162),\n",
       "   ('watch', 0.0017080781116692101),\n",
       "   ('much', 0.0016895124445930653),\n",
       "   ('get', 0.0016720367786175814),\n",
       "   ('show', 0.0016673616177961838)]),\n",
       " (7,\n",
       "  [('cagney', 0.0026746726025687929),\n",
       "   ('terminator', 0.0024439935891038462),\n",
       "   ('violin', 0.0020675254456287017),\n",
       "   ('hooper', 0.0020528987351565988),\n",
       "   ('edwards', 0.0020380339910986045),\n",
       "   ('sucker', 0.0018658796552165863),\n",
       "   ('axe', 0.0018528590783589403),\n",
       "   ('sarandon', 0.0018413976834761789),\n",
       "   ('sh*t', 0.0018283988349344941),\n",
       "   ('benny', 0.0018259871428690495),\n",
       "   ('heston', 0.0017317567271821028),\n",
       "   ('lad', 0.001668844650588139),\n",
       "   ('vividly', 0.001568661762075021),\n",
       "   ('investigating', 0.0015659968573156591),\n",
       "   ('beth', 0.00155862472082328),\n",
       "   ('sf', 0.0015101498789471476),\n",
       "   ('rooms', 0.0014666564994053641),\n",
       "   ('heather', 0.001465205299073465),\n",
       "   ('dna', 0.0014436609805812514),\n",
       "   ('arnold', 0.0014258998970216317)]),\n",
       " (8,\n",
       "  [('chaney', 0.004256829624159404),\n",
       "   ('turkish', 0.0027270255297051071),\n",
       "   ('tripe', 0.0027092132566094115),\n",
       "   ('sherlock', 0.0024219708731935707),\n",
       "   ('stalin', 0.00223477403477205),\n",
       "   ('campus', 0.0021178031055575177),\n",
       "   ('garbo', 0.0019444153799940197),\n",
       "   ('tierney', 0.001749029344718314),\n",
       "   ('lon', 0.0017373893706460106),\n",
       "   ('psychology', 0.0016339432148229486),\n",
       "   ('schindler', 0.001588416733005481),\n",
       "   ('pearl', 0.001582312534783904),\n",
       "   ('pigeon', 0.0015173824482070549),\n",
       "   ('milestone', 0.0015154975380233169),\n",
       "   ('harbor', 0.0015106603017939252),\n",
       "   ('betrayed', 0.001496399906947814),\n",
       "   ('protection', 0.0014745960488997552),\n",
       "   ('clouseau', 0.0014617986582189614),\n",
       "   ('cleese', 0.0014312592655345597),\n",
       "   ('demi', 0.0014141447836932099)]),\n",
       " (9,\n",
       "  [('sandler', 0.005972636610038354),\n",
       "   ('crawford', 0.0047321498493013859),\n",
       "   ('fonda', 0.0036712649708218253),\n",
       "   ('godzilla', 0.0029288169275052296),\n",
       "   ('welles', 0.0023399775472695275),\n",
       "   ('shore', 0.0022217430517941497),\n",
       "   ('adam', 0.0021176376291737794),\n",
       "   ('courtroom', 0.0021095695407554803),\n",
       "   ('costello', 0.0021093901448050222),\n",
       "   ('jenny', 0.0020269114028166734),\n",
       "   ('asoka', 0.0019307145172374144),\n",
       "   ('shields', 0.0018804689801045678),\n",
       "   ('hellraiser', 0.0018344662972582385),\n",
       "   ('marx', 0.0017066568716876205),\n",
       "   ('empire', 0.0016947517675209309),\n",
       "   ('miike', 0.0016863534413362235),\n",
       "   ('sharpe', 0.0016851026198519763),\n",
       "   ('sophie', 0.0016404539428287962),\n",
       "   ('conway', 0.0016328931257369764),\n",
       "   ('regards', 0.0016113593002858681)])]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_tfidf.show_topics(formatted=False, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing using PyLDAvis  \n",
    "http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1050951399045543141524567858469\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1050951399045543141524567858469_data = {\"mdsDat\": {\"x\": [-0.15064920830752435, -0.10411110587026587, -0.1270164667407319, -0.043545944712107536, -0.08256922663011246, 0.19602522822124935, 0.27739267302190523, 0.03536616506986994, 0.03001615584556612, -0.03090826989784831], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"y\": [-0.11248943785073999, -0.09082252501823632, -0.03175213639705951, -0.12786245686058528, 0.14366102938584815, -0.1083041299582548, -0.023886288357683593, -0.0321585017385261, 0.27426390762701763, 0.10935053916821987], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [19.417537790808744, 16.15892764239022, 15.697738513820708, 9.913883659218074, 8.29260691508051, 8.14516665120675, 6.921352748797196, 6.1510319078453595, 5.738262919455815, 3.5634912513766075]}, \"R\": 30, \"topic.order\": [6, 3, 10, 7, 1, 9, 8, 5, 4, 2], \"lambda.step\": 0.01, \"plot.opts\": {\"ylab\": \"PC2\", \"xlab\": \"PC1\"}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.383, -5.822, -5.9562, -6.4115, -7.1044, -7.2501, -7.5982, -7.6105, -7.9846, -8.0785, -8.1693, -8.3956, -8.4218, -8.4746, -8.5106, -8.5983, -8.7451, -8.7594, -8.7922, -8.7937, -8.9785, -9.0015, -9.0396, -9.1152, -9.1532, -9.171, -9.175, -9.2183, -9.252, -9.2758, -6.1251, -7.72, -4.2003, -5.9546, -5.9073, -7.7127, -8.2598, -8.2217, -6.7708, -6.1331, -7.2049, -7.9922, -6.243, -8.0952, -6.4725, -7.4163, -7.0081, -7.1519, -6.833, -5.9941, -5.0401, -5.8807, -5.7003, -5.331, -6.2033, -5.1211, -4.1231, -4.7321, -4.9373, -4.5468, -5.0589, -4.5666, -5.4851, -4.7118, -5.3418, -5.225, -5.46, -5.6125, -4.6142, -5.045, -5.1946, -4.9815, -5.1003, -5.2943, -5.0018, -4.3883, -5.177, -4.9085, -4.9845, -4.5572, -5.5078, -5.4525, -5.4524, -7.7991, -7.9233, -8.2302, -8.2655, -8.3142, -8.3933, -8.5054, -8.528, -8.528, -8.5372, -8.7739, -8.7995, -8.804, -8.806, -8.8358, -8.8792, -8.9528, -8.9926, -9.0228, -9.0483, -9.0591, -9.1027, -9.1182, -9.151, -9.1539, -9.1551, -9.1742, -9.1749, -9.1819, -9.1863, -8.9671, -8.376, -8.5752, -8.9173, -7.6713, -7.9692, -5.7968, -7.6783, -7.0986, -6.8564, -7.1372, -8.4361, -7.0136, -7.5753, -4.7898, -7.2263, -5.9227, -6.3904, -5.6855, -5.4437, -4.9722, -6.6294, -3.5276, -5.9324, -5.6473, -7.0415, -5.1266, -7.5982, -6.0185, -6.9357, -7.0034, -6.9164, -5.4385, -6.718, -5.7959, -6.9791, -6.0029, -5.7383, -5.1028, -6.4461, -5.953, -5.5252, -6.2415, -6.3388, -5.4368, -5.0971, -5.8721, -5.7911, -5.5533, -6.1506, -4.7999, -5.24, -5.5756, -5.5306, -6.027, -5.7616, -5.8718, -5.8861, -5.3096, -5.6561, -5.647, -5.7397, -5.7355, -5.7759, -5.8502, -5.8191, -7.7096, -8.2993, -8.3538, -8.4028, -8.652, -8.6956, -8.799, -8.855, -8.8636, -8.874, -8.9239, -8.953, -8.9942, -8.9988, -9.1055, -9.1237, -9.2015, -9.2296, -9.2449, -9.2573, -9.2608, -9.2783, -9.3253, -9.3308, -9.3335, -9.338, -9.3663, -9.3706, -9.4073, -9.4139, -7.8299, -8.2144, -7.9616, -5.9344, -6.0702, -5.6633, -8.5708, -7.3369, -6.0142, -6.2334, -4.3761, -6.0371, -6.7617, -6.0337, -4.8605, -5.4372, -6.1342, -7.2488, -6.3671, -4.4669, -5.9434, -6.1278, -4.5142, -4.5448, -4.7124, -6.1354, -4.384, -4.9893, -5.105, -5.1149, -5.6614, -5.1549, -5.0703, -4.2316, -5.9184, -3.9748, -4.4943, -4.8566, -5.079, -5.0499, -5.2539, -4.8497, -5.0908, -5.2783, -5.4425, -5.6215, -5.4593, -5.323, -5.4646, -5.424, -5.4329, -5.4303, -5.4392, -5.6819, -7.0884, -7.2306, -7.3393, -7.4015, -7.6241, -7.6384, -7.6851, -7.7867, -7.7877, -7.9632, -7.9802, -8.0037, -8.0082, -8.0284, -8.0419, -8.0506, -8.0525, -8.0628, -8.0696, -8.0724, -8.1247, -8.1527, -8.166, -8.2103, -8.2555, -8.2737, -8.3003, -8.3011, -8.3033, -6.2432, -6.859, -7.3342, -5.0301, -6.7106, -7.712, -7.2962, -6.377, -6.2079, -6.6702, -7.6223, -7.8546, -4.8157, -6.0607, -5.7689, -6.2211, -7.0346, -7.2663, -5.3762, -6.7735, -6.9131, -5.0231, -5.8033, -6.7157, -6.5389, -6.709, -6.7964, -4.547, -6.4039, -6.2612, -5.0862, -6.2986, -6.4953, -6.4265, -5.9903, -4.4455, -6.4212, -5.1114, -5.4498, -6.2364, -5.7188, -6.2915, -5.2804, -5.5973, -5.642, -6.1646, -5.9094, -6.1877, -5.984, -5.8349, -6.0853, -5.8827, -5.9551, -5.868, -5.998, -5.9909, -6.0128, -6.0338, -6.5609, -7.2959, -7.3516, -7.3821, -7.3834, -7.5388, -7.5647, -7.5867, -7.7823, -7.7829, -7.8176, -7.8789, -7.8935, -7.9513, -7.962, -7.9856, -7.9977, -8.0355, -8.0696, -8.0732, -8.0877, -8.0937, -8.1093, -8.1114, -8.1174, -8.1226, -8.1284, -8.1332, -8.1658, -8.1795, -7.661, -5.9204, -7.2355, -7.4496, -7.8919, -7.8461, -7.7725, -6.6995, -6.3967, -5.9857, -6.1372, -4.6543, -7.315, -6.813, -7.12, -3.5463, -6.7182, -6.3032, -6.0136, -6.4705, -5.4693, -5.4095, -5.9881, -6.4091, -5.9556, -5.5206, -5.735, -5.9352, -6.1494, -6.4346, -5.9438, -5.9504, -5.4216, -6.2828, -5.2851, -4.8048, -6.2835, -5.7908, -6.2515, -5.4829, -5.4785, -5.9353, -6.0687, -5.5744, -5.8431, -5.9306, -5.9691, -5.996, -6.139, -6.0988, -6.0611, -6.6615, -6.716, -6.8356, -6.8957, -7.0291, -7.176, -7.226, -7.2972, -7.3144, -7.3358, -7.4023, -7.4088, -7.4545, -7.4671, -7.6293, -7.6699, -7.6774, -7.7376, -7.7568, -7.8013, -7.8167, -7.8513, -7.8803, -7.9443, -8.0078, -8.0246, -8.0292, -8.0356, -8.0439, -8.0831, -5.8467, -6.085, -5.4306, -5.427, -6.944, -6.6072, -6.0478, -6.9808, -6.1947, -7.3247, -6.4898, -6.6121, -6.4535, -5.6544, -6.8407, -6.6557, -6.7525, -5.8713, -5.227, -6.9094, -5.0772, -5.4014, -6.1364, -6.2535, -6.5989, -4.8162, -6.0272, -5.1774, -5.1827, -5.6489, -5.6368, -6.1272, -6.4249, -4.6823, -5.3328, -5.5232, -5.8217, -5.7219, -5.9023, -6.1353, -6.2152, -5.8634, -6.1706, -5.7086, -5.9798, -5.9012, -5.9996, -5.9342, -5.906, -6.0636, -5.2488, -6.5306, -7.292, -7.4498, -7.4662, -7.4755, -7.4954, -7.5435, -7.5956, -7.6172, -7.6502, -7.7194, -7.8126, -7.8548, -7.8631, -7.9244, -7.94, -7.9483, -7.9494, -7.9581, -7.963, -7.9723, -7.9812, -7.9832, -8.0327, -8.073, -8.0839, -8.1048, -8.1237, -8.1316, -7.6857, -6.7661, -6.2326, -7.5952, -7.1056, -4.932, -5.9201, -7.4178, -4.8977, -5.2707, -5.2334, -7.0008, -7.1506, -6.979, -4.8589, -4.7856, -6.8803, -7.0491, -4.7041, -6.9331, -4.9975, -6.6152, -5.8476, -6.0606, -6.3374, -5.946, -4.699, -6.6242, -6.6753, -4.7784, -5.9382, -5.7993, -5.4065, -5.8627, -4.9649, -5.9137, -5.7191, -6.3542, -5.7637, -5.6596, -5.9472, -6.2071, -6.14, -5.7071, -5.7814, -5.6744, -5.821, -5.5921, -5.7066, -5.5194, -5.6497, -5.7359, -6.0136, -6.2338, -6.2937, -6.3182, -6.3296, -6.3624, -6.3677, -6.5207, -6.6383, -6.6485, -6.7322, -6.8287, -6.8461, -6.9202, -6.9506, -6.9608, -6.9739, -6.9769, -6.9934, -7.0044, -7.0223, -7.0766, -7.1066, -7.1092, -7.2458, -7.3275, -7.3409, -6.2865, -7.1909, -6.6341, -6.2826, -6.5774, -6.0785, -6.3858, -5.1978, -4.4484, -6.2958, -4.8435, -5.8019, -6.3887, -6.1477, -5.3945, -5.6805, -5.9675, -5.696, -6.1481, -5.5911, -5.2063, -5.0349, -6.1537, -5.9299, -5.4276, -5.3794, -5.538, -5.2848, -5.6878, -5.006, -5.9815, -5.3922, -5.9009, -5.2415, -5.7774, -5.6336, -5.8698, -5.8767, -5.9276, -5.9319, -6.435, -6.4352, -6.4732, -6.518, -6.5433, -6.572, -6.6054, -6.6148, -6.6398, -6.6954, -6.7411, -6.7711, -6.8839, -6.9, -6.9625, -7.0092, -7.0821, -7.0988, -7.1329, -7.1817, -7.1846, -7.2173, -7.2312, -7.3618, -7.3828, -7.3909, -7.4098, -7.4131, -7.4405, -7.4598, -5.8565, -5.8557, -6.421, -5.6247, -6.6612, -6.1719, -5.6988, -6.1421, -6.5822, -5.6185, -6.6638, -6.4252, -5.9016, -6.4823, -6.5651, -4.9184, -6.3616, -4.9809, -5.4872, -5.3634, -5.9776, -5.489, -5.2074, -5.8993, -5.5146, -5.4553, -5.0644, -5.7908, -6.0436, -5.9468, -6.0865, -5.1437, -5.6981, -5.8274, -5.6528, -4.8769, -4.7394, -5.6365, -5.4567, -5.4881, -5.9034, -5.8786, -5.8211, -5.8223, -4.9932, -5.1404, -5.9244, -6.389, -6.6523, -6.8505, -6.9032, -7.1074, -7.2216, -7.3088, -7.3133, -7.3136, -7.3369, -7.337, -7.3962, -7.4222, -7.4336, -7.4655, -7.4757, -7.4794, -7.5208, -7.5415, -7.5456, -7.5502, -7.5766, -7.5801, -7.6309, -7.6387, -7.6419, -7.644, -4.3122, -6.8169, -6.2326, -3.0694, -6.6671, -6.6307, -5.0821, -6.3491, -3.9638, -7.0758, -4.2232, -6.2468, -5.8208, -6.6845, -6.7287, -5.2499, -4.7085, -4.334, -5.1676, -4.6468, -7.1653, -6.1588, -6.3922, -5.6868, -6.4361, -6.0443, -6.2895, -5.3853, -6.0945, -6.2736, -5.4922, -5.0909, -5.9388, -6.262, -5.6521, -5.4648, -5.5595, -5.8056, -5.7679, -5.9498, -5.7297, -5.8414, -5.8383, -6.0155], \"Total\": [17349.0, 117857.0, 26834.0, 13078.0, 8635.0, 34678.0, 27252.0, 27624.0, 10277.0, 7393.0, 17379.0, 9411.0, 18475.0, 21622.0, 33686.0, 58761.0, 19746.0, 20840.0, 43233.0, 22407.0, 39808.0, 4282.0, 21018.0, 18899.0, 16183.0, 7686.0, 9334.0, 6673.0, 10626.0, 27348.0, 7621.595576631164, 4913.928614437548, 4296.7771602172725, 2725.6321041982897, 1363.6096503283534, 1178.7666432680164, 832.5541253250096, 822.377362311664, 565.9781393919047, 515.3393268721551, 470.7184988614513, 375.5769322322936, 365.8828304020512, 347.1012107157466, 334.86331533006046, 306.8192390703445, 265.05414056205075, 261.29920908164144, 252.88723198742522, 252.52720516794028, 210.06246528946969, 205.3114592447982, 197.6605223406543, 183.33575686229497, 176.5359954327039, 173.43828040103938, 172.7511500088236, 165.46757754616453, 160.013593963681, 156.26966625441187, 3705.6364294886, 747.1623148424561, 26834.1441979319, 4504.1918556083265, 4757.445765041518, 762.422508665708, 438.2407428452137, 456.56163877004167, 2057.0917823782406, 4083.33733641939, 1340.5451389431787, 592.2848645627162, 3814.8821955148314, 531.2932957877953, 3059.7794233745676, 1111.1824701970122, 1752.4516653020523, 1500.2701211311394, 2140.338360856269, 5446.663451177102, 17881.521749574138, 6755.070415303355, 8532.405273448549, 13227.52643620519, 4803.915971085417, 17423.103827608604, 58761.67608842563, 28406.08156437151, 22407.38421431021, 36793.84981129574, 19746.689033999133, 39808.3427455451, 12612.343448097106, 34678.78151901958, 15342.125803605302, 18048.98617987982, 13163.950871811083, 10699.757222405871, 43233.30184733197, 23673.236613805544, 19155.10491305018, 27348.325963200215, 23138.91172572921, 17154.493308085934, 27624.560865220832, 77496.90587400431, 21018.87463107935, 33686.347633602796, 35157.345786634054, 117857.54269972915, 15164.831871502069, 19123.876060402683, 28434.870867715133, 566.99229631957, 500.88436419736206, 368.75918716452753, 355.9838851264747, 339.1261511212019, 313.391108271125, 280.2674586927874, 274.02526466883035, 274.00604796413796, 271.51086489374603, 214.48199334231663, 209.0873089653981, 208.15263162218287, 207.7213987610754, 201.66468809990928, 193.13390355771756, 179.4968968602144, 172.5198826598922, 167.41511973609704, 163.22928347151654, 161.47966728897399, 154.62172318556713, 152.26320169969412, 147.3854126895036, 146.95261874222516, 146.78242241361602, 144.02508565022873, 143.91276609296165, 142.9152789459179, 142.3013318412869, 177.3375556199611, 323.497255581833, 264.70426628077655, 187.45180876598195, 681.5419481726267, 508.63017279166405, 5244.373104762087, 723.107901638803, 1413.7420053805708, 1852.0849774018725, 1398.8395602257438, 325.7282657631599, 1657.309498072247, 877.4457187763954, 21622.473313540082, 1335.9474738951021, 6186.220112959671, 3579.1945058660294, 8223.715630374545, 11012.879014456732, 20531.264618234938, 2803.4460450135693, 117857.54269972915, 6529.239841991746, 9490.156535884396, 1736.6652492092396, 19123.876060402683, 875.6904510569046, 6610.803740098601, 2042.486108033777, 1891.111576475445, 2126.88289009205, 15244.924339627149, 2796.7891122040046, 9807.796462013697, 1970.0134480701533, 7525.0500111943265, 11103.797294323771, 28434.870867715133, 4175.524164497692, 8593.19216688066, 16239.876454885814, 5723.245551943377, 4963.97631187271, 19294.974969483334, 34500.4940140037, 10560.624067998067, 12024.638269447742, 20084.70946946591, 7231.522830461274, 77496.90587400431, 36793.84981129574, 20840.37888132487, 23391.193983593166, 9108.989625824599, 15571.044130587208, 13028.39646992401, 12612.343448097106, 58761.67608842563, 34678.78151901958, 39808.3427455451, 27118.522434686947, 28406.08156437151, 23673.236613805544, 15164.831871502069, 28712.36449131325, 602.3795815147299, 334.3968421180613, 316.71674248370766, 301.59200380101896, 235.27643801828725, 225.28569775370866, 203.2361696763451, 192.209633543399, 190.58681218889913, 188.60867786505693, 179.48406105198, 174.35352374442414, 167.3476280098002, 166.59627429564986, 149.82266255133055, 147.13502741296392, 136.18907895439622, 132.43589262727397, 130.44271248782914, 128.8498697470897, 128.40025721619642, 126.1830693892579, 120.43193353698675, 119.78024892795402, 119.46079978985703, 118.92734886738327, 115.63233895916792, 115.14611766614368, 111.0272777545916, 110.30172013102145, 544.8016112339504, 373.6812815654066, 485.7342195583008, 3984.7834771346033, 3725.483490060689, 5840.075486312539, 268.7847585124689, 1003.799560231993, 4397.771390627339, 3462.468370842604, 27252.474862277653, 4434.4314772712905, 2007.500023830538, 4726.291307637531, 18475.431656545636, 9657.731620448167, 4439.773371188661, 1249.1239033124252, 3484.329992132606, 33686.347633602796, 5936.150225520468, 4811.862434269126, 34500.4940140037, 34678.78151901958, 28712.36449131325, 5085.81788743017, 43233.30184733197, 21018.87463107935, 18899.893620223887, 19746.689033999133, 9851.946868604242, 19155.10491305018, 22407.38421431021, 77496.90587400431, 7495.345503343542, 117857.54269972915, 58761.67608842563, 35157.345786634054, 25773.11701696424, 27118.522434686947, 19878.39248306302, 39808.3427455451, 28434.870867715133, 21622.473313540082, 15841.37688751393, 12379.414834429735, 19242.147800139057, 28406.08156437151, 19294.974969483334, 23138.91172572921, 23391.193983593166, 27624.560865220832, 27348.325963200215, 2886.3714747227336, 707.8673089300893, 614.13742737932, 550.9957936439547, 517.8283835468922, 414.6484793901417, 408.7659540713261, 390.1734617965906, 352.551511431427, 352.2246878277333, 295.6470046299199, 290.68554866530843, 283.9589417204773, 282.68038901677096, 277.05252050219434, 273.33904391581825, 271.00198358254295, 270.48579912366256, 267.7223587917573, 265.8965392897541, 265.1569254661423, 251.6999102690392, 244.76904024531598, 241.5646756964453, 231.11565706402178, 220.95081171434174, 216.98358181919164, 211.30906820192124, 211.13656679363845, 210.67651358137945, 1683.6952908548355, 907.9220890945401, 561.1103182924361, 6027.760572262788, 1077.8444877922097, 388.0697023603029, 605.4088264530252, 1610.8120502718243, 1947.18394657598, 1205.770553072293, 432.55040843670406, 337.012392998121, 11137.503908161018, 2663.4563955944764, 3870.508829319224, 2298.3727708467486, 884.9593839248101, 678.4607272530394, 6531.667095561017, 1246.361618630969, 1058.6671762620795, 10812.596743899872, 4299.058304047318, 1362.6300497320265, 1710.813052582786, 1386.54470622549, 1242.5846524013873, 27624.560865220832, 2251.279873473675, 2803.4978880158546, 17379.87563347582, 2973.3843630517235, 2307.2903964234065, 2605.937294552677, 5681.431407645083, 117857.54269972915, 2734.347015374015, 39808.3427455451, 19878.39248306302, 4176.815871115959, 14064.13344269575, 3769.944547598864, 77496.90587400431, 35157.345786634054, 33686.347633602796, 6294.234655038909, 16183.930185985006, 6032.251788049633, 13097.531143550184, 23391.193983593166, 10543.576255353084, 36793.84981129574, 27118.522434686947, 58761.67608842563, 23138.91172572921, 34500.4940140037, 28406.08156437151, 23673.236613805544, 1003.0352053040658, 481.4321921891235, 455.41353291799834, 441.762428575736, 441.16773709090677, 377.80666768658176, 368.1574329651624, 360.1836085628345, 296.3572037891567, 296.15556350042647, 286.1088305357672, 269.1321437091146, 265.24466554686364, 250.4127480831421, 247.73845632043904, 241.9945027445613, 239.0821809887457, 230.26783064399947, 222.57497668838445, 221.77413768502973, 218.59972162608048, 217.28425276138438, 213.9313021370783, 213.483989392558, 212.2213538541699, 211.1254319495089, 209.90822807938952, 208.90641746033828, 202.22769122759914, 199.49780452647605, 342.92220651512304, 2181.8509410534148, 540.8892867860394, 434.11417325793457, 270.11613547404045, 286.27443074341875, 312.79559818300714, 1061.4013162393978, 1740.2795288157306, 3142.056565549981, 2806.702395078661, 20840.37888132487, 610.546842921363, 1214.5793267616173, 799.9836865899493, 117857.54269972915, 1482.4496064718855, 2752.416139836734, 4353.768505517017, 2216.9158647112617, 10341.329525868734, 12024.638269447742, 5074.669178683026, 2607.849053883985, 5449.168328015097, 11239.090338404176, 8221.856234235895, 5936.150225520468, 4322.329904766986, 2680.5312452527523, 6598.529300629261, 6683.708246596395, 18899.893620223887, 3735.9999303660456, 27252.474862277653, 77496.90587400431, 3977.721456998976, 12099.286455866963, 4289.781806297497, 27118.522434686947, 28712.36449131325, 9334.40774136952, 6821.705352228483, 34500.4940140037, 15244.924339627149, 23138.91172572921, 28434.870867715133, 35157.345786634054, 11103.797294323771, 17423.103827608604, 43233.30184733197, 891.0375782927806, 843.782914169851, 748.8201571260739, 705.1643323254276, 617.226171336509, 533.0391817160843, 507.04894723736703, 472.3004128193896, 464.2370248047605, 454.44600416849613, 425.253761244476, 422.48020391934, 403.6698964269778, 398.62825926340787, 339.06261666236617, 325.61792776307686, 323.1950874496343, 304.35811585764225, 298.5792891959693, 285.639617090793, 281.27879559798816, 271.7432101490192, 263.99899973818816, 247.69297640828532, 232.51329319517805, 228.66083948506434, 227.60929051935793, 226.15519011780626, 224.30244395658517, 215.71207209608477, 2133.0870279511814, 1675.8977242936353, 3298.4983840274367, 3367.021885244134, 704.510799059373, 1030.0429634732234, 1935.3420500627449, 713.4933553759884, 1732.9589039421182, 489.344006819892, 1278.6372523527593, 1127.5240607226883, 1357.8938543817712, 3796.394103440397, 894.2633962897577, 1155.0618958445777, 1042.143756708035, 3691.308592748208, 9176.529512647807, 876.7532539247836, 14134.198563278072, 9114.758466631774, 2891.041209781725, 2446.2825666554018, 1426.6802887518684, 27348.325963200215, 3647.660035795013, 16239.876454885814, 16183.930185985006, 7285.268785165192, 10341.329525868734, 3792.708699538367, 2043.5238079085982, 77496.90587400431, 20084.70946946591, 14530.591366743516, 8237.535165081683, 10661.337391272215, 6870.370947407824, 4240.489645800643, 3375.4201561545046, 12413.478305459592, 4155.782063081358, 35157.345786634054, 10191.180531867005, 15571.044130587208, 9904.12267060907, 19294.974969483334, 27118.522434686947, 23391.193983593166, 3107.5126181230353, 863.0921611785147, 403.52889831160275, 344.7567959026287, 339.1602396481435, 336.0393300864715, 329.4461260940524, 314.01455665251626, 298.0993015653401, 291.7656705498345, 282.3256858624151, 263.4915670090671, 240.1238285225366, 230.235143018288, 228.3414661875592, 214.81644971350283, 211.51037282872284, 209.77997467373777, 209.53664136615842, 207.74593406338346, 206.72689351644112, 204.82194490147472, 203.01748968901256, 202.60711265515738, 192.8701593355945, 185.29009221323494, 183.2801613679558, 179.51924811274185, 176.17145759610352, 174.7841573483733, 275.3297893309718, 704.0498235481259, 1222.7341405005784, 302.9875911628041, 502.34803100995157, 4860.235341956521, 1776.0086117280184, 368.9174073463294, 5405.185603836644, 3639.36604857562, 3904.943077637378, 584.1955095127277, 497.5859568160785, 606.5760042196669, 6245.8194718123, 7686.7790641728725, 744.33864041041, 608.5865153654632, 10626.41070143282, 703.7461508351784, 8052.936747602554, 1047.5701107439245, 2828.459426960479, 2160.3048080360622, 1555.7881400247225, 2810.097618920023, 16183.930185985006, 1129.100355755416, 1051.3587825104453, 17379.87563347582, 3460.1843626261934, 4324.471390392545, 8230.657129670362, 4037.154360352593, 18475.431656545636, 3777.5019665015598, 5364.429272539425, 1826.810736375297, 5335.234778286066, 6800.491462737812, 4421.456617776811, 2566.334844978228, 3082.969252036716, 9334.40774136952, 7847.850309448226, 11670.80761768451, 8221.856234235895, 34500.4940140037, 20531.264618234938, 2107.0677858995223, 1849.9050795087203, 1697.1062813800145, 1285.8725573219567, 1031.8448767709308, 971.8857107980007, 948.4268050118131, 937.7230420259555, 907.4374282594581, 902.6564573476782, 774.7325108361349, 688.9068790456346, 681.9152743738172, 627.2391809553629, 569.6049007017491, 559.8095690399675, 519.8649868588321, 504.3347023651461, 499.2305541996134, 492.7554068190429, 491.2650743604045, 483.25633831835194, 477.94476254986944, 469.4954231342881, 444.71512545359235, 431.599776076596, 430.4832155585676, 375.629571895792, 346.2475490822978, 341.6383761542006, 981.620821947412, 396.928650568545, 711.8497184072409, 1054.1923173659268, 783.8123115326821, 1390.6401591286378, 987.3118232775396, 3976.3732598906718, 10277.511833651502, 1120.6288560930273, 6673.260262255657, 2117.6239906400706, 1023.0870692951603, 1393.27806351807, 3898.8845933499815, 2665.327233992045, 1845.3799183097801, 2682.719673168844, 1454.0651231006218, 3302.902525183711, 6118.058430797544, 9232.980125778371, 1505.8886289308543, 2246.1820621419356, 5620.0814170289495, 8635.890332776527, 10341.329525868734, 25773.11701696424, 7797.608798323735, 77496.90587400431, 2966.2536109370444, 27118.522434686947, 4225.304901009787, 58761.67608842563, 12553.041599524167, 43233.30184733197, 14134.198563278072, 15841.37688751393, 11670.80761768451, 10507.707584132762, 787.3586172329594, 787.2726692086525, 757.9371307429153, 724.7366361492298, 706.6512246007275, 686.6726154716636, 664.1493501701405, 657.9567324786788, 641.7140567388998, 607.0792760177294, 580.0107982587342, 562.8853256674153, 502.94750818964195, 494.9313985150161, 464.99132541442384, 443.79976077558115, 412.67337952638377, 405.84162704544207, 392.2755878665796, 373.6518724296373, 372.5501045026979, 360.6170430181043, 355.64955234738056, 312.195232343423, 305.7268291449455, 303.2901245428982, 297.61789193151463, 296.6295651479492, 288.6567608760326, 283.1536287499535, 1517.81286141979, 1596.1981488230304, 873.4217675369345, 2173.6091303866965, 685.0347657924721, 1240.3412522243955, 2221.50941972657, 1354.1414625460845, 787.7028742186892, 2625.8164460485855, 716.8594903356731, 986.2411062800916, 1998.4641893191033, 933.977238436034, 834.2958071278139, 9334.40774136952, 1121.8122266104917, 9411.825359381219, 5397.292485487861, 6683.708246596395, 2357.652273913535, 6489.771037852888, 11239.090338404176, 3073.462728962251, 6800.491462737812, 8221.856234235895, 18899.893620223887, 4330.408550681651, 2506.6918069585117, 3234.8166628863955, 2312.955266838909, 27252.474862277653, 6373.647852246694, 4647.556074624908, 7847.850309448226, 77496.90587400431, 117857.54269972915, 8918.802421346212, 27118.522434686947, 43233.30184733197, 5157.640552766171, 6821.705352228483, 13078.927079643803, 18475.431656545636, 2066.1739307517123, 1783.3858466871768, 814.7386480842148, 512.3242223528378, 393.932854144209, 323.2521407556598, 306.7135166993454, 250.225914338407, 223.31731207557766, 204.7339935075161, 203.83196680188772, 203.76617037959144, 199.10000773961292, 199.0816713961814, 187.6734348992095, 182.89271813860967, 180.8249629863151, 175.181041037121, 173.41474336838266, 172.7765875701506, 165.80481055016654, 162.42658529730392, 161.76235985437287, 161.02036087091605, 156.8483001106217, 156.3014468158636, 148.60085725192718, 147.4635626399438, 146.99606610891047, 146.6901536794096, 4282.877612431583, 340.17647121096104, 620.6244229538678, 17349.45554643886, 408.25830169959534, 433.7868775056501, 2385.436350791724, 602.2300969011845, 8635.890332776527, 275.67001179144467, 7393.285646273203, 741.4049620730028, 1313.9130354730673, 454.2947612290535, 431.97594411639284, 2839.6794201176044, 7177.587894230846, 13078.927079643803, 4315.489718115122, 9411.825359381219, 252.7822353585002, 1290.5429821005011, 880.3108657109022, 3076.223794891414, 878.9936107468902, 1893.7088134406881, 1176.3214348227284, 12553.041599524167, 2138.2765033721757, 1353.91227182259, 11792.604960729812, 58761.67608842563, 4093.089781522602, 1487.8803763852834, 13097.531143550184, 25773.11701696424, 21622.473313540082, 9127.440403896211, 18899.893620223887, 8052.936747602554, 39808.3427455451, 35157.345786634054, 77496.90587400431, 27252.474862277653], \"Term\": [\"show\", \"film\", \"bad\", \"funny\", \"series\", \"really\", \"great\", \"people\", \"horror\", \"tv\", \"life\", \"comedy\", \"love\", \"characters\", \"see\", \"like\", \"watch\", \"films\", \"good\", \"movies\", \"would\", \"episode\", \"think\", \"best\", \"man\", \"woman\", \"role\", \"effects\", \"young\", \"get\", \"worst\", \"stupid\", \"waste\", \"crap\", \"na\", \"garbage\", \"gon\", \"sucks\", \"blah\", \"seagal\", \"wan\", \"horrid\", \"1/10\", \"mst3k\", \"stinks\", \"stinker\", \"troma\", \"2/10\", \"troll\", \"ninjas\", \"dumber\", \"nope\", \"bava\", \"stiller\", \"dont\", \"sci\", \"fi\", \"buffy\", \"rodman\", \"aubrey\", \"horrible\", \"crappy\", \"bad\", \"terrible\", \"awful\", \"rubbish\", \"wasting\", \"suck\", \"lame\", \"worse\", \"yeah\", \"idiot\", \"oh\", \"atrocious\", \"ok\", \"trash\", \"dumb\", \"pathetic\", \"rent\", \"boring\", \"acting\", \"money\", \"minutes\", \"thing\", \"mean\", \"ever\", \"like\", \"could\", \"movies\", \"even\", \"watch\", \"would\", \"nothing\", \"really\", \"say\", \"know\", \"watching\", \"ca\", \"good\", \"make\", \"seen\", \"get\", \"made\", \"better\", \"people\", \"one\", \"think\", \"see\", \"time\", \"film\", \"something\", \"plot\", \"much\", \"erotic\", \"lugosi\", \"argento\", \"giallo\", \"ringu\", \"sadako\", \"bela\", \"cronenberg\", \"emmanuelle\", \"fellini\", \"leatherface\", \"delpy\", \"unsatisfying\", \"dario\", \"sorority\", \"inexplicable\", \"trivial\", \"solondz\", \"unsympathetic\", \"d'amato\", \"unappealing\", \"transitions\", \"extremes\", \"poe\", \"perverse\", \"mean-spirited\", \"nuns\", \"predictability\", \"pornography\", \"underdeveloped\", \"hackneyed\", \"explicit\", \"troy\", \"abrupt\", \"gratuitous\", \"karloff\", \"sex\", \"exploitation\", \"bizarre\", \"development\", \"tension\", \"devoid\", \"fails\", \"contrived\", \"characters\", \"disturbing\", \"audience\", \"viewer\", \"rather\", \"seems\", \"character\", \"lack\", \"film\", \"seem\", \"interesting\", \"nudity\", \"plot\", \"pacing\", \"main\", \"sexual\", \"twist\", \"premise\", \"scenes\", \"female\", \"however\", \"male\", \"sense\", \"quite\", \"much\", \"sort\", \"point\", \"scene\", \"completely\", \"perhaps\", \"little\", \"story\", \"enough\", \"director\", \"two\", \"ending\", \"one\", \"even\", \"films\", \"way\", \"bit\", \"end\", \"though\", \"nothing\", \"like\", \"really\", \"would\", \"also\", \"could\", \"make\", \"something\", \"well\", \"bambi\", \"porky\", \"ants\", \"emma\", \"kidman\", \"deniro\", \"mins\", \"woodstock\", \"poirot\", \"olsen\", \"dustin\", \"cinderella\", \"rochester\", \"heartwarming\", \"grodin\", \"leung\", \"tucci\", \"hutton\", \"mcconaughey\", \"dickens\", \"claus\", \"crusoe\", \"winkler\", \"hustle\", \"austen\", \"liza\", \"brightest\", \"indigo\", \"aileen\", \"scrooge\", \"10/10\", \"angus\", \"quaid\", \"loved\", \"enjoyed\", \"book\", \"pryor\", \"christmas\", \"liked\", \"amazing\", \"great\", \"wonderful\", \"fantastic\", \"recommend\", \"love\", \"saw\", \"perfect\", \"books\", \"favorite\", \"see\", \"excellent\", \"definitely\", \"story\", \"really\", \"well\", \"enjoy\", \"good\", \"think\", \"best\", \"watch\", \"thought\", \"seen\", \"movies\", \"one\", \"different\", \"film\", \"like\", \"time\", \"first\", \"also\", \"many\", \"would\", \"much\", \"characters\", \"still\", \"lot\", \"never\", \"could\", \"little\", \"made\", \"way\", \"people\", \"get\", \"documentary\", \"freedom\", \"propaganda\", \"countries\", \"cultural\", \"documentaries\", \"candyman\", \"iraq\", \"nazis\", \"racism\", \"che\", \"historically\", \"jews\", \"hitler\", \"soviet\", \"cities\", \"germans\", \"beliefs\", \"communist\", \"troops\", \"leaders\", \"global\", \"lime\", \"cultures\", \"9/11\", \"muslim\", \"economic\", \"educated\", \"it`s\", \"natives\", \"political\", \"facts\", \"religion\", \"war\", \"americans\", \"nation\", \"politics\", \"culture\", \"society\", \"soldiers\", \"germany\", \"spiritual\", \"world\", \"country\", \"history\", \"america\", \"media\", \"united\", \"american\", \"military\", \"states\", \"us\", \"human\", \"government\", \"social\", \"german\", \"historical\", \"people\", \"message\", \"french\", \"life\", \"power\", \"truth\", \"view\", \"men\", \"film\", \"reality\", \"would\", \"many\", \"lives\", \"real\", \"white\", \"one\", \"time\", \"see\", \"true\", \"man\", \"black\", \"years\", \"way\", \"fact\", \"even\", \"also\", \"like\", \"made\", \"story\", \"could\", \"make\", \"noir\", \"scorsese\", \"welles\", \"eastwood\", \"errol\", \"mitchum\", \"bronson\", \"sherlock\", \"asoka\", \"spaghetti\", \"kurosawa\", \"cinematographer\", \"orson\", \"griffith\", \"widescreen\", \"panther\", \"keane\", \"arizona\", \"django\", \"hackman\", \"nero\", \"heston\", \"femme\", \"sellers\", \"technicolor\", \"crisp\", \"apes\", \"outlaw\", \"sergio\", \"rex\", \"clint\", \"western\", \"westerns\", \"hopkins\", \"cassavetes\", \"edwards\", \"lana\", \"hitchcock\", \"italian\", \"score\", \"cinematography\", \"films\", \"fonda\", \"photography\", \"ford\", \"film\", \"visual\", \"british\", \"style\", \"peter\", \"action\", \"director\", \"production\", \"sets\", \"performances\", \"cast\", \"performance\", \"excellent\", \"picture\", \"novel\", \"set\", \"john\", \"best\", \"fine\", \"great\", \"one\", \"cinema\", \"work\", \"early\", \"also\", \"well\", \"role\", \"screen\", \"story\", \"scenes\", \"made\", \"much\", \"time\", \"quite\", \"ever\", \"good\", \"cops\", \"sheriff\", \"sandler\", \"jackie\", \"fbi\", \"hong\", \"woo\", \"chaney\", \"chan\", \"connery\", \"nikki\", \"willis\", \"hardy\", \"chases\", \"pammy\", \"snoopy\", \"vegas\", \"stan\", \"lance\", \"andre\", \"tarzan\", \"hostage\", \"costner\", \"undercover\", \"gorilla\", \"hogan\", \"roswell\", \"stallone\", \"ash\", \"joshua\", \"cop\", \"gang\", \"police\", \"car\", \"bank\", \"plane\", \"gun\", \"driver\", \"prison\", \"truck\", \"chase\", \"agent\", \"charlie\", \"town\", \"cars\", \"boss\", \"bond\", \"killed\", \"gets\", \"brown\", \"back\", \"guy\", \"jack\", \"crime\", \"train\", \"get\", \"kill\", \"scene\", \"man\", \"goes\", \"action\", \"city\", \"dog\", \"one\", \"two\", \"go\", \"away\", \"around\", \"takes\", \"head\", \"run\", \"going\", \"killer\", \"time\", \"take\", \"end\", \"big\", \"little\", \"also\", \"way\", \"husband\", \"holmes\", \"watson\", \"courtroom\", \"berry\", \"streep\", \"leigh\", \"melanie\", \"shields\", \"arkin\", \"sharpe\", \"stuart\", \"cabaret\", \"meryl\", \"marries\", \"rathbone\", \"maris\", \"laughton\", \"peck\", \"reilly\", \"thurman\", \"leopard\", \"baron\", \"uma\", \"sarandon\", \"husbands\", \"lewton\", \"farrah\", \"kay\", \"val\", \"rebecca\", \"susan\", \"marriage\", \"columbo\", \"gregory\", \"mother\", \"married\", \"claire\", \"father\", \"daughter\", \"son\", \"emily\", \"mickey\", \"anne\", \"wife\", \"woman\", \"marry\", \"wealthy\", \"young\", \"anna\", \"family\", \"affair\", \"relationship\", \"sister\", \"mary\", \"finds\", \"man\", \"lover\", \"mrs\", \"life\", \"child\", \"friend\", \"girl\", \"becomes\", \"love\", \"wants\", \"home\", \"meets\", \"women\", \"plays\", \"boy\", \"parents\", \"david\", \"role\", \"played\", \"old\", \"performance\", \"story\", \"character\", \"monster\", \"sci-fi\", \"alien\", \"vampire\", \"aliens\", \"creature\", \"dracula\", \"scientist\", \"monsters\", \"anime\", \"shark\", \"bugs\", \"robot\", \"dragon\", \"vampires\", \"batman\", \"fu\", \"freddy\", \"demon\", \"predator\", \"chainsaw\", \"graphics\", \"kung\", \"werewolf\", \"franchise\", \"superman\", \"13th\", \"robots\", \"mummy\", \"dinosaurs\", \"martial\", \"virus\", \"creatures\", \"zombies\", \"beast\", \"zombie\", \"arts\", \"game\", \"horror\", \"planet\", \"effects\", \"space\", \"cgi\", \"giant\", \"evil\", \"gore\", \"animation\", \"sequel\", \"science\", \"blood\", \"special\", \"original\", \"cartoon\", \"earth\", \"dead\", \"series\", \"action\", \"first\", \"fun\", \"one\", \"scary\", \"also\", \"fans\", \"like\", \"new\", \"good\", \"back\", \"still\", \"old\", \"pretty\", \"cagney\", \"powell\", \"flynn\", \"khan\", \"chaplin\", \"cary\", \"lloyd\", \"dunne\", \"bogart\", \"keaton\", \"bollywood\", \"bette\", \"mgm\", \"wilder\", \"irene\", \"broadway\", \"rogers\", \"neil\", \"barrymore\", \"beatles\", \"costello\", \"stanwyck\", \"norma\", \"sings\", \"humphrey\", \"loy\", \"ginger\", \"hughes\", \"carrey\", \"screwball\", \"grant\", \"dance\", \"singer\", \"musical\", \"kelly\", \"singing\", \"songs\", \"robin\", \"gene\", \"song\", \"warner\", \"eddie\", \"stage\", \"douglas\", \"murphy\", \"role\", \"dancing\", \"comedy\", \"star\", \"john\", \"george\", \"play\", \"cast\", \"career\", \"plays\", \"performance\", \"best\", \"stars\", \"supporting\", \"roles\", \"de\", \"great\", \"actor\", \"playing\", \"played\", \"one\", \"film\", \"music\", \"also\", \"good\", \"hollywood\", \"screen\", \"funny\", \"love\", \"episodes\", \"season\", \"woody\", \"seasons\", \"lenny\", \"sitcom\", \"sopranos\", \"sketch\", \"richards\", \"hawn\", \"benny\", \"abc\", \"skits\", \"programme\", \"goldie\", \"nbc\", \"fort\", \"o'neill\", \"sh*t\", \"mulder\", \"contestants\", \"goodfellas\", \"frankie\", \"canceled\", \"snl\", \"sitcoms\", \"scully\", \"christians\", \"mexicans\", \"reruns\", \"episode\", \"buscemi\", \"network\", \"show\", \"coach\", \"aired\", \"television\", \"program\", \"series\", \"sketches\", \"tv\", \"guest\", \"allen\", \"football\", \"baseball\", \"jokes\", \"shows\", \"funny\", \"humor\", \"comedy\", \"simpsons\", \"steve\", \"pilot\", \"hilarious\", \"fox\", \"writers\", \"tony\", \"new\", \"team\", \"news\", \"every\", \"like\", \"laugh\", \"humour\", \"years\", \"first\", \"characters\", \"always\", \"best\", \"family\", \"would\", \"time\", \"one\", \"great\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6389, 1.6388, 1.6388, 1.6387, 1.6383, 1.6382, 1.6379, 1.6379, 1.6374, 1.6373, 1.6371, 1.6366, 1.6365, 1.6364, 1.6363, 1.6361, 1.6356, 1.6356, 1.6354, 1.6354, 1.6347, 1.6346, 1.6344, 1.6341, 1.6339, 1.6338, 1.6338, 1.6336, 1.6334, 1.6332, 1.6179, 1.6244, 1.5629, 1.5932, 1.5858, 1.6114, 1.618, 1.6151, 1.5608, 1.5128, 1.5549, 1.5844, 1.4709, 1.5901, 1.462, 1.5311, 1.4838, 1.4953, 1.4588, 1.3637, 1.129, 1.2618, 1.2087, 1.1395, 1.2802, 1.0739, 0.8563, 0.9742, 1.0061, 0.9007, 1.0109, 0.8022, 1.0331, 0.7949, 0.9804, 0.9348, 1.0154, 1.0701, 0.6721, 0.8435, 0.9057, 0.7627, 0.811, 0.9163, 0.7323, 0.3143, 0.8304, 0.6273, 0.5085, -0.2738, 0.826, 0.6494, 0.2529, 1.8211, 1.8209, 1.8203, 1.8202, 1.8201, 1.8198, 1.8195, 1.8194, 1.8194, 1.8194, 1.8185, 1.8184, 1.8184, 1.8184, 1.8182, 1.818, 1.8177, 1.8175, 1.8173, 1.8172, 1.8171, 1.8169, 1.8168, 1.8166, 1.8166, 1.8166, 1.8165, 1.8165, 1.8164, 1.8164, 1.8154, 1.8054, 1.8068, 1.8097, 1.765, 1.7597, 1.5989, 1.6988, 1.608, 1.5802, 1.58, 1.7384, 1.534, 1.6083, 1.1893, 1.5369, 1.3078, 1.3873, 1.2604, 1.2101, 1.0587, 1.3926, 0.7558, 1.2442, 1.1553, 1.4594, 0.9754, 1.5874, 1.1456, 1.403, 1.4123, 1.3818, 0.8901, 1.3064, 0.9737, 1.3957, 1.0317, 0.9072, 0.6024, 1.1775, 0.9489, 0.7401, 1.0668, 1.1119, 0.6562, 0.4148, 0.8236, 0.7748, 0.4996, 0.9238, -0.0973, 0.2076, 0.4404, 0.3699, 0.8166, 0.5458, 0.6139, 0.6321, -0.3302, -0.1494, -0.2782, 0.013, -0.0292, 0.1126, 0.4837, -0.1236, 1.8502, 1.849, 1.8488, 1.8487, 1.8478, 1.8477, 1.8472, 1.847, 1.8469, 1.8469, 1.8467, 1.8465, 1.8463, 1.8463, 1.8457, 1.8455, 1.8451, 1.8449, 1.8448, 1.8447, 1.8447, 1.8445, 1.8442, 1.8442, 1.8441, 1.8441, 1.8439, 1.8439, 1.8436, 1.8435, 1.8303, 1.8228, 1.8134, 1.736, 1.6675, 1.6248, 1.7959, 1.7122, 1.5576, 1.5774, 1.3716, 1.5264, 1.5942, 1.466, 1.2759, 1.3479, 1.428, 1.5816, 1.4375, 1.0689, 1.3284, 1.3539, 0.9977, 0.9619, 0.9831, 1.291, 0.9023, 1.0181, 1.0087, 0.9549, 1.1038, 0.9454, 0.8732, 0.471, 1.1202, 0.3086, 0.4851, 0.6364, 0.7245, 0.7027, 0.8093, 0.5191, 0.6144, 0.7008, 0.8477, 0.9153, 0.6365, 0.3833, 0.6284, 0.4873, 0.4676, 0.3038, 0.305, 2.3109, 2.31, 2.3098, 2.3096, 2.3095, 2.3091, 2.309, 2.3089, 2.3087, 2.3087, 2.3082, 2.3081, 2.3081, 2.3081, 2.308, 2.3079, 2.3079, 2.3079, 2.3079, 2.3079, 2.3078, 2.3077, 2.3076, 2.3075, 2.3073, 2.3072, 2.3071, 2.307, 2.307, 2.307, 2.2887, 2.2904, 2.2965, 2.2264, 2.2673, 2.2874, 2.2585, 2.1992, 2.1785, 2.1955, 2.2686, 2.2859, 1.8269, 2.0126, 1.9306, 1.9996, 2.1404, 2.1745, 1.8, 2.0592, 2.0828, 1.649, 1.7911, 2.0277, 1.977, 2.0171, 2.0393, 1.1871, 1.8375, 1.7608, 1.1114, 1.6645, 1.7215, 1.6685, 1.3253, -0.1621, 1.6257, 0.2573, 0.6135, 1.3869, 0.6904, 1.4342, -0.5778, -0.1042, -0.1063, 1.0487, 0.3595, 1.0681, 0.4964, 0.0656, 0.6121, -0.4352, -0.2025, -0.8886, -0.0867, -0.479, -0.3065, -0.1453, 2.4889, 2.4879, 2.4878, 2.4878, 2.4878, 2.4874, 2.4874, 2.4873, 2.4868, 2.4868, 2.4867, 2.4865, 2.4864, 2.4862, 2.4862, 2.4861, 2.4861, 2.4859, 2.4858, 2.4858, 2.4857, 2.4857, 2.4856, 2.4856, 2.4856, 2.4856, 2.4855, 2.4855, 2.4854, 2.4853, 2.4621, 2.3523, 2.4318, 2.4377, 2.4699, 2.4575, 2.4425, 2.2938, 2.1021, 1.9223, 1.8836, 1.3616, 2.2313, 2.0455, 2.1561, 0.7371, 1.941, 1.7372, 1.5682, 1.7862, 1.2474, 1.1564, 1.4405, 1.6852, 1.4018, 1.1128, 1.211, 1.3366, 1.4397, 1.6322, 1.2222, 1.2028, 0.6921, 1.452, 0.4626, -0.1022, 1.3886, 0.7689, 1.3451, 0.2697, 0.2171, 0.8838, 1.0641, -0.0626, 0.4855, -0.0192, -0.2638, -0.503, 0.5066, 0.0962, -0.7749, 2.5067, 2.5067, 2.5065, 2.5065, 2.5063, 2.5061, 2.506, 2.5058, 2.5058, 2.5058, 2.5056, 2.5056, 2.5055, 2.5055, 2.5051, 2.505, 2.505, 2.5048, 2.5047, 2.5046, 2.5045, 2.5044, 2.5043, 2.5041, 2.5039, 2.5038, 2.5038, 2.5038, 2.5037, 2.5036, 2.4486, 2.4515, 2.4288, 2.4119, 2.4591, 2.4161, 2.3447, 2.4096, 2.3083, 2.4428, 2.3172, 2.3208, 2.2934, 2.0644, 2.3239, 2.253, 2.2591, 1.8756, 1.6092, 2.275, 1.3271, 1.4416, 1.8548, 1.9048, 2.0986, 0.928, 1.7316, 1.0879, 1.0861, 1.4181, 1.0799, 1.5926, 1.9133, 0.0204, 0.7201, 0.8534, 1.1225, 0.9644, 1.2233, 1.4729, 1.6212, 0.6707, 1.4578, -0.2156, 0.7515, 0.4062, 0.7603, 0.1588, -0.1533, -0.1631, 2.6703, 2.6695, 2.6683, 2.668, 2.6679, 2.6679, 2.6678, 2.6677, 2.6675, 2.6675, 2.6674, 2.6672, 2.6668, 2.6667, 2.6666, 2.6664, 2.6663, 2.6663, 2.6663, 2.6662, 2.6662, 2.6662, 2.6661, 2.6661, 2.6659, 2.6657, 2.6657, 2.6656, 2.6655, 2.6654, 2.6569, 2.6377, 2.6191, 2.6517, 2.6357, 2.5398, 2.5584, 2.6322, 2.4678, 2.4903, 2.4572, 2.5895, 2.6003, 2.5738, 2.362, 2.2278, 2.4678, 2.5004, 1.9854, 2.4711, 1.9693, 2.3911, 2.1655, 2.222, 2.2735, 2.0737, 1.5698, 2.3073, 2.3275, 1.4192, 1.8733, 1.7892, 1.5385, 1.7946, 1.1715, 1.8101, 1.654, 2.0961, 1.6148, 1.4762, 1.6192, 1.9032, 1.7869, 1.112, 1.2112, 0.9214, 1.1251, -0.0802, 0.3243, 2.7881, 2.7881, 2.788, 2.7879, 2.7877, 2.7876, 2.7876, 2.7876, 2.7876, 2.7876, 2.7874, 2.7872, 2.7872, 2.7871, 2.787, 2.7869, 2.7868, 2.7868, 2.7868, 2.7867, 2.7867, 2.7867, 2.7867, 2.7866, 2.7865, 2.7865, 2.7865, 2.7862, 2.786, 2.7859, 2.7849, 2.7859, 2.7586, 2.7174, 2.7191, 2.6446, 2.6798, 2.4746, 2.2745, 2.6432, 2.3113, 2.5007, 2.6413, 2.5735, 2.2976, 2.392, 2.4726, 2.3701, 2.5304, 2.267, 2.0353, 1.7952, 2.4898, 2.3137, 1.8989, 1.5176, 1.1787, 0.5187, 1.3112, -0.3034, 1.9841, 0.3604, 1.7109, -0.2622, 0.7455, -0.3474, 0.5345, 0.4135, 0.6682, 0.7688, 2.8569, 2.8569, 2.8568, 2.8568, 2.8567, 2.8567, 2.8567, 2.8567, 2.8566, 2.8565, 2.8565, 2.8564, 2.8562, 2.8562, 2.8561, 2.856, 2.8558, 2.8558, 2.8557, 2.8556, 2.8556, 2.8555, 2.8555, 2.8551, 2.8551, 2.8551, 2.855, 2.855, 2.8549, 2.8548, 2.7791, 2.7295, 2.7672, 2.6518, 2.7699, 2.6655, 2.5558, 2.6076, 2.7093, 2.469, 2.7219, 2.6415, 2.4589, 2.6388, 2.6689, 1.9007, 2.5763, 1.83, 1.8798, 1.7898, 2.2176, 1.6936, 1.4261, 2.0307, 1.6213, 1.4908, 1.0493, 1.7964, 2.0903, 1.9321, 2.1278, 0.604, 1.5026, 1.6891, 1.3398, -0.1743, -0.456, 1.2281, 0.2959, -0.2019, 1.5089, 1.2541, 0.6607, 0.3141, 3.334, 3.3339, 3.3333, 3.3327, 3.3322, 3.3317, 3.3315, 3.3309, 3.3304, 3.3301, 3.33, 3.33, 3.3299, 3.3299, 3.3297, 3.3295, 3.3295, 3.3293, 3.3293, 3.3292, 3.329, 3.3289, 3.3289, 3.3289, 3.3287, 3.3287, 3.3284, 3.3284, 3.3283, 3.3283, 3.286, 3.3143, 3.2973, 3.1299, 3.2816, 3.2574, 3.1013, 3.2108, 2.9331, 3.2656, 2.8291, 3.1053, 2.9591, 3.1573, 3.1635, 2.7593, 2.3734, 2.1479, 2.423, 2.1641, 3.2628, 2.639, 2.7881, 2.2424, 2.7457, 2.3701, 2.601, 1.1375, 2.1984, 2.4763, 1.0932, -0.1116, 1.7047, 2.3935, 0.8283, 0.3388, 0.4196, 1.036, 0.3458, 1.017, -0.361, -0.3484, -1.1357, -0.2678], \"Freq\": [17349.0, 117857.0, 26834.0, 13078.0, 8635.0, 34678.0, 27252.0, 27624.0, 10277.0, 7393.0, 17379.0, 9411.0, 18475.0, 21622.0, 33686.0, 58761.0, 19746.0, 20840.0, 43233.0, 22407.0, 39808.0, 4282.0, 21018.0, 18899.0, 16183.0, 7686.0, 9334.0, 6673.0, 10626.0, 27348.0, 7620.6984183013965, 4913.031465735275, 4295.880050252994, 2724.734997920676, 1362.7124409431874, 1177.8695162953147, 831.6569765781776, 821.4802795419786, 565.0810310682373, 514.4422233896523, 469.82134283768755, 374.672195373529, 364.98573093039704, 346.2041327964156, 333.9662104231671, 305.9221257008635, 264.1570469331262, 260.40212399815493, 251.9901501034379, 251.6300932075479, 209.16533060236742, 204.41424943358064, 196.7630513662797, 182.43839036865054, 175.63891792097166, 172.54112717981081, 171.85399700752353, 164.57027263782092, 159.11649960424992, 155.37128429512939, 3628.2821317738503, 736.3171232822899, 24866.95295430648, 4302.786403532559, 4511.004529140207, 741.7034442104846, 429.1485578048644, 445.80392766262565, 1902.3806839264362, 3599.297997266695, 1232.403195508546, 560.8196587165672, 3224.6818830997654, 505.9610936758671, 2563.347583957342, 997.5236935930551, 1500.489936144795, 1299.4595449376916, 1787.5013135458812, 4136.081820280465, 10737.658190988817, 4632.599437605458, 5548.759043342519, 8027.279675836125, 3355.497261259793, 9901.722605428513, 26864.27473475753, 14611.246139034063, 11899.720116991435, 17585.519422261295, 10537.301213417042, 17241.119790335884, 6881.064695594635, 14910.409579443534, 7940.897972472048, 8924.917662537797, 7056.017767954856, 6057.57364024388, 16439.64204478392, 10685.03659700319, 9200.254079695693, 11386.142266159104, 10110.21546304094, 8327.442698847677, 11156.77575137505, 20604.928639754853, 9363.515490349586, 12248.246490347115, 11351.368680641539, 17403.19837749677, 6726.18522880064, 7109.158431341657, 7109.972183791278, 566.096246840371, 499.9883084027513, 367.8631541309398, 355.0878929389501, 338.23015000736365, 312.49514025744764, 279.3713675709551, 273.12922758145874, 273.10996230706394, 270.6146892487105, 213.58587998186638, 208.19107773593885, 207.25654261463353, 206.82533521968978, 200.76859823063708, 192.23783409516085, 178.6007445460515, 171.62385694726186, 166.51902944127448, 162.33326031236203, 160.58351850804118, 153.72550633067723, 151.36710282308727, 146.4892241937361, 146.0565472843666, 145.8861894158064, 143.1288830194531, 143.0166794403047, 142.01912402646192, 141.40523879598288, 176.04909453406484, 317.96065653334205, 260.5191499475768, 185.039044409681, 643.3176097568097, 477.5760525623262, 4192.617200641374, 638.8332428190381, 1140.6234197596077, 1453.221015019816, 1097.4283415024277, 299.4007079500493, 1241.7323885006465, 708.1277836814047, 11477.245849253306, 1003.8110554189526, 3696.7908635802646, 2315.8229520796885, 4686.532877884669, 5968.133027247662, 9563.36670652281, 1823.53462822678, 40549.874004487305, 3661.04323968345, 4868.959537191708, 1207.6417283340922, 8195.574332899798, 692.0775764286119, 3358.94121024094, 1342.386073757573, 1254.497088867322, 1368.586324000322, 5999.290893768308, 1668.8518734906404, 4196.341577400031, 1285.4109115191732, 3411.705346513026, 4445.18188091549, 8392.665992858598, 2190.258437909139, 3586.3090289326697, 5500.848932960232, 2687.5502813678595, 2438.4478930066402, 6009.740096973352, 8440.727018778429, 3888.5717035821267, 4216.781764825976, 5348.633332412339, 2943.385680274284, 11361.913225972325, 7316.913639526065, 5230.699442347545, 5471.659061001795, 3330.5729177112466, 4342.9293988050795, 3889.8819178910867, 3834.724500859842, 6824.724605200412, 4826.195791577162, 4870.331583050888, 4439.258024435788, 4457.861515606161, 4281.474097672195, 3974.7854658563756, 4100.1176717238795, 601.4848395270421, 333.5017641652074, 315.8216996072829, 300.6970684347545, 234.38146057015626, 224.39081536840138, 202.34114541701237, 191.31471612986783, 189.6919501773966, 187.71357629918938, 178.58922512071135, 173.4587431840614, 166.4528092852044, 165.7014546187837, 148.9277355751957, 146.23956337039994, 135.2940103882182, 131.54046913307886, 129.54772080508334, 127.95498286937284, 127.5054088170687, 125.28775605899477, 119.53689350625497, 118.88511838810078, 118.56606069747973, 118.0324995837645, 114.73684334936351, 114.25131493883993, 110.13219490063298, 109.40691496181948, 533.267791758496, 363.03718323472333, 467.49721454877647, 3549.57340854863, 3098.8752849391853, 4654.643589229617, 254.21683480327837, 873.1161792886492, 3277.2876584013684, 2632.0455168065037, 16862.392612566386, 3203.1018674031093, 1551.8967756459897, 3213.828359441525, 10388.590700650822, 5835.561758400643, 2906.564909702151, 953.544437745064, 2302.8294506276056, 15399.257444260536, 3517.829433676793, 2925.1662898392847, 14687.2409905884, 14244.901964177941, 12047.037399355853, 2903.1976215589957, 16730.286662713967, 9132.829144389387, 8135.211387204928, 8054.651513270278, 4663.686710371717, 7739.468000869366, 8422.789163205847, 19483.133081256292, 3606.8141491725287, 25189.140452410913, 14983.324082380304, 10428.610353185702, 8349.49551547941, 8595.814487820784, 7009.724664797311, 10501.729499833966, 8251.265304082965, 6840.955164582925, 5804.853583684957, 4853.287618445506, 5708.346808836483, 6542.020503202459, 5678.2066511021085, 5913.3192598658825, 5861.1248327606645, 5876.096082128976, 5824.327203344328, 2885.4747386380227, 706.9705586670956, 613.2407260686142, 550.0990261352061, 516.9315617744461, 413.7517893257123, 407.8690962311987, 389.27674283390667, 351.6548076944792, 351.3279731147755, 294.7503897309587, 289.78877769566, 283.0623035348656, 281.7836921131069, 276.15588658488963, 272.44233050519665, 270.10529532014755, 269.5890251600867, 266.8257070489441, 264.999822513577, 264.260184544034, 250.803207072633, 243.87194099451122, 240.667938664341, 230.2189621489675, 220.0540907108826, 216.08688285340878, 210.41219923153832, 210.2398355977253, 209.77962807323155, 1646.187262345857, 889.2030993622849, 552.9019448356454, 5537.3918308006405, 1031.474132588174, 378.9375815877945, 574.2918590989588, 1440.0265058845368, 1705.2302632127614, 1074.0039061998987, 414.4912020405832, 328.56670486058914, 6861.586422015928, 1975.761074221423, 2645.175337782712, 1682.956436184033, 746.0007136865254, 591.7553752250782, 3917.5432551637487, 968.6590384260907, 842.4346995108345, 5576.23740040902, 2555.5693135576516, 1026.2153144087154, 1224.770554548904, 1033.191005220862, 946.72678931703, 8976.308703490566, 1401.7980839834513, 1616.7641935516438, 5235.456836890196, 1557.3329821725872, 1279.2784343279827, 1370.3523377705385, 2119.751677973389, 9935.967370363112, 1377.6678466433198, 5104.801909732489, 3639.543314655641, 1657.3500339383795, 2780.9804465706147, 1568.4208818433312, 4310.982745007109, 3140.4148200010036, 3002.9440833268595, 1780.7902493275908, 2298.509140484976, 1740.1009657388, 2133.168614735206, 2476.17019169605, 1927.7490212967848, 2360.5103044705475, 2195.66123306386, 2395.6249257279046, 2103.4391213243607, 2118.6210820777305, 2072.6493390846285, 2029.5177125397313, 1002.1392909867598, 480.5362631498706, 454.5176169649938, 440.8665185641423, 440.2717518633092, 376.91076604936217, 367.2615077722734, 359.2876525673494, 295.4612815716164, 295.25969220945854, 285.21294876440516, 268.23620567421636, 264.34873164534065, 249.5167487907742, 246.8425224473471, 241.09864260819228, 238.18613955069105, 229.37173339057796, 221.67914007380296, 220.8781737304225, 217.70382680417148, 216.38837310750372, 213.03534821278754, 212.58811985126084, 211.32542101529378, 210.2292700149491, 209.0121389382457, 208.01046471080298, 201.33178965685295, 198.60170171467558, 333.5488934512068, 1901.5721410840315, 510.431371456913, 412.09063721476434, 264.7894913874378, 277.1773913038247, 298.35261494672915, 872.4516858840047, 1180.9889871816454, 1781.3437646363789, 1530.829240800053, 6744.387158868766, 471.46293305671867, 778.8509079710843, 572.9721002147542, 20425.335249045493, 856.2941687896333, 1296.7217396506262, 1732.3202918694865, 1096.978919513181, 2985.5461576089992, 3169.3163575193194, 1776.9746746387984, 1166.4053439625404, 1835.7241988595383, 2836.043022533485, 2288.777222995513, 1873.559119066727, 1512.3298860295222, 1137.0198994179304, 1857.5596581487328, 1845.2761119610489, 3131.395083189618, 1323.4208366513526, 3589.085141888276, 5802.272271455551, 1322.4721103787924, 2164.6018742036695, 1365.554868832705, 2945.072192817306, 2958.190983688245, 1873.363284991611, 1639.457208056403, 2687.463988584323, 2054.2193401843397, 1882.2879229579303, 1811.1712603118897, 1763.0909375562865, 1528.1199995599432, 1590.8015227813269, 1651.8702130866318, 890.137931000741, 842.8832795868649, 747.920398389086, 704.2645856115497, 616.3265428318172, 532.1394101576091, 506.1492945815031, 471.4006852704733, 463.33736093030143, 453.5463145425665, 424.35370232808265, 421.5804801213958, 402.7701419523996, 397.728596856613, 338.1625717084575, 324.7183494422371, 322.29538998679334, 303.458369255726, 297.67947859776507, 284.7399161343427, 280.3791362628089, 270.8435660873191, 263.0992699354259, 246.79337173941295, 231.61358782781804, 227.76122715856522, 226.7090924487954, 225.25560423124352, 223.40258078178096, 214.81212056654851, 2010.6228687609794, 1584.3143949721057, 3048.1685246136813, 3059.2015518787407, 671.0685897824409, 939.8364663654097, 1644.255434941123, 646.8001751003196, 1419.6522299387566, 458.58668809040466, 1056.8179009156834, 935.2227187095651, 1095.9491810907439, 2436.8446881326827, 744.1128001689151, 895.3375918131708, 812.7155103384835, 1961.7398270609517, 3736.288917508662, 694.7044426638337, 4340.338725497968, 3138.4129278131404, 1504.8301916828445, 1338.56647821128, 947.6473750389221, 5634.2869424619, 1678.4960531378015, 3926.1330850014547, 3905.5195549421323, 2450.3752482674527, 2480.002024958992, 1518.7843290440605, 1127.708504020684, 6442.062758014117, 3361.3298165136302, 2778.5581229011186, 2061.482287498456, 2277.8667784916265, 1901.7877517528352, 1506.52040489793, 1390.8685068497375, 1977.2365442049165, 1454.3098936926094, 2308.176192838532, 1759.9009680862043, 1903.8557606479242, 1725.45655310029, 1842.0866122094428, 1894.875695476184, 1618.5345574765518, 3106.616150703883, 862.1957127705442, 402.63242222068965, 343.8602152382063, 338.26360084188383, 335.14291201657716, 328.54965870031793, 313.1180036547631, 297.202765267701, 290.86906714923833, 281.4292043247103, 262.59489124055114, 239.22722295884654, 229.3387312915065, 227.4450316911151, 213.9198613021579, 210.61385783487398, 208.88347669832154, 208.64009845181948, 206.84921554815563, 205.8304516785431, 203.92544287823327, 202.12092001294917, 201.71066904514169, 191.97370659049494, 184.39363351825455, 182.38372860091656, 178.62271949954223, 175.27503853984675, 173.88768289179905, 271.6034142755605, 681.274413428189, 1161.4554437016252, 297.33242846742456, 485.1544716547667, 4264.4473284918495, 1587.5029547718098, 355.03491072370286, 4413.213307324261, 3039.1081339675457, 3154.6006722624807, 538.7237186683676, 463.8086148931433, 550.6472570812747, 4587.601105445218, 4936.781788202408, 607.7178365178148, 513.3586856786745, 5355.711401037802, 576.4971324618851, 3993.943088489501, 792.2005650503208, 1706.8843391988923, 1379.4721489915803, 1045.9578474954571, 1547.0039176202663, 5382.989717003078, 785.1551092757131, 746.0076813885071, 4972.522012314307, 1559.0629743696134, 1791.3178864475958, 2653.3691177058795, 1681.3405235644138, 4126.24482991471, 1597.8149006222752, 1941.0445487711584, 1028.4666589828112, 1856.3416222802384, 2059.950929499023, 1545.176447809119, 1191.4163552120006, 1274.1384182111997, 1964.3230442727229, 1823.6930747569063, 2029.7347231628162, 1752.9476939108558, 2203.76807557614, 1965.3479730383322, 2106.1703954147165, 1849.0076703108177, 1696.2089046486813, 1284.97519314033, 1030.9474934968296, 970.9883175919979, 947.5294473037568, 936.8256112948916, 906.54003530995, 901.7591043232416, 773.8338306449496, 688.0094575857073, 681.0178391118781, 626.341764253779, 568.7075221911605, 558.9122266209863, 518.9676301421476, 503.43726508539413, 498.33318617880576, 491.8580157025047, 490.3676455276005, 482.35892071027837, 477.04739896643326, 468.59810569680553, 443.81771526160816, 430.7023685301534, 429.5857989298619, 374.73213711243915, 345.3501225010848, 340.74101741602715, 978.0897438369085, 395.8803815322031, 690.8498786957331, 981.8282105547777, 731.209977835101, 1204.220554337577, 885.612042775866, 2905.0950247237897, 6146.80729273667, 969.0241249220481, 4140.580131506027, 1587.9230926142343, 882.9839107975339, 1123.7039089147786, 2386.3936179321436, 1792.806471391668, 1345.483214442426, 1765.3315477675917, 1123.213278357025, 1960.5636894242502, 2880.7196417312534, 3419.1669670381193, 1116.9336710594375, 1397.0537295979739, 2308.7171657083695, 2422.8310315095455, 2067.3468539364744, 2663.190865321563, 1779.7558937867284, 3519.2966795199372, 1326.8895808270968, 2391.8899868372505, 1438.204205607153, 2780.8919760672243, 1627.2786336070092, 1878.897287306103, 1483.7137937392426, 1473.4064494642337, 1400.316759404333, 1394.2535644840434, 786.4626380625572, 786.3766747100785, 757.0411415948342, 723.8406450611654, 705.7552049305692, 685.7765997434974, 663.2532825573455, 657.0607616501062, 640.81803857259, 606.183259614555, 579.114794270287, 561.9893368407865, 502.05152021628186, 494.03533865033563, 464.09534151802245, 442.9037634764216, 411.7772885955717, 404.94540160754696, 391.3795915199745, 372.75582739475595, 371.65410761255964, 359.72105669928635, 354.75356320508257, 311.29921036849464, 304.8307795606294, 302.39416680380276, 296.7218220617961, 295.73355322788564, 287.76071748221176, 282.2576629326588, 1402.6648189765672, 1403.7378845074513, 797.6177388311837, 1768.5782791493468, 627.2681468237514, 1023.1504106923707, 1642.1649798716087, 1054.135857487179, 678.8587083027227, 1779.585715932552, 625.6192127315617, 794.2689140552286, 1340.7912670452556, 750.1500881312986, 690.5570237050099, 3583.7921381728843, 846.3892830313009, 3366.840235264278, 2029.2338695224262, 2296.5684375895553, 1242.6279674384928, 2025.5609833544083, 2684.4534669383047, 1343.8416295324348, 1974.4366754456462, 2095.0604803891374, 3096.911434257819, 1497.8050484538862, 1163.2627894051702, 1281.547233146149, 1114.4510472475426, 2860.9323545848456, 1643.3644378822146, 1444.0203497472783, 1719.424946543502, 3735.7303932429854, 4286.284411352436, 1747.6836115117508, 2092.005289580754, 2027.3237052734314, 1338.2912041975171, 1371.9386691531874, 1453.1302672201198, 1451.3631533963728, 2065.281299300372, 1782.4932054143098, 813.8459746392931, 511.43163927779256, 393.04016206613426, 322.3594535325892, 305.82087667479163, 249.33318081121257, 222.42460837460644, 203.84137851724122, 202.93932929835103, 202.87350400956825, 198.20736496155604, 198.1889660006551, 186.780829791978, 182.0001255342008, 179.93209474859466, 174.28787031029066, 172.52181253658784, 171.88395352041704, 164.91222615312836, 161.53370071517185, 160.8696533634878, 160.12770554712736, 155.95569554125674, 155.40879413749624, 147.70824431085137, 146.5708585339346, 146.10302794311275, 145.7975416256844, 4080.357754589495, 333.3839136854124, 597.9748570019647, 14140.004702604485, 387.2563083816302, 401.61748106365724, 1889.4648666850228, 532.2143210838319, 5781.034860694067, 257.3283760407147, 4460.475428193558, 589.5674179292747, 902.723358803509, 380.56349253197453, 364.11299092330023, 1597.6384585251474, 2745.4644047974302, 3992.633481304115, 1734.5950986517769, 2920.167482005634, 235.31270488709316, 643.8065474812136, 509.77098921758034, 1032.1595412525558, 487.87148014079605, 721.9251145122699, 564.9413196878443, 1395.2578412751373, 686.556183435657, 573.9802714169706, 1253.8420870641085, 1872.9157584588788, 802.1903262627931, 580.6530216979083, 1068.5873206329982, 1288.7380965088428, 1172.266808958909, 916.5294502611708, 951.7104191802271, 793.398574585977, 988.7498210104897, 884.2725181945868, 887.0107028127427, 742.9826431090966]}, \"token.table\": {\"Term\": [\"1/10\", \"10/10\", \"10/10\", \"13th\", \"2/10\", \"9/11\", \"abc\", \"abrupt\", \"abrupt\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"acting\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actor\", \"actor\", \"actor\", \"actor\", \"actor\", \"affair\", \"affair\", \"affair\", \"agent\", \"agent\", \"agent\", \"agent\", \"aileen\", \"aired\", \"aired\", \"alien\", \"aliens\", \"allen\", \"allen\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"always\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"americans\", \"americans\", \"andre\", \"angus\", \"angus\", \"animation\", \"animation\", \"anime\", \"anna\", \"anna\", \"anna\", \"anna\", \"anna\", \"anne\", \"anne\", \"ants\", \"apes\", \"argento\", \"arizona\", \"arkin\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"arts\", \"arts\", \"arts\", \"ash\", \"asoka\", \"atrocious\", \"atrocious\", \"aubrey\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"audience\", \"austen\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"away\", \"awful\", \"awful\", \"awful\", \"awful\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bambi\", \"bank\", \"bank\", \"baron\", \"barrymore\", \"baseball\", \"baseball\", \"baseball\", \"batman\", \"bava\", \"beast\", \"beast\", \"beatles\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"becomes\", \"bela\", \"beliefs\", \"benny\", \"berry\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bette\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"bizarre\", \"bizarre\", \"bizarre\", \"bizarre\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blah\", \"blood\", \"blood\", \"blood\", \"blood\", \"blood\", \"bogart\", \"bollywood\", \"bond\", \"bond\", \"bond\", \"bond\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"books\", \"books\", \"books\", \"books\", \"boring\", \"boring\", \"boring\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"brightest\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"broadway\", \"bronson\", \"brown\", \"brown\", \"brown\", \"buffy\", \"bugs\", \"buscemi\", \"buscemi\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"ca\", \"cabaret\", \"cagney\", \"canceled\", \"candyman\", \"car\", \"car\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"career\", \"carrey\", \"cars\", \"cars\", \"cars\", \"cartoon\", \"cartoon\", \"cary\", \"cassavetes\", \"cassavetes\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cast\", \"cgi\", \"cgi\", \"chainsaw\", \"chan\", \"chaney\", \"chaplin\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"characters\", \"charlie\", \"charlie\", \"charlie\", \"chase\", \"chase\", \"chase\", \"chase\", \"chases\", \"che\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"christians\", \"christmas\", \"christmas\", \"cinderella\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinema\", \"cinematographer\", \"cinematography\", \"cinematography\", \"cinematography\", \"cinematography\", \"cities\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"claire\", \"claire\", \"claus\", \"clint\", \"clint\", \"coach\", \"coach\", \"columbo\", \"columbo\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"comedy\", \"communist\", \"completely\", \"completely\", \"completely\", \"completely\", \"completely\", \"completely\", \"completely\", \"completely\", \"connery\", \"contestants\", \"contrived\", \"contrived\", \"contrived\", \"contrived\", \"contrived\", \"cop\", \"cop\", \"cop\", \"cop\", \"cops\", \"costello\", \"costner\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"countries\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"courtroom\", \"crap\", \"crappy\", \"crappy\", \"creature\", \"creatures\", \"creatures\", \"crime\", \"crime\", \"crime\", \"crime\", \"crime\", \"crime\", \"crisp\", \"cronenberg\", \"crusoe\", \"cultural\", \"culture\", \"culture\", \"culture\", \"culture\", \"cultures\", \"d'amato\", \"dance\", \"dance\", \"dance\", \"dancing\", \"dancing\", \"dancing\", \"dancing\", \"dario\", \"daughter\", \"daughter\", \"daughter\", \"daughter\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"david\", \"de\", \"de\", \"de\", \"de\", \"dead\", \"dead\", \"dead\", \"dead\", \"dead\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delpy\", \"demon\", \"deniro\", \"development\", \"development\", \"development\", \"development\", \"development\", \"development\", \"devoid\", \"devoid\", \"dickens\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dinosaurs\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"director\", \"disturbing\", \"disturbing\", \"disturbing\", \"disturbing\", \"disturbing\", \"django\", \"documentaries\", \"documentary\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dont\", \"douglas\", \"douglas\", \"dracula\", \"dragon\", \"driver\", \"driver\", \"driver\", \"dumb\", \"dumb\", \"dumb\", \"dumb\", \"dumb\", \"dumb\", \"dumber\", \"dunne\", \"dustin\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"earth\", \"earth\", \"earth\", \"eastwood\", \"economic\", \"eddie\", \"eddie\", \"educated\", \"edwards\", \"edwards\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"effects\", \"emily\", \"emily\", \"emma\", \"emmanuelle\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"ending\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enjoyed\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"enough\", \"episode\", \"episode\", \"episodes\", \"erotic\", \"errol\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"even\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"ever\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"every\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"evil\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"explicit\", \"explicit\", \"exploitation\", \"exploitation\", \"extremes\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"facts\", \"facts\", \"fails\", \"fails\", \"fails\", \"fails\", \"fails\", \"fails\", \"fails\", \"fails\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fans\", \"fans\", \"fans\", \"fans\", \"fans\", \"fans\", \"fans\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"farrah\", \"father\", \"father\", \"father\", \"father\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"fbi\", \"fellini\", \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"female\", \"femme\", \"fi\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"film\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"films\", \"finds\", \"finds\", \"finds\", \"finds\", \"finds\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"flynn\", \"fonda\", \"fonda\", \"fonda\", \"football\", \"football\", \"ford\", \"ford\", \"ford\", \"fort\", \"fox\", \"fox\", \"fox\", \"fox\", \"fox\", \"fox\", \"franchise\", \"frankie\", \"freddy\", \"freedom\", \"french\", \"french\", \"french\", \"french\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fu\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funny\", \"funny\", \"funny\", \"funny\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gang\", \"gang\", \"garbage\", \"gene\", \"gene\", \"gene\", \"george\", \"george\", \"george\", \"george\", \"george\", \"george\", \"george\", \"george\", \"german\", \"german\", \"german\", \"germans\", \"germany\", \"germany\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"gets\", \"giallo\", \"giant\", \"giant\", \"giant\", \"giant\", \"giant\", \"ginger\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"global\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"goes\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"going\", \"goldie\", \"gon\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goodfellas\", \"gore\", \"gore\", \"gore\", \"gorilla\", \"government\", \"government\", \"government\", \"grant\", \"grant\", \"graphics\", \"gratuitous\", \"gratuitous\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"gregory\", \"gregory\", \"griffith\", \"grodin\", \"guest\", \"guest\", \"gun\", \"gun\", \"gun\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"hackman\", \"hackneyed\", \"hardy\", \"hawn\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"heartwarming\", \"heston\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"hilarious\", \"historical\", \"historical\", \"historically\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hitchcock\", \"hitchcock\", \"hitchcock\", \"hitler\", \"hogan\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"hollywood\", \"holmes\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hong\", \"hopkins\", \"hopkins\", \"horrible\", \"horrible\", \"horrible\", \"horrid\", \"horror\", \"horror\", \"horror\", \"hostage\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hughes\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humor\", \"humour\", \"humour\", \"humour\", \"humour\", \"humour\", \"humour\", \"humphrey\", \"husband\", \"husbands\", \"hustle\", \"hutton\", \"idiot\", \"idiot\", \"idiot\", \"indigo\", \"inexplicable\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"iraq\", \"irene\", \"it`s\", \"italian\", \"italian\", \"italian\", \"italian\", \"italian\", \"jack\", \"jack\", \"jack\", \"jack\", \"jack\", \"jackie\", \"jews\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"john\", \"jokes\", \"jokes\", \"joshua\", \"karloff\", \"karloff\", \"kay\", \"keane\", \"keaton\", \"kelly\", \"kelly\", \"khan\", \"kidman\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"killed\", \"killed\", \"killed\", \"killed\", \"killed\", \"killed\", \"killer\", \"killer\", \"killer\", \"killer\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kung\", \"kurosawa\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lame\", \"lame\", \"lana\", \"lana\", \"lance\", \"laugh\", \"laugh\", \"laugh\", \"laugh\", \"laughton\", \"leaders\", \"leatherface\", \"leigh\", \"lenny\", \"leopard\", \"leung\", \"lewton\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"liked\", \"liked\", \"liked\", \"liked\", \"liked\", \"lime\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"lives\", \"liza\", \"lloyd\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loved\", \"loved\", \"loved\", \"loved\", \"loved\", \"lover\", \"lover\", \"lover\", \"lover\", \"lover\", \"loy\", \"lugosi\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"made\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"maris\", \"marriage\", \"marriage\", \"married\", \"married\", \"married\", \"married\", \"marries\", \"marry\", \"marry\", \"martial\", \"martial\", \"mary\", \"mary\", \"mary\", \"mary\", \"mcconaughey\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean-spirited\", \"media\", \"media\", \"media\", \"meets\", \"meets\", \"meets\", \"meets\", \"meets\", \"meets\", \"melanie\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"meryl\", \"message\", \"message\", \"message\", \"message\", \"mexicans\", \"mgm\", \"mickey\", \"mickey\", \"military\", \"military\", \"military\", \"mins\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"minutes\", \"mitchum\", \"money\", \"money\", \"money\", \"money\", \"money\", \"monster\", \"monsters\", \"mother\", \"mother\", \"mother\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"movies\", \"mrs\", \"mrs\", \"mrs\", \"mst3k\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mulder\", \"mummy\", \"murphy\", \"murphy\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"musical\", \"musical\", \"musical\", \"musical\", \"muslim\", \"na\", \"nation\", \"nation\", \"natives\", \"nazis\", \"nbc\", \"neil\", \"nero\", \"network\", \"network\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"nikki\", \"ninjas\", \"noir\", \"nope\", \"norma\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"nothing\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"nudity\", \"nudity\", \"nudity\", \"nuns\", \"o'neill\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"ok\", \"ok\", \"ok\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"olsen\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"orson\", \"outlaw\", \"pacing\", \"pacing\", \"pacing\", \"pammy\", \"panther\", \"parents\", \"parents\", \"parents\", \"parents\", \"parents\", \"parents\", \"pathetic\", \"pathetic\", \"pathetic\", \"peck\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performances\", \"performances\", \"performances\", \"performances\", \"performances\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perhaps\", \"perverse\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"peter\", \"photography\", \"photography\", \"photography\", \"photography\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"picture\", \"pilot\", \"pilot\", \"pilot\", \"plane\", \"plane\", \"plane\", \"plane\", \"planet\", \"planet\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"played\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"playing\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"plot\", \"poe\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"poirot\", \"police\", \"police\", \"police\", \"police\", \"political\", \"political\", \"politics\", \"politics\", \"porky\", \"pornography\", \"powell\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"predator\", \"predictability\", \"premise\", \"premise\", \"premise\", \"premise\", \"premise\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"prison\", \"prison\", \"prison\", \"prison\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"program\", \"program\", \"program\", \"programme\", \"propaganda\", \"pryor\", \"pryor\", \"quaid\", \"quaid\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"quite\", \"racism\", \"rathbone\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"reality\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"really\", \"rebecca\", \"rebecca\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"reilly\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"religion\", \"religion\", \"rent\", \"rent\", \"reruns\", \"rex\", \"richards\", \"ringu\", \"robin\", \"robin\", \"robin\", \"robin\", \"robot\", \"robots\", \"rochester\", \"rodman\", \"rogers\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roles\", \"roswell\", \"rubbish\", \"rubbish\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"sadako\", \"sandler\", \"sarandon\", \"saw\", \"saw\", \"saw\", \"saw\", \"saw\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scary\", \"scary\", \"scary\", \"scary\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"scenes\", \"sci\", \"sci-fi\", \"science\", \"science\", \"science\", \"scientist\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"scorsese\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screwball\", \"scrooge\", \"scully\", \"seagal\", \"season\", \"seasons\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seem\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seems\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"seen\", \"sellers\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sense\", \"sequel\", \"sequel\", \"sequel\", \"sergio\", \"series\", \"series\", \"series\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sets\", \"sex\", \"sex\", \"sex\", \"sex\", \"sexual\", \"sexual\", \"sexual\", \"sexual\", \"sh*t\", \"shark\", \"sharpe\", \"sheriff\", \"sherlock\", \"shields\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"simpsons\", \"simpsons\", \"singer\", \"singer\", \"singer\", \"singer\", \"singing\", \"singing\", \"singing\", \"sings\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sister\", \"sitcom\", \"sitcoms\", \"sketch\", \"sketches\", \"sketches\", \"skits\", \"snl\", \"snoopy\", \"social\", \"social\", \"social\", \"social\", \"society\", \"society\", \"society\", \"society\", \"soldiers\", \"soldiers\", \"solondz\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"something\", \"son\", \"son\", \"son\", \"son\", \"song\", \"song\", \"song\", \"song\", \"song\", \"songs\", \"songs\", \"sopranos\", \"sorority\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"sort\", \"soviet\", \"space\", \"space\", \"space\", \"space\", \"spaghetti\", \"special\", \"special\", \"special\", \"special\", \"special\", \"special\", \"spiritual\", \"spiritual\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stage\", \"stallone\", \"stan\", \"stanwyck\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"stars\", \"stars\", \"stars\", \"stars\", \"stars\", \"stars\", \"stars\", \"stars\", \"stars\", \"states\", \"states\", \"states\", \"states\", \"steve\", \"steve\", \"steve\", \"steve\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stiller\", \"stinker\", \"stinks\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"streep\", \"stuart\", \"stupid\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"suck\", \"suck\", \"sucks\", \"superman\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"supporting\", \"susan\", \"susan\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"takes\", \"tarzan\", \"team\", \"team\", \"team\", \"team\", \"team\", \"technicolor\", \"television\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tension\", \"tension\", \"tension\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"though\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thurman\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tony\", \"tony\", \"tony\", \"tony\", \"tony\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"transitions\", \"trash\", \"trash\", \"trivial\", \"troll\", \"troma\", \"troops\", \"troy\", \"troy\", \"truck\", \"truck\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"truth\", \"tucci\", \"tv\", \"tv\", \"tv\", \"tv\", \"tv\", \"twist\", \"twist\", \"twist\", \"twist\", \"twist\", \"twist\", \"twist\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"uma\", \"unappealing\", \"undercover\", \"underdeveloped\", \"united\", \"united\", \"unsatisfying\", \"unsympathetic\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"us\", \"val\", \"vampire\", \"vampires\", \"vegas\", \"view\", \"view\", \"view\", \"view\", \"view\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"viewer\", \"virus\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"wan\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"wants\", \"war\", \"war\", \"warner\", \"warner\", \"waste\", \"wasting\", \"wasting\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watching\", \"watching\", \"watching\", \"watching\", \"watching\", \"watson\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wealthy\", \"wealthy\", \"wealthy\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"welles\", \"werewolf\", \"western\", \"western\", \"western\", \"westerns\", \"westerns\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"widescreen\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"wilder\", \"willis\", \"winkler\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"women\", \"women\", \"women\", \"women\", \"women\", \"women\", \"women\", \"women\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"woo\", \"woodstock\", \"woody\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worse\", \"worse\", \"worse\", \"worse\", \"worst\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"writers\", \"writers\", \"writers\", \"writers\", \"writers\", \"yeah\", \"yeah\", \"yeah\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"young\", \"zombie\", \"zombie\", \"zombies\", \"zombies\"], \"Topic\": [1, 3, 5, 8, 1, 4, 10, 2, 5, 1, 2, 3, 5, 7, 9, 1, 2, 3, 5, 6, 8, 1, 3, 5, 7, 9, 2, 5, 7, 1, 6, 7, 10, 3, 3, 10, 8, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 10, 4, 10, 6, 3, 6, 3, 8, 8, 3, 4, 5, 7, 9, 7, 9, 3, 5, 2, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 8, 6, 5, 1, 2, 1, 1, 2, 3, 4, 5, 7, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 3, 4, 6, 7, 9, 3, 6, 10, 8, 1, 5, 8, 9, 2, 4, 5, 6, 7, 8, 9, 2, 4, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 9, 10, 2, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 5, 6, 8, 9, 9, 3, 5, 6, 7, 1, 2, 3, 4, 5, 7, 8, 3, 4, 8, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 3, 1, 3, 4, 5, 7, 9, 10, 9, 5, 6, 9, 10, 1, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 7, 9, 10, 4, 1, 6, 1, 2, 3, 5, 6, 7, 9, 9, 1, 4, 6, 8, 10, 9, 5, 7, 1, 2, 3, 5, 7, 8, 9, 10, 1, 8, 8, 6, 6, 9, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 10, 3, 6, 9, 2, 5, 6, 10, 6, 4, 1, 2, 3, 4, 7, 8, 10, 3, 6, 3, 1, 2, 3, 4, 5, 9, 5, 1, 2, 3, 5, 4, 3, 4, 5, 6, 7, 8, 9, 10, 7, 9, 3, 3, 5, 2, 10, 7, 10, 1, 2, 3, 9, 10, 4, 1, 2, 3, 4, 5, 7, 8, 9, 6, 10, 1, 2, 7, 9, 10, 1, 2, 5, 6, 6, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 3, 4, 5, 6, 7, 9, 10, 7, 1, 1, 8, 8, 4, 8, 2, 4, 5, 6, 7, 10, 5, 2, 3, 4, 1, 4, 8, 10, 4, 2, 1, 4, 9, 1, 2, 3, 9, 2, 3, 6, 7, 8, 1, 2, 3, 5, 6, 7, 9, 10, 4, 5, 7, 9, 1, 2, 6, 7, 8, 1, 2, 3, 5, 8, 10, 2, 8, 3, 1, 2, 3, 4, 8, 10, 1, 2, 3, 1, 2, 3, 4, 5, 8, 9, 10, 8, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 5, 4, 4, 1, 2, 3, 5, 6, 8, 1, 7, 9, 8, 8, 3, 4, 6, 1, 2, 6, 8, 9, 10, 1, 9, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 8, 5, 4, 6, 9, 4, 5, 9, 1, 2, 3, 4, 5, 8, 3, 7, 3, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 5, 8, 9, 10, 1, 2, 3, 4, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 10, 10, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 5, 7, 9, 2, 4, 2, 5, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 8, 10, 1, 2, 3, 5, 8, 9, 10, 3, 5, 8, 9, 10, 7, 3, 6, 7, 8, 1, 3, 5, 6, 8, 9, 10, 6, 2, 1, 2, 3, 5, 6, 7, 8, 9, 10, 5, 1, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 8, 9, 2, 6, 7, 8, 9, 1, 2, 3, 4, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 5, 7, 9, 6, 10, 5, 6, 7, 10, 3, 4, 6, 7, 9, 10, 8, 10, 8, 4, 2, 4, 5, 9, 1, 3, 6, 7, 9, 8, 1, 2, 3, 6, 8, 9, 10, 1, 3, 9, 10, 1, 2, 6, 7, 8, 10, 5, 6, 1, 6, 8, 9, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 4, 4, 5, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 6, 7, 8, 9, 10, 2, 1, 5, 6, 8, 9, 9, 1, 2, 3, 6, 7, 9, 4, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 10, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 8, 6, 4, 6, 8, 7, 9, 8, 1, 2, 1, 2, 3, 4, 5, 6, 8, 9, 10, 6, 7, 5, 3, 9, 10, 1, 5, 6, 1, 2, 3, 6, 7, 9, 10, 5, 2, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 1, 3, 6, 8, 9, 10, 4, 5, 4, 1, 3, 4, 5, 8, 9, 10, 2, 5, 9, 4, 6, 1, 2, 3, 4, 5, 7, 9, 7, 1, 2, 3, 4, 6, 7, 8, 10, 6, 5, 9, 1, 7, 8, 1, 1, 2, 8, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 2, 3, 4, 5, 7, 8, 1, 2, 3, 5, 8, 9, 10, 1, 2, 3, 5, 9, 10, 9, 7, 7, 3, 3, 1, 2, 10, 3, 2, 1, 2, 3, 4, 5, 8, 9, 10, 4, 9, 4, 1, 2, 4, 5, 7, 1, 5, 6, 9, 10, 6, 4, 3, 5, 6, 7, 8, 9, 10, 1, 10, 6, 2, 9, 7, 5, 9, 6, 9, 9, 3, 1, 2, 6, 7, 8, 1, 2, 4, 6, 7, 8, 1, 2, 6, 8, 1, 2, 3, 4, 6, 7, 8, 10, 8, 5, 1, 2, 3, 4, 5, 8, 10, 1, 8, 5, 8, 6, 1, 3, 9, 10, 7, 4, 2, 7, 10, 7, 3, 7, 1, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 8, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 7, 8, 10, 3, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 1, 3, 4, 8, 10, 2, 3, 5, 7, 9, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 7, 4, 7, 3, 7, 9, 10, 7, 7, 9, 6, 8, 3, 7, 9, 10, 3, 1, 2, 3, 4, 5, 6, 8, 10, 2, 2, 4, 10, 2, 3, 6, 7, 8, 9, 7, 1, 2, 3, 4, 5, 6, 7, 9, 10, 7, 1, 2, 3, 4, 10, 9, 7, 10, 4, 6, 8, 3, 1, 2, 3, 5, 6, 8, 9, 5, 1, 4, 6, 7, 10, 8, 8, 3, 6, 7, 1, 2, 3, 4, 5, 8, 9, 3, 7, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 8, 8, 9, 1, 2, 3, 4, 5, 8, 9, 3, 4, 5, 9, 4, 1, 4, 10, 4, 4, 10, 9, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 10, 6, 1, 5, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 1, 2, 8, 2, 10, 1, 2, 3, 6, 8, 9, 10, 1, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 8, 9, 10, 5, 5, 2, 5, 8, 6, 5, 2, 3, 4, 6, 7, 10, 1, 2, 7, 7, 1, 2, 3, 4, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 9, 2, 3, 5, 7, 9, 1, 2, 3, 4, 5, 7, 9, 10, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 9, 6, 8, 10, 1, 4, 6, 8, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 8, 9, 2, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 2, 5, 6, 7, 4, 10, 4, 10, 3, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 2, 1, 2, 6, 8, 10, 1, 2, 3, 5, 6, 8, 9, 4, 5, 6, 7, 1, 2, 3, 4, 5, 8, 9, 10, 4, 8, 10, 10, 4, 3, 8, 1, 3, 1, 2, 3, 4, 5, 7, 8, 9, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 7, 9, 1, 2, 3, 4, 8, 7, 2, 3, 4, 7, 4, 10, 1, 3, 10, 5, 10, 2, 3, 7, 8, 9, 8, 8, 3, 1, 9, 1, 2, 3, 5, 6, 7, 9, 1, 2, 3, 5, 7, 9, 6, 1, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 7, 1, 3, 4, 8, 10, 1, 2, 3, 4, 5, 8, 10, 1, 2, 3, 8, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 3, 5, 6, 8, 9, 1, 8, 1, 4, 8, 8, 1, 2, 3, 5, 8, 9, 5, 1, 2, 3, 4, 5, 7, 8, 9, 9, 3, 10, 1, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 9, 10, 5, 1, 2, 3, 4, 5, 7, 8, 1, 3, 8, 5, 5, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 1, 2, 7, 10, 2, 4, 7, 10, 10, 8, 7, 6, 5, 7, 1, 2, 3, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 10, 4, 7, 8, 9, 1, 3, 9, 9, 1, 2, 3, 6, 7, 8, 9, 10, 10, 10, 9, 10, 10, 10, 6, 2, 4, 7, 10, 2, 3, 4, 7, 4, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 6, 7, 8, 1, 3, 6, 8, 9, 3, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 1, 2, 4, 8, 5, 1, 3, 4, 5, 8, 9, 3, 4, 1, 2, 4, 5, 7, 9, 10, 6, 6, 9, 1, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 4, 5, 6, 10, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 1, 1, 2, 3, 4, 5, 7, 8, 9, 7, 7, 1, 1, 2, 3, 4, 5, 8, 9, 10, 1, 8, 1, 8, 3, 5, 7, 9, 10, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 5, 6, 8, 9, 10, 5, 3, 4, 5, 8, 9, 10, 2, 3, 5, 1, 2, 4, 7, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 9, 10, 3, 4, 5, 6, 7, 8, 9, 1, 2, 5, 6, 7, 9, 2, 1, 2, 2, 1, 1, 4, 2, 4, 1, 6, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 7, 9, 10, 3, 1, 3, 5, 8, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 2, 6, 2, 4, 6, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 10, 7, 8, 8, 6, 1, 2, 3, 4, 5, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 8, 1, 1, 2, 3, 4, 6, 7, 8, 9, 4, 5, 8, 9, 1, 1, 2, 1, 2, 3, 4, 5, 9, 10, 1, 2, 3, 4, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 8, 4, 5, 6, 5, 6, 1, 3, 4, 5, 6, 8, 9, 5, 1, 2, 3, 6, 7, 9, 10, 9, 6, 3, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 4, 6, 7, 9, 10, 3, 4, 5, 9, 6, 3, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 7, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 10, 1, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 1, 8, 1, 8], \"Freq\": [0.9975871226286266, 0.9783377820648872, 0.02019083602760555, 0.9988775042995797, 0.9950278874313948, 0.9951727326560453, 0.9962399529904097, 0.986920324844436, 0.010669408917237146, 0.6005081754440577, 0.08483618012186958, 0.24405081743694057, 0.057713208889762294, 0.005033128682246712, 0.007829311283494886, 0.19842709729604324, 0.0019339873030803435, 0.07126743211851065, 0.2887443043498953, 0.2398144255819626, 0.1998775877733535, 0.27692148058946214, 0.22812681743743793, 0.1586218792498279, 0.0784480114984312, 0.25778016578384494, 0.1985547295276402, 0.044865732152880235, 0.7560353162783222, 0.03458906231677476, 0.8292505965688309, 0.0975588937139801, 0.038136658451828585, 0.9907475192099888, 0.07146366477993844, 0.9267223626301695, 0.99934813665346, 0.9991811978816285, 0.31204500521024336, 0.687260096841097, 0.04428707363730911, 0.16368885918069537, 0.3169789217204905, 0.080977863203606, 0.10859736208316015, 0.06987843842023378, 0.0287626290067453, 0.08820539562068559, 0.07714284600270663, 0.02149822142427245, 0.07285722728095084, 0.07230942857959032, 0.4444838662839362, 0.04480993377129157, 0.0921397415688416, 0.029471570133196658, 0.023884023379319223, 0.0251987402625845, 0.09433093637428372, 0.10046628182952169, 0.056607015287276886, 0.7601513481434325, 0.04620980839777705, 0.07278044822649886, 0.03754546932319386, 0.012418886006902583, 0.014151753821819222, 0.054386303904021834, 0.004785994743553922, 0.7322571957637499, 0.03002123975502005, 0.04655467614184269, 0.0030456330186252227, 0.02044925026791221, 0.10790242694557932, 0.11635620567932851, 0.022505739782712225, 0.010563918673517983, 0.5998468603310646, 0.14207705114528535, 0.010104617861625897, 0.032304157103076735, 0.06629241718309112, 0.9565387323284799, 0.04267777079254129, 0.9977607549775221, 0.9714160647259046, 0.02676077313294503, 0.2704050234040934, 0.7288472073717547, 0.9992727495135779, 0.11367735355292406, 0.03126127222705412, 0.03126127222705412, 0.8184769455810533, 0.004262900758234652, 0.9083775094414377, 0.09067289114206728, 0.9977369605468693, 0.995673213538604, 0.9979412386431235, 0.9944941043633682, 0.9973757346147283, 0.2017570517710932, 0.24152692157627384, 0.1076787984347815, 0.07616305255143081, 0.0018759372549613503, 0.2136692533400978, 0.04286516627586685, 0.08591792627722984, 0.002438718431449755, 0.02607552784396277, 0.09925942107598117, 0.0030385537064075863, 0.8973861946257072, 0.9941933581569122, 0.9954203786113386, 0.9523929701573014, 0.047054988644135445, 0.9918751585970318, 0.04752498207816626, 0.5976185671529954, 0.12172214797571154, 0.15938650452065284, 0.013901865505858157, 0.00016164959890532741, 0.009214027137603662, 0.050434674858462154, 0.9961426694726, 0.2585724925374421, 0.14433929278263785, 0.18561377515950653, 0.067374522703418, 0.0002427910728051099, 0.25019620052566577, 0.08861874157386512, 0.004855821456102198, 0.9481978823904958, 0.02249106038922258, 0.0010509841303375038, 0.0281663746930451, 0.21953844684635143, 0.01351332366988499, 0.21239265789002482, 0.05603430547931368, 0.0002830015428248166, 0.307056673964926, 0.03169617279637946, 0.10499357238800694, 0.015989587169602134, 0.038558960209881254, 0.9266924935849639, 0.024334668368157855, 0.000708053137817763, 0.0011179786386596258, 0.02981276369759002, 0.002049627504209314, 0.014570988257197123, 0.000708053137817763, 0.9977097804157623, 0.046841013713430545, 0.952433945506421, 0.9949881673220806, 0.9967482354088946, 0.0648184242233072, 0.0902828051681779, 0.8426395149029937, 0.9985538492288443, 0.9966582991240106, 0.06634241289004274, 0.9326212273581007, 0.9982554016780417, 0.34727431127442787, 0.037154883517235504, 0.021302133216548357, 0.1174094319144642, 0.4163823946164859, 0.04532895789102732, 0.014861953406894201, 0.9954776815735261, 0.9982039754943273, 0.9959183693561847, 0.9965790811760625, 0.11254031597955648, 0.017830788192341577, 0.4304257030999962, 0.0014814898201352053, 0.16566230810154742, 0.020158843623982615, 0.014761987850632938, 0.022910181861376566, 0.16386335617709752, 0.05037065388459698, 0.9984271651310762, 0.4854121803804598, 0.10055674446454824, 0.27631244565910645, 0.002390044361186364, 0.04628524933614568, 0.0009327002385117518, 0.0011658752981396896, 0.02640707550286397, 0.04937481887621586, 0.011134109097234036, 0.32572294460500784, 0.015044240156895898, 0.258680156254814, 0.009591965200705438, 0.027766215054673637, 0.1741698944338619, 0.00020193610948853553, 0.035944627488959324, 0.11984908098144584, 0.033117521956119826, 0.12734672533947364, 0.3656827087118851, 0.34537310165343454, 0.04555938880679445, 0.023054148552835746, 0.056317991464784466, 0.03447144116947821, 0.0023054148552835744, 0.8070779503314324, 0.04385524357629168, 0.004951398468290996, 0.14429789821876618, 0.16793065601254126, 0.05636369500914515, 0.0004973267206689277, 0.2884494979879781, 0.19312854319310027, 0.21816065480010297, 0.0036470626182388034, 0.05536904156780729, 0.011935841296054265, 0.00447594048602035, 0.9982717717808753, 0.11959178237572411, 0.15289582303731816, 0.06630531731717362, 0.06751637334123159, 0.5937202157944177, 0.9988872664835666, 0.9982572768269681, 0.08348176481411648, 0.10267297511621222, 0.7801226987801919, 0.03358461802866755, 0.04982812305800164, 0.04229397386710105, 0.7970787382645967, 0.09006732896394799, 0.0003424613268591178, 0.012157377103498682, 0.008219071844618828, 0.7637352847625315, 0.21535093459237, 0.019213466283334125, 0.001601122190277844, 0.7593639733892776, 0.2056304763529959, 0.03488374152416895, 0.019912352820869804, 0.7748502510729771, 0.12899741610041743, 0.0025972634114178003, 0.030301406466541003, 0.042421969053157406, 0.10041939532208986, 0.012213169701335255, 0.2605476202951521, 0.0006785094278519586, 0.22911001680467802, 0.34943235534375866, 0.0474956599496371, 0.9945314696142987, 0.008719611708650864, 0.030155323825750903, 0.2706712801227039, 0.47122234942167374, 0.025068883662371234, 0.050864401633796706, 0.1427836417291579, 0.9981979242751653, 0.9968561466874637, 0.7926973716822086, 0.18477262476621265, 0.02167086339850642, 0.9971742044387271, 0.9986835970532173, 0.017637904169683416, 0.9789036814174296, 0.5661810706615118, 0.08560941906998099, 0.2345847618620658, 0.007383345094463426, 0.03915976702000222, 0.028038019346063644, 0.0009346006448687881, 0.020935054445060854, 0.0171966518655857, 0.9953197959175837, 0.9982744619754921, 0.9936631562282112, 0.9981261793853006, 0.09117849852577962, 0.9085180032259279, 0.12689270519694337, 0.0003253659107613933, 0.006181952304466473, 0.173745396346584, 0.044575129774310884, 0.21116247608414423, 0.4372917840633126, 0.9977247687736832, 0.13530689112628488, 0.03131068554988411, 0.8319696446112063, 0.741754721126385, 0.25765517618356076, 0.9990204713913026, 0.9810594970009404, 0.018510556547187557, 0.15917658334740442, 0.05872361375588984, 0.2033972440090366, 0.25233358880561146, 0.029895657912089372, 0.007918790339809387, 0.23880936260728533, 0.04964814617543414, 0.13586331424925727, 0.8630741473531954, 0.9974248640368918, 0.9973353594421283, 0.9972466405192687, 0.999078435615681, 0.05990863314418391, 0.46577744614457783, 0.24752493791767693, 0.04617348310624906, 0.007208518459625381, 0.09570769441326941, 0.000487062058082796, 0.047098901016606375, 0.030100435189516793, 0.06359124509308421, 0.5307903417696928, 0.3163837874049375, 0.0009249635649903158, 0.015863125139583916, 0.00013874453474854736, 0.018083037695560675, 0.054202864908432506, 0.11856596852579533, 0.8071323074799484, 0.07364345871167412, 0.05787411548023971, 0.027372892456870135, 0.826661352197478, 0.08759325586198442, 0.9984239470012267, 0.9978115637236719, 0.035258236907181745, 0.0667594485701556, 0.3629864389788547, 0.045662306814218984, 0.45055402736308475, 0.03901526215138964, 0.9968564258746707, 0.869695539414499, 0.12950792683148324, 0.9922369005492071, 0.15813073057019789, 0.1553653282867763, 0.12142630026296593, 0.23103679076949418, 0.3323510744257577, 0.0015084012455026823, 0.9957933538019218, 0.08693476031795727, 0.19845353072582866, 0.1688814606176711, 0.5454799919950515, 0.9951011611929446, 0.07198021826280211, 0.23386979340331673, 0.2135676805599623, 0.40050531700071945, 0.017929138614910415, 0.029794009757130542, 0.02346607848127981, 0.00870090550429476, 0.9622750050033173, 0.03523823961983979, 0.9968827382057149, 0.026245019508828734, 0.9739818351054219, 0.04898859353683493, 0.9479292849377559, 0.9802381637484725, 0.01650232598903152, 0.20665491822596613, 0.018274882228722967, 0.10699306049025599, 0.35774144455878043, 0.31024800062715735, 0.9973018361446637, 0.3052463823445058, 0.46966358084832943, 0.15917541746756997, 0.04385623466998909, 0.0010483562072507353, 0.012405548452467035, 0.0034945206908357845, 0.005241781036253676, 0.9990185761027601, 0.9951460361886, 0.12764322351037832, 0.8068875200477487, 0.011396716384855206, 0.022793432769710412, 0.030771134239109058, 0.01922096916944841, 0.032347484699803426, 0.005625649513009291, 0.9427650975551404, 0.9988355392431725, 0.9985234080032478, 0.9962158957451396, 0.5143616857851289, 0.1569382242988231, 0.23030279572967716, 0.07297733041082555, 0.0012321305182725008, 0.0020770200165165014, 0.002992316972947502, 0.0003168335618415002, 0.01489117740655051, 0.003942817658472003, 0.9981927382106329, 0.00037545198849662516, 0.016519887493851505, 0.7418931292693313, 0.026657091183260383, 0.1494298914216568, 0.005631779827449377, 0.03416613095319289, 0.025155283229273885, 0.9978048412341016, 0.9997680889518009, 0.9850603883243098, 0.01338397266744986, 0.9990886677433776, 0.028095817814959415, 0.9707105055068478, 0.04414862022569183, 0.02861484644257804, 0.3229389812805236, 0.5473611340944571, 0.03760808389595971, 0.019212825468588112, 0.994669368161302, 0.996258503134485, 0.9906241828243353, 0.9984002739648644, 0.029177829897702068, 0.893959043674276, 0.0142785125031308, 0.06208048914404695, 0.9976624243804798, 0.9924689771015809, 0.09585276120813434, 0.023806568143196764, 0.8795900440275857, 0.14440919447764994, 0.005348488684357405, 0.09538138153770706, 0.7541369044943941, 0.9965270850024212, 0.05852667666759276, 0.09369763729412738, 0.8350355417503024, 0.012639563975160879, 0.004216714127593634, 0.13039377533020313, 0.0690892391674957, 0.18099434486132676, 0.06325071191390451, 0.41323798450417615, 0.026273372641160336, 0.11190510569383105, 0.09900753520104684, 0.372683385778613, 0.04626116273586032, 0.4816349092312934, 0.15551376130455405, 0.12455335573591286, 0.19234596792931685, 0.11672428766108406, 0.4108481405631754, 0.1479676548791748, 0.14588945747918639, 0.6078727394966099, 0.03740755319979138, 0.05902080615967084, 0.001454738179991887, 0.9947997371491445, 0.9975350983843803, 0.9942930342825658, 0.003779524204024205, 0.7845212383495958, 0.1436219197529198, 0.040494902185973625, 0.02267714522414523, 0.00431945623317052, 0.07982113538437847, 0.9179430569203525, 0.9934041862148727, 0.053499868661307334, 0.2434844396680446, 0.48123198568911607, 0.15916544467067245, 0.005870309778298061, 0.031085958598714734, 0.0009339129192746916, 0.024548568163791893, 0.9981314272670806, 0.20349884505194216, 0.3506966201814631, 0.03434614753022154, 0.06528262908286661, 0.2635422312912157, 0.027277327820611784, 0.055219956319775075, 0.026947167237824126, 0.751526552965984, 0.06362525597819586, 0.15494621161748873, 0.0029941296930915697, 0.9974167056111188, 0.9984360743560534, 0.9995248446935039, 0.22510136569966238, 0.015169874644977249, 0.11352938443982973, 0.005872209539991193, 0.5519876967591721, 0.0880831430998679, 0.9969638178809361, 0.19593625247916924, 0.8030174281933166, 0.999549986346276, 0.9980243884741455, 0.04765286143706703, 0.04484975194076897, 0.9068059220524226, 0.8559437214158259, 0.001141258295221101, 0.07189927259892938, 0.02168390760920092, 0.003994404033273854, 0.04565033180884405, 0.9949421459564154, 0.9985459036568035, 0.9973030415673522, 0.00722647477186166, 0.14359704707957363, 0.08555213681526545, 0.08392035218936121, 0.3184311141407428, 0.0421932881840955, 0.0013986725364893535, 0.0361323738593083, 0.21562868270877533, 0.06643694548324429, 0.1549295606377298, 0.2225999434450141, 0.6219442419853694, 0.9982741208250911, 0.9954670219242152, 0.19366461079726702, 0.8050769684451834, 0.9938049596590416, 0.9676030069491913, 0.027945213197088557, 0.2668860392083669, 0.0014985179068409146, 0.057692939413375215, 0.0008991107441045488, 0.05244812673943201, 0.6205362652228228, 0.07702900701433688, 0.9226363284606127, 0.9980370706333132, 0.9963283731449984, 0.263887249020663, 0.2789151429780335, 0.22805150189154888, 0.05304718123413669, 0.001220213611923241, 0.12227824826851848, 0.037248626048183145, 0.013807680345447201, 0.001412878919069016, 6.422176904859163e-05, 0.15916426276795445, 0.4069682235674109, 0.35027200485771387, 0.0022125353642808614, 0.05918532099451304, 0.015349464089698476, 0.006637606092842584, 0.2650507803929911, 0.07255470175446121, 0.5708029788433628, 0.020645646840700346, 0.021825398088740365, 0.023201774544787054, 0.025757902248873765, 0.08750542067083202, 0.05744220866122102, 0.8318383394445044, 0.0005368430716001964, 0.01637371368380599, 0.0002684215358000982, 0.006173695323402259, 0.29174412233235114, 0.3682547522721563, 0.17574718956470098, 0.030017165459057225, 0.01685506451644223, 0.04374741464379949, 0.01022666835829079, 0.03711901848564805, 0.02045333671658158, 0.005681482421272661, 0.04716455109846473, 0.9526305370382975, 0.9994318335285136, 0.9982498945294123, 0.9973530768623133, 0.4779603137533351, 0.19886475695059438, 0.1370881282026511, 0.06416833280857638, 0.03508738570769678, 0.00668589998767886, 0.007963287383698805, 0.0331848938412841, 0.02641745848790184, 0.012610803228792647, 0.5683258332140176, 0.0013200862617574638, 0.2769885347496313, 0.02427810820536553, 0.09131553228070108, 0.000688740658308242, 5.739505485902016e-05, 0.004878579663016714, 0.009814554380892447, 0.022326676340158842, 0.3380084394647031, 0.13610224418987668, 0.25558390279644133, 0.09183721523840277, 0.0373963175624521, 0.007377504825245652, 0.0061903201407233635, 0.00695351029505912, 0.01416141730823016, 0.10633782817078216, 0.03359935306201069, 0.06925057501330448, 0.013850115002660894, 0.07976640307088034, 0.06925057501330448, 0.06950705862446487, 0.05257914028787932, 0.6119698962286833, 0.5926399882663936, 0.010107560914151114, 0.3156928192186531, 0.03133343883386845, 0.05036934522218638, 0.9830067937610606, 0.015456081662909758, 0.8836855447877329, 0.1147823164591265, 0.9917038280714371, 0.28453343792879277, 0.29373335242182375, 0.15715730221600321, 0.18286015610890416, 0.022952363992922616, 0.007113335948219819, 0.014416360855058834, 0.02124516336534986, 0.009768981368888551, 0.006164891155123843, 0.019825489671642625, 0.9791589065605719, 0.18403321790816415, 0.7494073988260325, 0.018101627990966967, 0.004827100797591191, 0.0024135503987955955, 0.02292872878855816, 0.004827100797591191, 0.013877914793074675, 0.0003725349017416699, 0.0024835660116111326, 0.31106664295429437, 0.06258586349260054, 0.026325799723078004, 0.49596813251874317, 0.002607744312191689, 0.09847339236038141, 0.05585395741348735, 0.1375048697340515, 0.13300815282364362, 0.1583317691085722, 0.3403304693245543, 0.09111768476352809, 0.08354426680915693, 0.7731008625537188, 0.12254047177075697, 0.049813199906811774, 0.029887919944087064, 0.02440846795433777, 0.997107563015105, 0.09897902481281173, 0.08362340040259983, 0.816438199063436, 0.0009250376150730069, 0.006313982903362939, 0.6609592102929477, 0.022959937830410688, 0.0410408888718591, 0.08150777929795794, 0.11709568293509451, 0.07031480960563273, 0.9980134164858016, 0.9981184366454507, 0.009653927742418411, 0.5967557556331974, 0.0007151057586976601, 0.020738067002232144, 0.11048383971878849, 0.0951090659067888, 0.1294341423242765, 0.03146465338269704, 0.005363293190232451, 0.9956467233743964, 0.9956518378674456, 0.1476613172254778, 0.34405943880325945, 0.2137241234035833, 0.08430516853142259, 0.17330244235651232, 8.484819699217249e-06, 0.0005684829198475557, 0.03636593723084513, 0.17825971500590249, 0.2510031141846234, 0.1894879177815098, 0.007437484744526214, 0.3236025620457083, 0.007053614564163571, 0.04308942774570671, 0.0003558595236219303, 0.39251305455498914, 0.5505146830431262, 0.05551408568502113, 0.0010675785708657909, 0.1001070682470158, 0.09930407037337663, 0.19057816201036162, 0.004014989368195821, 0.3541220622748714, 0.1252676682877096, 0.1266059980771082, 0.22915350115054323, 0.1301355981813278, 0.3239421911794591, 0.03860611812475288, 0.054979768223894304, 0.028556887376701628, 0.011989236683968483, 0.10332471614695168, 0.029294089632350175, 0.05001335302794619, 0.9987635772085783, 0.7714395798794814, 0.2145617515163738, 0.01146513176041692, 0.16068862384084087, 0.8386625436076763, 0.716264605897776, 0.19500397647478718, 0.08875180980583262, 0.9954377815281095, 0.18202635154998031, 0.05233257607061934, 0.014789641063435902, 0.025028623338122294, 0.16951203988091917, 0.55518037222744, 0.9983919470855349, 0.9952871616421818, 0.9973535385154207, 0.9987747577559413, 0.055288074466751096, 0.5767794607273324, 0.1690744986918711, 0.19868037082567974, 0.1951683625136407, 0.1898497933930083, 0.20048693163427309, 0.4141546650022873, 0.00023124213567966907, 0.9983361317251646, 0.23366137582994398, 0.009233599923027424, 0.3271515750505966, 0.023981710911196226, 0.22827510920817798, 0.1181131323487258, 0.05963366616955211, 0.42885780812478985, 0.15475275515146633, 0.11109473974065245, 0.30530027239120805, 0.0015089126718865842, 0.0032693107890875987, 0.061613934102035514, 0.03671687501590688, 0.7305652186384212, 0.16598039390752425, 0.054299256261807434, 0.945165076029703, 0.9993496225292812, 0.11044773714491522, 0.02665979862118643, 0.8620001554183613, 0.02163168867788278, 0.0631984630000889, 0.0653192167920382, 0.040718472805426405, 0.08228524712763254, 0.12173126765788936, 0.527219392678594, 0.07846789030212381, 0.038945733056816506, 0.7450174490313232, 0.21564396637015065, 0.9963026706694279, 0.9571138806601808, 0.03930177770826829, 0.4163326126550104, 0.09730028827287746, 0.2129563618569103, 0.018282654692385842, 0.20600895307380368, 0.005082578004483264, 0.016820042316994976, 0.005740753573409155, 0.02142727129947621, 0.19113979828458033, 0.10526855481351459, 0.07769821902902267, 0.40712559087297157, 0.11616592073623867, 0.05459580327284763, 0.045877910534668366, 0.002070499525317575, 0.997236152624928, 0.05741854558306727, 0.04306390918730046, 0.0437816410070888, 0.8067305654420952, 0.04880576374560718, 0.997923875048289, 0.1596470341673224, 0.17410517500895967, 0.15017026958204754, 0.17471265991827215, 0.3223314928812072, 0.01895352917054969, 0.9972192669107784, 0.38897246900328203, 0.05815317344440434, 0.2500242356491372, 0.048243046845594606, 0.1912516792923073, 0.03957168607163609, 0.013901705367774766, 0.0010323048540426805, 0.008877821744767053, 0.15551931348189915, 0.2183859026917048, 0.07618112884594354, 0.013040013045702046, 0.3362950732838949, 0.14110666748401793, 0.04310067469842571, 0.007961271122639144, 0.008373061008292894, 0.4233301795588421, 0.1720710301689223, 0.22499737231357678, 0.008055759839369021, 0.15926237202432555, 0.007572414249006881, 0.0035445343293223695, 0.001127806377511663, 0.9964116663630569, 0.9993344272664635, 0.38026242034563806, 0.07961455297017557, 0.38697021243202706, 2.313031753927239e-05, 0.038211284574877984, 0.018064777998171737, 0.004926757635865019, 0.04346186665629282, 0.04688515365210513, 0.0015959919102097949, 0.9973736731796516, 0.13356708904624598, 0.19359724142658127, 0.6727128951121322, 0.9977924135513956, 0.7529556538121056, 0.14237173181242543, 0.1034763617811958, 0.07510807352980414, 0.9243563786168001, 0.9974002652035071, 0.05428866132041564, 0.9434488980818178, 0.06667284381262034, 0.0004403269817014002, 0.6187327971207508, 0.0077791100100580695, 0.13169446144386043, 0.006348047319528519, 0.03603342466923125, 0.10498129122064215, 0.027263578950345025, 0.031850428412812944, 0.9654661112633924, 0.9983517289502967, 0.9945090913662764, 0.2036673717124808, 0.7957864192739317, 0.13072624551913065, 0.019118067526513176, 0.8494622436104773, 0.5652371391804797, 0.018102509309933842, 0.06505932133812586, 0.3442768134216509, 0.0015359704869034774, 0.0029622287961709923, 0.002742804440899067, 0.9965093419227756, 0.9924575727048617, 0.998340484557042, 0.9964148918558112, 0.2728458495697017, 0.11673180253846356, 0.03560909531981414, 0.01815828039487211, 0.004244792819580494, 0.3553834877282113, 0.0662659323501177, 0.13017364646713514, 0.0004716436466200548, 0.9964208425538276, 0.9940895267601619, 0.24933166477475802, 0.14010684161397746, 0.03673334826538156, 0.007476699204458193, 0.2308024537028399, 0.3354762425652546, 0.7621211143802976, 0.23740837248383082, 0.9976416142169566, 0.0850017438296006, 0.05425643223165996, 0.6833726821559076, 0.11548869146453335, 0.004392187371134378, 0.03100367556094855, 0.026353124226806267, 0.11494238619587602, 0.8215554160885564, 0.06218194663055588, 0.9975930802305122, 0.9971099577586064, 0.19000936377282082, 0.015898742683031947, 0.1973770737966649, 0.18593773454911752, 0.1483236360063346, 0.0029083065883595024, 0.2594209476816676, 0.9987345949509917, 0.13104842365965472, 0.006151632974066296, 0.14316527648736108, 0.0805304680241406, 0.2634017391622932, 0.36182786674735395, 0.01062554786429633, 0.002982609926820022, 0.9980504590436697, 0.9490590848670699, 0.04837437083060308, 0.9790490969727125, 0.011334085466608027, 0.009175212044396974, 0.9984638773503354, 0.16005819566305934, 0.241790040256962, 0.5981019627603804, 0.9972650277127011, 0.09237548959228542, 0.4278229076481563, 0.18087651052617476, 0.052203367186810304, 0.11857913288722731, 0.00448622686761651, 0.04598382539306923, 0.04241523583928337, 0.024878167174964282, 0.010297929855210625, 0.9978776048583181, 0.08606535986070858, 0.048150079705855886, 0.594548810281003, 0.00418696345268312, 0.025819607958212577, 0.24121561669068867, 0.15177883456667918, 0.09083557732845532, 0.2692626042236354, 0.015062021750891826, 0.015988915397100553, 0.05538189536097148, 0.40204011904303566, 0.05847244266461885, 0.2325455765742313, 0.21171056826844756, 0.08871680956011137, 0.018146620137295507, 0.3904883814729144, 0.9976226190322312, 0.9998350390855871, 0.9930374463209275, 0.9934859967737809, 0.9967086518720364, 0.9471793617657042, 0.030390781660931683, 0.021948897866228438, 0.9900464063454857, 0.9941289253889145, 0.11190542495103652, 0.5130579228687352, 0.22402159458183016, 0.05911388267187522, 0.07534122301317431, 0.005163244654049708, 0.00231819147732844, 0.008956648889678063, 0.9969924612730264, 0.9978680776172753, 0.9946169116468143, 0.09193925306291503, 0.1603190725284581, 0.0574620331643219, 0.6786266116706416, 0.011492406632864379, 0.014527637951992743, 0.1342077029850758, 0.52057369327974, 0.1573827444799214, 0.17294807085705646, 0.9983488496623362, 0.996622956422266, 0.04608220296821688, 0.2760443651829875, 0.23085986746739817, 0.03919979603140526, 0.02992350842092005, 0.34367149421426674, 0.03426241714195346, 0.43702116203969404, 0.5627395785168663, 0.9966989696535501, 0.9397790881662653, 0.05898195114014217, 0.9933504688438848, 0.9954736024898625, 0.9982221827356567, 0.08320745580563406, 0.9152820138619747, 0.9989835809141046, 0.9945747307761093, 0.23055876692102498, 0.062231676683796276, 0.46002094042030905, 0.01672304968154878, 0.23028461856558977, 0.15766766320848197, 0.01706711818236145, 0.027903383694971896, 0.5315188233935423, 0.0717902590210442, 0.1942400593135422, 0.09986086702830922, 0.4511786160917585, 0.34987397749195565, 0.09889835264731346, 0.49448760783855994, 0.04543191466976125, 0.29896416043662405, 0.09468675874466094, 0.05124941593845019, 0.002160786185513035, 0.0009418811577877332, 0.012022835955290476, 0.9980232808811859, 0.99612444490549, 0.18976644866993508, 0.650627824011206, 0.0007134077017666732, 0.0906027781243675, 0.04815501986925044, 0.019618711798583512, 0.0007134077017666732, 0.9246062894680682, 0.0748629698097174, 0.9526988286633411, 0.04475766309156636, 0.9980598480305541, 0.6247114371991159, 0.16393483549495766, 0.01539179528492151, 0.19593999711915955, 0.9962819393273794, 0.995636827270838, 0.9977527561414101, 0.9986458298983761, 0.997631946321828, 0.9959870271622013, 0.9922858109797456, 0.9930152758574577, 0.11651406734462448, 0.00420026020550992, 0.28826443328225615, 0.3012104407649922, 0.0031070417958566528, 0.28613553427398397, 5.753781103438246e-05, 0.0004603024882750597, 0.4571687158748598, 0.11614712946120899, 0.25497911219301017, 0.04077487504601564, 0.010551094544461476, 0.026803183721817458, 0.0009359841934602922, 0.047326764400237684, 0.013461154491401657, 0.03187451626092959, 0.21510895314298129, 0.0004547757994566201, 0.745150147409672, 0.037291615555442845, 0.0020464910975547903, 0.9968580983749202, 0.1261986607316756, 0.3114800620112404, 0.2942735095008025, 0.00995077735543397, 0.02492877035397781, 0.09546527025369464, 0.021974633326583348, 0.06582024604896428, 0.042342630725987256, 0.007566736947361248, 0.07110679741806471, 0.16328227555259303, 0.39671368121795697, 0.027532935027196774, 0.33374705589488957, 0.005506587005439354, 0.0016759177842641513, 0.9922023918281625, 0.99826944019467, 0.2981562577363963, 0.187569447429941, 0.3920217607138259, 0.008966498132955835, 0.0022618193488537245, 0.021325725289192257, 0.05105249387412692, 0.01793299626591167, 0.020679491189519766, 0.09147282896642105, 0.010554557188433197, 0.562314331439141, 0.03003989353630987, 5.4125934299657423e-05, 0.22332360492038653, 0.07853673066880293, 0.0036805635323767048, 0.055711935485045924, 0.8908890584320407, 0.006273866608676343, 0.0065248212730233965, 0.040654655624222706, 0.10893628664009035, 0.1000796779701643, 0.06553890415745273, 0.6952437805891946, 0.030112469477748552, 0.995746236232246, 0.9982343944818897, 0.4369263394854578, 0.052595386266449266, 0.25554356531923955, 0.09088586468228661, 0.0813348537004581, 0.00021608622130833718, 4.3217244261667435e-05, 0.024504177496365437, 0.051947127602524255, 0.005920762463848438, 0.1725962598283067, 0.5081076571106768, 0.23764130078025403, 0.03448899845824183, 0.013916613412974774, 0.026320551454974028, 0.00030253507419510377, 0.002874083204853486, 0.0037816884274387975, 0.45135357595204445, 0.18083712294344428, 0.1915665387873207, 0.08575084316168931, 0.012461329424974554, 0.03176582958501988, 0.011827702505060593, 0.00849060072684707, 0.020402786821229524, 0.005575916895242851, 0.004060885984223553, 0.6522798112159082, 0.06649700799166068, 0.0700502832278563, 0.010152214960558882, 0.16852676834527744, 0.027918591141536928, 0.06592959730659262, 0.0433145714263556, 0.047948798041158265, 0.1420544931657511, 0.0657442282420005, 0.24135052209892294, 0.3326138915664368, 0.04084298389846085, 0.02020522804053963, 6.17896881973689e-05, 0.15418751806070188, 0.1787367868416554, 0.35264420933295926, 0.18311339828416145, 0.06182592486022924, 0.0006539764224434338, 0.016902775226230286, 0.028875574344810074, 0.023191010057417152, 0.9975870080417468, 0.04907035635353769, 0.9495113954409542, 0.000563060332813939, 0.8941398085085352, 0.09346801524711389, 0.01238732732190666, 0.9941251748534481, 0.8168325100854146, 0.18271253515068483, 0.0030561698905778897, 0.996311384328392, 0.0025710441525389426, 0.6723280458889335, 0.2834576178174184, 0.04113670644062308, 0.9966060772626877, 0.6983885688662362, 0.06057558078690752, 0.1644491712084431, 0.012489810471527325, 0.0035387796335994083, 0.038926575969593494, 0.020400023770161296, 0.0012489810471527324, 0.9946695087821126, 0.039549837693990425, 0.8429765405633388, 0.1163895223566004, 0.008758433307517514, 0.06787785813326075, 0.2600159888169262, 0.5627293400080003, 0.06076163107090276, 0.03941294988382882, 0.996769077639802, 0.028337929026715728, 0.08959009859998948, 0.0007040479261295834, 0.3731454008486792, 0.03045007280510448, 0.26384196031706136, 0.21103836585734262, 0.0014080958522591667, 0.0015841078337915624, 0.9946352976261756, 0.07506840974828988, 0.0821754781268262, 0.2198749279609674, 0.622756866669245, 0.9932238587380122, 0.9981160893051195, 0.9325022011654304, 0.06632019965185172, 0.7774629654147814, 0.045733115612634205, 0.17651377955753553, 0.9939175704880007, 0.6503441669921121, 0.2971026244954053, 0.016525234735247394, 0.0173456364596923, 0.017931637691438664, 0.0001172002463492723, 0.0005860012317463615, 0.9978648664632596, 0.6858551747298022, 0.03671316281739498, 0.14403994927953756, 0.12272262893395339, 0.010658660172792092, 0.999493236094886, 0.999517952152032, 0.12159904992627146, 0.0008230054140525988, 0.8773237713800702, 0.5310749298617465, 0.005355377443983999, 0.3759028684223102, 0.007319015840111465, 0.024813248823792527, 0.04498517052946559, 0.010532242306501865, 0.09606615903167819, 0.709557966709227, 0.19308346815277894, 0.996827407448462, 0.25004509544204306, 0.295165750498603, 0.2901718822070741, 0.04378426776727758, 0.06368940476027285, 0.003481640569446169, 0.002321093712964113, 0.028310309680850163, 0.016775177289149724, 0.0062247513211310295, 0.9955052499816545, 0.996396944655336, 0.17140203603839094, 0.8282434049127841, 0.22379686259477893, 0.034645907085063474, 0.36484718982134806, 0.018724487000665372, 0.15158985883173404, 0.010427408928514249, 0.19599043878540762, 0.09661350657035558, 0.0027603859020101595, 0.08695215591332003, 0.8138537767759955, 0.9956967267648195, 0.9995529143342403, 0.9766286770001896, 0.020614853340373395, 0.9967888514487016, 0.9984356571634376, 0.9951188973093336, 0.9979262180383782, 0.997256530696291, 0.03544816991779149, 0.9635457095836051, 0.29819955961249467, 0.160636953426668, 0.2966404820962216, 0.09125800395251667, 0.02759567203803323, 0.03876906090465686, 0.032428812338479725, 0.0019748315206125477, 0.03237684308793729, 0.020112099959922525, 0.027483379009361163, 0.07496191202263436, 0.20982962408886177, 0.11495222003045844, 0.02700540720050271, 0.11933362827832761, 0.1229980788129091, 0.1296100221687844, 0.06261430696045761, 0.1111284455595908, 0.0731214289584137, 0.02732821082284148, 0.312427923731404, 0.0856776339310706, 0.06130382427826603, 0.015510606142693815, 0.42395656790029757, 0.9970517339086974, 0.99791228367815, 0.9989679272486234, 0.9936123426835396, 0.9981736168565563, 0.5455766430970586, 0.3040672033537596, 0.07690878415988182, 0.041625888333956654, 7.928740635039363e-05, 0.018156816054240143, 0.002537197003212596, 0.0038057955048188944, 0.0063429925080314906, 0.001030736282555117, 0.04066358121847678, 0.34358860827722126, 0.004103664159662794, 0.4241696499578725, 0.18764937021003505, 0.2142036297262145, 0.6955859804012557, 0.09040314480380558, 0.9928825895461142, 0.9932581686344087, 0.8453734177667772, 0.003407706800300187, 0.08440627613051234, 0.012582302031877615, 0.04141674418826381, 0.01074738298556213, 0.0018349190463154855, 0.8376420798246039, 0.14641578297363345, 0.01536058437446601, 0.17668014652863426, 0.011824374500946424, 0.262963806836265, 0.015080361682366454, 0.0017136774639052788, 0.12415593225993746, 0.1739382625863858, 0.11995742247336952, 0.05612293694289788, 0.057493878914022106, 0.9967728003189099, 0.2658815828531262, 0.14661230499282796, 0.251403585475732, 0.055628027356458484, 0.07486750515476041, 0.08312589938072502, 0.017420050320394096, 0.04540826450182728, 0.048208376294068406, 0.011445618247547825, 0.10440832611656159, 0.08523791769059541, 0.3166908149012719, 0.048630019114456594, 0.37030297405863494, 0.05447861829525983, 0.02014517495610006, 0.9953074813237149, 0.9956611315661934, 0.7902335798737995, 0.20098426308929004, 0.006851736241680343, 0.9968660164519868, 0.9958903911730134, 0.0011689822962386855, 0.3471877419828896, 0.012079483727799751, 0.05766979328110849, 0.4640859716067582, 0.11728789038928145, 0.8658440781454807, 0.1086471014147139, 0.025328772108951705, 0.9974389139643569, 0.4038797233532353, 0.04524958807847487, 0.21270926363929468, 0.32492824207391235, 0.002787374625634052, 0.0006515940683300382, 0.009810110695413352, 0.017568464306347478, 0.01441515020007998, 0.6547631504942578, 0.0024776039406387467, 0.14707957938519106, 0.025451749572016215, 0.04302021387836369, 0.0015766570531337478, 0.09077039891612863, 0.002928077384391246, 0.007175995093945256, 0.016906157933193063, 0.229388588935267, 0.2784042842379778, 0.21321219321501753, 0.2548086393528019, 0.11744911543834159, 0.28609870463808523, 0.33693214991374243, 0.09781309145099386, 0.1616760479705921, 0.04311060056595362, 0.4911385242046492, 0.09407780590794551, 0.17183804805027308, 0.15753499832979315, 0.008863861798607287, 0.03162787050866691, 0.0018130626406242176, 0.9935175109475513, 0.113671431564599, 0.4948315889935123, 0.023456009687933127, 0.21110408719139814, 0.023004932578549797, 0.1339699014868488, 0.03540320426385745, 0.1737226999924168, 0.14819945970917073, 0.6413743284080222, 0.10688679720871656, 0.0724146483253859, 0.11614106536531539, 0.07912399273892005, 0.349811336319436, 0.04881626452605886, 0.04418913044775944, 0.18254043938891204, 0.10450853622680739, 0.3157975333810049, 0.5793407986486062, 0.0669874970722944, 0.011649999490833809, 0.9125832934486484, 0.008737499618125357, 0.1347457716968382, 0.8646930647300411, 0.14761691813351707, 0.055780088062978274, 0.23806695043453432, 0.01355980041309969, 0.08551919578716283, 0.0024654182569272165, 0.08952550045466955, 0.028968664518894793, 0.3121835867834088, 0.026195068979851674, 0.03771733510814269, 0.013761730377295304, 0.2565033634212541, 0.07237650791021975, 0.09875315780003574, 0.23242033526098735, 0.039883533408272505, 0.21904087517195026, 0.02943481219588162, 0.14136462034038497, 0.08520607253393066, 0.18870132730367978, 0.04862770806229376, 0.0875729078820954, 0.0901549100800933, 0.024744187730813196, 0.3107009311590805, 0.022807686082314774, 0.011910903858026487, 0.04264397677565039, 0.08455271257241026, 0.09866933936710831, 0.1660174130334803, 0.30291928330289586, 0.0008822891746686287, 0.29027313846597885, 0.002058674740893467, 0.3717342644109517, 0.42857420609258123, 0.09992744117165968, 0.049989865913190296, 0.0021962074982782346, 0.042512302288100115, 0.005124484162649215, 0.9906000691369489, 0.27998908359957936, 0.41730708802497574, 0.06353866984429357, 0.1770005802805321, 0.0015128254724831804, 0.05353074748786638, 0.0011637119019101388, 0.000930969521528111, 0.005003961178213596, 0.996921024166575, 0.008488711146740734, 0.0651811748767592, 0.924056841973777, 0.0021221777866851834, 0.977611571963418, 0.021975472759809517, 0.9481196423298889, 0.0495532914109698, 0.9988132599711538, 0.9935956536441128, 0.9983834454586976, 0.03396802689052248, 0.002690536783407721, 0.03464066108637441, 0.5236457214707277, 0.08609717706904707, 0.005381073566815442, 0.04136700304489371, 0.2323951146668419, 0.04002173465318985, 0.9984669740634214, 0.993657504349739, 0.17396350392568474, 0.6436649645250335, 0.031031327727284303, 0.08792209522730553, 0.06347317035126335, 0.4051311826024082, 0.21869660738086308, 0.14103932643100048, 0.00019033647291632992, 0.06709360670300629, 0.13266452162268194, 0.035021911016604707, 0.14368488452529207, 0.02769829099282739, 0.8194077752044769, 0.009232763664275797, 0.2788359103178465, 0.18740137859524525, 0.04039672198951133, 0.011823430826198439, 0.3501706096359104, 0.003941143608732813, 0.1274959957425065, 0.00019705718043664064, 0.08468523951629772, 0.029888908064575666, 0.8833832827974586, 0.9945666952231437, 0.9981479269482505, 0.9449940592082232, 0.0520862867280123, 0.03499856364960006, 0.9614311308448956, 0.07105677265950582, 0.4003135037661639, 0.29323301873175023, 0.0010807113712472368, 0.13761058127214815, 0.0020713634615572036, 0.04142726923114408, 0.053225035033926414, 0.9965229926519736, 0.9961993147424617, 0.09278044551806196, 0.5699370224680949, 0.0792828970875182, 0.07186532542748966, 0.12074833866243187, 0.00048639814164121604, 0.007539171195438848, 0.029183888498472962, 0.0212799186968032, 0.006931173518387328, 0.1880670437874499, 0.1783970559027266, 0.30695101248787193, 0.1977370316721732, 0.0043372739777067845, 0.049487585057113474, 0.041737374178916105, 0.0006399256688419846, 0.013509541897775229, 0.01912666721316598, 0.0043886163433278025, 0.19895060756419372, 0.07424075980796199, 0.5039594434254759, 0.032914622574958516, 0.1857847585342103, 0.4299459019868564, 0.13916290563303615, 0.4107699110531703, 0.00023068861273607317, 2.8836076592009146e-05, 0.0012399512934563933, 0.017388154184981516, 0.00020185253614406402, 0.00100926268072032, 0.9879061784812211, 0.010896024027366408, 0.2452239873845893, 0.045701795750708624, 0.6800257941795255, 0.004866394917899529, 0.024120392201762885, 0.9964093927193016, 0.17253208419694918, 0.20717992077748407, 0.016616819584542235, 0.6035087453364595, 0.9855459469768488, 0.012475265151605682, 0.8353819343240142, 0.1644599781219536, 0.9952951601583435, 0.9975047117553116, 0.9941011645566832, 0.9966792560305991, 0.12406384757182086, 0.03249291245928641, 0.06424734963540724, 0.7783529484565428, 0.9986577887191959, 0.9983239554526695, 0.9919471340835422, 0.9936655759140623, 0.9983682506316337, 0.00846331145894526, 0.010498791430083993, 0.18437163317525054, 0.20065547294436042, 0.0014998273471548563, 0.21040435070086697, 0.3839558008716432, 0.04173343161882343, 0.004327911427137245, 0.25720159338415627, 0.17157077443294078, 0.12860079669207813, 0.3963130321135677, 0.9973230858987889, 0.9732136598361336, 0.026232174119572335, 0.21656563218275088, 0.07465737251717267, 0.04058753982084387, 0.005925188295013704, 0.032588535622575375, 0.4120968459182031, 0.03495861094058085, 0.0660658494894028, 0.0020738159032547966, 0.1143561340937645, 0.9955611112299921, 0.9989047341764655, 0.9954883672072856, 0.3631287488435362, 0.6042826855577066, 0.030752562990171158, 0.0007248078819232259, 0.001242527797582673, 0.5175945042853133, 0.09829146360184517, 0.33039749933538004, 0.04569119097141476, 6.518001565109096e-05, 0.0009125202191152733, 0.007039441690317823, 0.42005848569582915, 0.0734933787172478, 0.0586598527376198, 0.4473656585219625, 0.20357297724425608, 0.3387341039989875, 0.12192210978331126, 0.030172643330213394, 0.24175060758044445, 0.006711873720394408, 0.0003078841156144224, 0.05683540774242237, 0.23338915436605043, 0.393508020528931, 0.15506800475585814, 0.1347333679223911, 0.02669741029426153, 0.024532755405537624, 0.03201065411203839, 0.997472989238443, 0.9995107427301293, 0.09077997807864728, 0.13685769422462732, 0.7723175407751582, 0.9992289386166802, 0.08624924292302312, 0.10820938226504745, 0.09484234092642393, 0.5668262053354397, 0.07542830469651837, 0.06810825824917692, 0.999102278168898, 0.16154318357359188, 0.19144772935309526, 0.18851591113941846, 0.0013193181961545614, 0.24026250261081403, 0.012753409229494094, 0.002785227302992963, 0.2011227294582287, 0.9959257850409813, 0.9881985509430387, 0.9959565694099023, 0.997401077693247, 0.9992229125908164, 0.9974152649922421, 0.3635894319330238, 0.0439347125457932, 0.45712880979234427, 0.08914590660474121, 0.0007421404146248852, 0.03226826522789001, 0.0008311972643798714, 0.004393471254579321, 0.00685737743113394, 0.0010983678136448302, 0.1458056409380716, 0.5607084574309665, 0.17750917841095062, 0.05146081444873116, 0.015775190143509848, 0.001991043416171146, 0.0026036721596084215, 0.012405732054604832, 0.011793103311167556, 0.01991043416171146, 0.12331017150168797, 0.5419109746112473, 0.07236981346601275, 0.03132695815028155, 0.04067964420674242, 0.03623030812162997, 0.10260713828932799, 0.017252527676966653, 0.031690169259270325, 0.0025424777629214014, 0.48028972129158803, 0.03685701556868056, 0.4040176253343043, 0.0301747237941889, 0.045575318118212645, 0.0021404215840168596, 0.00010441080897643217, 0.0007308756628350252, 0.9977329007485052, 0.28358615515185204, 0.4534189134855291, 0.10126178548533797, 0.11136138613741893, 0.0495677768845552, 0.0005315579290568923, 0.00026577896452844614, 0.1714677849499815, 0.16997676073302515, 0.6579144357319943, 0.9939291636068899, 0.049907998294534746, 0.2805732711546582, 0.6694156337371354, 0.09092935299124635, 0.18670827147535918, 0.057133943462833126, 0.07228883562804085, 0.28157789642955955, 0.09411188034593997, 0.01742812598998888, 0.10244707103680421, 0.05046579091014172, 0.046980165712143945, 0.09893210637162805, 0.11810499520333892, 0.07055623090069597, 0.44711176755549736, 0.004984951096244824, 0.019939804384979297, 0.15415002620695534, 0.08589454196606466, 0.07417473780551072, 0.7995235877082428, 0.09000122427815183, 0.036419986943065676, 0.6570424125390463, 0.08372149966034047, 0.21934053712182766, 0.03965755247068759, 0.9976083730810499, 0.9990544983902323, 0.9953044093088251, 0.9990721379199515, 0.9967138744387698, 0.9963122974137557, 0.07049212563048021, 0.03141308950826796, 0.06334492728364494, 0.000576386963454458, 0.0050722052783992305, 0.01400620321194333, 0.8150111663246037, 0.02145567595539737, 0.06060531844544062, 0.18306985847657234, 0.18167663276518292, 0.014907515111867003, 0.05628631874013336, 0.04110015848598846, 0.023406191951342584, 0.034830642784735986, 0.3824404577764011, 0.06725156131280469, 0.9296539357946529, 0.012594144557468729, 0.057246111624857855, 0.0160289112549602, 0.9136479415327314, 0.06288591938719836, 0.1112597035311971, 0.8247730196551785, 0.9961715227537228, 0.05230743346015535, 0.00833215754232563, 0.1444240640669776, 0.11665020559255883, 0.6383358472703914, 0.03656891365798471, 0.00324028348868219, 0.9961264270277291, 0.9916734819646499, 0.9951007698717046, 0.06529545917246057, 0.9322740559623538, 0.9944750994633234, 0.99459158875153, 0.9981022919489664, 0.1250867239275079, 0.7160338168747531, 0.14262224597342021, 0.01578196984132109, 0.04776128118945084, 0.012839054083185709, 0.8756234884732654, 0.06368170825260112, 0.8907167265475651, 0.10864421897367879, 0.9969865348162965, 0.4435261832766889, 0.26211962214166495, 0.21503700322112437, 0.05558914250702479, 0.00026376817322431694, 0.01285869844468545, 0.0027036237755492486, 0.002242029472406694, 0.005671015724322814, 0.0719600758354753, 0.08911781633717225, 0.8079503176545357, 0.030986367174706445, 0.05941009328155969, 0.2528737303779207, 0.003808339312920493, 0.006093342900672788, 0.6778843976998477, 0.2606336011266003, 0.7391370864419303, 0.9976736705084803, 0.9967039936134977, 0.2176972193643983, 0.5244850499538308, 0.07735555759593031, 0.02873890684678525, 0.022033161915868692, 0.058196286364740134, 0.0031133815750684023, 0.0627466132821478, 0.0035923633558481564, 0.001915927123119017, 0.996201007302563, 0.1548905761597738, 0.014639048356563988, 0.08027865227793154, 0.7498970577491488, 0.9960981198976369, 0.25629045844133874, 0.21330296445532337, 0.0016345054747534359, 0.05083312026483185, 0.4709010272764648, 0.00686492299396443, 0.023737999451090227, 0.9762252274260855, 0.03602766583700012, 0.12309452494308375, 0.05153957751681962, 0.07305610016947246, 0.03352574459831956, 0.6710152762141273, 0.011508837697930594, 0.9948920468409126, 0.9955377701895175, 0.9982889244142759, 0.1741613971315163, 0.02278920409274096, 0.08615430815548412, 0.02112170135424772, 0.035202835590412866, 0.24086150667124592, 0.37592922848919846, 0.04372562736493388, 0.2577124044853297, 0.0046185018724969485, 0.1193882734040461, 0.04710871909946887, 0.1203119737785455, 0.02493991011148352, 0.027018235954107146, 0.3459257902500214, 0.05288184644009006, 0.7953396675364173, 0.0009445839281905194, 0.17947094635619867, 0.023614598204762985, 0.3386171604209061, 0.035643911623253276, 0.12707829361333778, 0.4990147627255459, 0.1871680739025276, 0.09910754672073131, 0.3664454195629588, 0.05599260760591635, 0.07404659382383301, 0.03452982678741403, 0.001767523126229603, 0.09298434160486448, 0.0523313097015836, 0.03553984000240237, 0.9927141497918582, 0.997329896675232, 0.9974218874073754, 0.11373750179945986, 0.24466316327452617, 0.425704049166327, 0.061419410375396395, 0.07788294274596042, 0.06388314321254066, 0.012086783448107737, 0.0006376720284373387, 0.9969071177287371, 0.9981344108479563, 0.9998110240277361, 0.05673246055388707, 0.347055659501714, 0.10427747810309607, 0.00275623290140342, 0.3978162821025603, 0.04501847072292253, 0.026184212563332494, 0.02021237461029175, 0.9768670035474415, 0.021902847613171333, 0.9983251456389896, 0.9986103420116474, 0.1400251913799826, 0.27566212889905406, 0.09933411012426116, 0.4639581127490591, 0.02074447279703446, 0.9672610903700478, 0.031247788528841483, 0.2652293315330776, 0.130799370674655, 0.15670412225613206, 0.13070124661563426, 0.018251074977858835, 0.17269834387651373, 0.07359304426555982, 0.04346895814619067, 0.003924962360829857, 0.004709954832995829, 0.028819404587565243, 0.24496493899430455, 0.0895148172795587, 0.048032340979275404, 0.04162802884870535, 0.27684094709873275, 0.17349863771907964, 0.0835471627942548, 0.013245281906406248, 0.995453636683599, 0.001870665460566857, 0.41809373043669257, 0.1828575487704103, 0.07622961751809942, 0.3212867928523577, 0.9942449059343521, 0.08426131342103858, 0.008384210290650606, 0.07168499798506267, 0.022218157270224102, 0.021379736241159044, 0.7918886619519496, 0.7842214584086876, 0.01787195666382606, 0.19730640156863974, 0.9553323077573138, 0.023755648833379637, 0.009546662615283405, 0.00799255474767913, 0.003330231144866304, 0.6068405940228719, 0.12489107528663068, 0.18756341270346896, 0.017765982259296736, 0.00982799018599394, 0.0010583989431070396, 0.03439796565097879, 0.0042335957724281585, 0.013381186637853287, 0.4455043461819794, 0.04415079381213978, 0.43451422401538, 0.06503678355732229, 0.010799817020857468, 0.17630719216310414, 0.29857857096842627, 0.2863744597129045, 0.024868755011251957, 0.09525347212643111, 0.012587888339028767, 0.004758835835486485, 0.049660754605802514, 0.047204581271357876, 0.004375058751979511, 0.47797659313474755, 0.019488533846224575, 0.4734089680145387, 0.02882678964754052, 0.000304508341347259, 0.9964837979999767, 0.3228628255639073, 0.08109258353296622, 0.29663786519302165, 0.08931277176201821, 0.050145992553005767, 0.06564773160087199, 0.004721630609074848, 0.03017861491703864, 0.03421760013684965, 0.025144105171217866, 0.19552478871105583, 0.1819230642789824, 0.12921638210469777, 0.012751616655068859, 0.48031089400759364, 0.043725702726586324, 0.01949218073353848, 0.06269106776462376, 0.6419249249680172, 0.1854391248163661, 0.04425251842208736, 0.00210726278200416, 0.06869093291092941, 0.05397144728715883, 0.16471805340886136, 0.6644796367302152, 0.003504639434231093, 0.04485938475815799, 0.9959790696109305, 0.898142318446632, 0.10169346892231404, 0.9972317245094138, 0.9964915904197593, 0.9960229236192446, 0.9966282400961333, 0.9860060197260011, 0.01133340252558622, 0.06130656467003958, 0.9379904394516057, 0.0514756785784303, 0.09310107298444492, 0.4634399827570407, 0.28295735663019866, 0.05608306956230215, 0.04686828759455845, 0.0028597599210239055, 0.00031775110233598947, 0.0028597599210239055, 0.05374269324408222, 0.05244246679462862, 0.07584654288479346, 0.5543298762837191, 0.22927326392031852, 0.021670440824226703, 0.012135446861566953, 0.991268911108545, 0.19517709297859814, 0.17488841387479376, 0.0044635094028369635, 0.02218228915349279, 0.6032500586864502, 0.11844885451839891, 0.6631020694913939, 0.18137480848129833, 0.0010575790581999903, 0.01427731728569987, 0.007931842936499928, 0.013219738227499878, 0.10918753907463496, 0.26632200023265956, 0.1408036299603592, 0.06831067196096634, 0.05446929673855479, 0.1673412306565655, 0.07005329114364404, 0.027832117803338326, 0.0626347123373875, 0.033010186231866384, 0.9970034978180125, 0.9970295499301741, 0.997202276712348, 0.990855097247169, 0.8725634015647409, 0.12675752117325628, 0.9944625652186084, 0.9975204166938362, 0.1096868798578939, 0.15814887399409658, 0.17368630722860431, 0.5156948078310425, 0.006936354122548096, 0.006381445792744248, 0.009340956885031436, 0.005734052741306426, 0.014242647131632091, 0.9955135673606256, 0.9993214278374725, 0.9989380345902855, 0.9963022722311009, 0.055258428627968335, 0.15387937416538405, 0.2640124923336265, 0.5257225501410876, 0.0007674781753884491, 0.647072964658459, 0.05224639222415019, 0.19557473025082958, 0.10058128984328378, 0.004470279548590391, 0.9976604093274325, 0.004047355116697377, 0.29748060107725727, 0.0182130980251382, 0.012816624536208363, 0.5774226633154925, 0.08971637175345853, 0.9984736124388798, 0.08735931918140637, 0.06062207300770321, 0.08338844103679699, 0.030178673899031293, 0.27372586676840666, 0.42303088500571934, 0.031767025156875045, 0.00979483275670314, 0.9185832671388673, 0.08129055461405905, 0.12554761597402728, 0.8732534177749008, 0.9998191295037434, 0.978914003327898, 0.018254806588865233, 0.5336084435146456, 0.009875073216793765, 0.40791648595525015, 0.014787289124634766, 0.002683994258923434, 0.004152594891164558, 0.026991866792569628, 0.5360092930086454, 0.07710451139509285, 0.3424503816444123, 0.0034184266135755446, 0.04102111936290654, 0.9986893173851595, 0.2765998180570914, 0.23393418924395734, 0.25056437923224306, 0.10585180054240467, 0.011029791817423426, 0.06921408121088576, 0.029028018000118245, 0.01004651580269188, 0.008934986394734482, 0.004788126680431875, 0.0953028017145111, 0.8429368496473136, 0.06079661488684328, 0.17205131264945342, 0.1427956238588581, 0.4195753367384545, 0.04680910206495251, 0.10302181838402494, 0.020374497550593167, 0.011458478109649832, 0.040644510498362776, 0.04130624631624529, 0.0019503792527063545, 0.9990919617268537, 0.9989447753697348, 0.0916652903444625, 0.8717369111758384, 0.03620778968606269, 0.9428916646332869, 0.055464215566663935, 0.15597046390894695, 0.0002652558910016105, 0.4159212370905252, 0.1238745010977521, 0.22440648378736247, 0.019363680043117563, 0.059682575475362355, 0.9970192099708417, 0.02321552850741081, 0.007044712098800522, 0.04002677328863933, 0.17451673153846747, 0.734571343393109, 0.020653815016937895, 0.00016010709315455733, 0.9981181260315861, 0.9988633694197144, 0.9964134634037567, 0.03733683479178835, 0.19800230854739326, 0.024327484690119937, 0.0002601870020333683, 0.09405760123506264, 0.6422716145193696, 0.0003902805030500524, 0.0032523375254171036, 0.07740990174994238, 0.2622190134338242, 0.00018743317614998154, 0.25359708733092506, 0.047608026742095313, 0.3478759749343658, 0.0005622995284499447, 0.010496257864398967, 0.7223022875462162, 0.011049894501955851, 0.08862466406670715, 0.17770034423553493, 0.9979312702588534, 0.9937066965838324, 0.999093392603933, 0.1829033479017197, 0.26431310736091257, 0.17463839770281686, 0.08331069800494055, 0.17893617180624632, 0.014215714342112872, 0.05529251683065995, 0.04330833904225084, 0.0029753820716050196, 0.04713803059726028, 0.009517392844399218, 0.15129063153596872, 0.6161165065874287, 0.019034785688798437, 0.019304145863639925, 0.014186302541651666, 0.09194160634589434, 0.004579122972305285, 0.02684623075920157, 0.8813868910365125, 0.07934686098800502, 0.03183670348284152, 0.007102033853864646, 0.9999218566997979, 0.4331001697358883, 0.12233616533923647, 0.2638140468978771, 0.1282394505250107, 0.0009545737747209416, 0.011203681671724735, 0.001256018124632818, 0.0010048144997062544, 0.013238431033629901, 0.02484403850523714, 0.3295121169480389, 0.19696798016285016, 0.059671264767833965, 0.032211921688830726, 0.3812624173661604, 0.9190291055557066, 0.007459651830809307, 0.07235862275885029, 0.0838325931785018, 0.0002290508010341579, 0.36396172284327694, 0.16285511953528628, 0.03649542763144249, 0.03458667095615785, 0.07879347555575032, 0.08184748623620576, 0.07581581514230627, 0.0816184354351716, 0.10285693172508611, 0.18454020412890565, 0.07989527450557923, 0.021173659321266584, 0.04573510413393582, 0.5040271969986836, 0.0016938927457013267, 0.060133192472397096, 0.13375135097245133, 0.865788314896943, 0.06829873336575233, 0.9315188356273444]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1050951399045543141524567858469\", ldavis_el1050951399045543141524567858469_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1050951399045543141524567858469\", ldavis_el1050951399045543141524567858469_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1050951399045543141524567858469\", ldavis_el1050951399045543141524567858469_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "5      19.417538        1       1 -0.150649 -0.112489\n",
       "2      16.158928        1       2 -0.104111 -0.090823\n",
       "9      15.697739        1       3 -0.127016 -0.031752\n",
       "6       9.913884        1       4 -0.043546 -0.127862\n",
       "0       8.292607        1       5 -0.082569  0.143661\n",
       "8       8.145167        1       6  0.196025 -0.108304\n",
       "7       6.921353        1       7  0.277393 -0.023886\n",
       "4       6.151032        1       8  0.035366 -0.032159\n",
       "3       5.738263        1       9  0.030016  0.274264\n",
       "1       3.563491        1      10 -0.030908  0.109351, topic_info=      Category           Freq        Term          Total  loglift  logprob\n",
       "term                                                                      \n",
       "15410  Default   17349.000000        show   17349.000000  30.0000  30.0000\n",
       "5518   Default  117857.000000        film  117857.000000  29.0000  29.0000\n",
       "53567  Default   26834.000000         bad   26834.000000  28.0000  28.0000\n",
       "35287  Default   13078.000000       funny   13078.000000  27.0000  27.0000\n",
       "27231  Default    8635.000000      series    8635.000000  26.0000  26.0000\n",
       "17517  Default   34678.000000      really   34678.000000  25.0000  25.0000\n",
       "22462  Default   27252.000000       great   27252.000000  24.0000  24.0000\n",
       "45326  Default   27624.000000      people   27624.000000  23.0000  23.0000\n",
       "50035  Default   10277.000000      horror   10277.000000  22.0000  22.0000\n",
       "37201  Default    7393.000000          tv    7393.000000  21.0000  21.0000\n",
       "13322  Default   17379.000000        life   17379.000000  20.0000  20.0000\n",
       "42696  Default    9411.000000      comedy    9411.000000  19.0000  19.0000\n",
       "1568   Default   18475.000000        love   18475.000000  18.0000  18.0000\n",
       "19352  Default   21622.000000  characters   21622.000000  17.0000  17.0000\n",
       "20446  Default   33686.000000         see   33686.000000  16.0000  16.0000\n",
       "18565  Default   58761.000000        like   58761.000000  15.0000  15.0000\n",
       "31654  Default   19746.000000       watch   19746.000000  14.0000  14.0000\n",
       "37505  Default   20840.000000       films   20840.000000  13.0000  13.0000\n",
       "45392  Default   43233.000000        good   43233.000000  12.0000  12.0000\n",
       "42559  Default   22407.000000      movies   22407.000000  11.0000  11.0000\n",
       "14884  Default   39808.000000       would   39808.000000  10.0000  10.0000\n",
       "13028  Default    4282.000000     episode    4282.000000   9.0000   9.0000\n",
       "13960  Default   21018.000000       think   21018.000000   8.0000   8.0000\n",
       "55228  Default   18899.000000        best   18899.000000   7.0000   7.0000\n",
       "17991  Default   16183.000000         man   16183.000000   6.0000   6.0000\n",
       "29092  Default    7686.000000       woman    7686.000000   5.0000   5.0000\n",
       "53310  Default    9334.000000        role    9334.000000   4.0000   4.0000\n",
       "13095  Default    6673.000000     effects    6673.000000   3.0000   3.0000\n",
       "13519  Default   10626.000000       young   10626.000000   2.0000   2.0000\n",
       "1247   Default   27348.000000         get   27348.000000   1.0000   1.0000\n",
       "...        ...            ...         ...            ...      ...      ...\n",
       "33856  Topic10     364.112991    baseball     431.975944   3.1635  -6.7287\n",
       "4969   Topic10    1597.638459       jokes    2839.679420   2.7593  -5.2499\n",
       "31384  Topic10    2745.464405       shows    7177.587894   2.3734  -4.7085\n",
       "35287  Topic10    3992.633481       funny   13078.927080   2.1479  -4.3340\n",
       "50400  Topic10    1734.595099       humor    4315.489718   2.4230  -5.1676\n",
       "42696  Topic10    2920.167482      comedy    9411.825359   2.1641  -4.6468\n",
       "18039  Topic10     235.312705    simpsons     252.782235   3.2628  -7.1653\n",
       "41663  Topic10     643.806547       steve    1290.542982   2.6390  -6.1588\n",
       "50883  Topic10     509.770989       pilot     880.310866   2.7881  -6.3922\n",
       "27467  Topic10    1032.159541   hilarious    3076.223795   2.2424  -5.6868\n",
       "48228  Topic10     487.871480         fox     878.993611   2.7457  -6.4361\n",
       "13390  Topic10     721.925115     writers    1893.708813   2.3701  -6.0443\n",
       "55062  Topic10     564.941320        tony    1176.321435   2.6010  -6.2895\n",
       "17419  Topic10    1395.257841         new   12553.041600   1.1375  -5.3853\n",
       "19921  Topic10     686.556183        team    2138.276503   2.1984  -6.0945\n",
       "21646  Topic10     573.980271        news    1353.912272   2.4763  -6.2736\n",
       "19115  Topic10    1253.842087       every   11792.604961   1.0932  -5.4922\n",
       "18565  Topic10    1872.915758        like   58761.676088  -0.1116  -5.0909\n",
       "158    Topic10     802.190326       laugh    4093.089782   1.7047  -5.9388\n",
       "16803  Topic10     580.653022      humour    1487.880376   2.3935  -6.2620\n",
       "49019  Topic10    1068.587321       years   13097.531144   0.8283  -5.6521\n",
       "45100  Topic10    1288.738097       first   25773.117017   0.3388  -5.4648\n",
       "19352  Topic10    1172.266809  characters   21622.473314   0.4196  -5.5595\n",
       "47255  Topic10     916.529450      always    9127.440404   1.0360  -5.8056\n",
       "55228  Topic10     951.710419        best   18899.893620   0.3458  -5.7679\n",
       "760    Topic10     793.398575      family    8052.936748   1.0170  -5.9498\n",
       "14884  Topic10     988.749821       would   39808.342746  -0.3610  -5.7297\n",
       "35221  Topic10     884.272518        time   35157.345787  -0.3484  -5.8414\n",
       "51675  Topic10     887.010703         one   77496.905874  -1.1357  -5.8383\n",
       "22462  Topic10     742.982643       great   27252.474862  -0.2678  -6.0155\n",
       "\n",
       "[838 rows x 6 columns], token_table=       Topic      Freq     Term\n",
       "term                           \n",
       "539        1  0.997587     1/10\n",
       "43848      3  0.978338    10/10\n",
       "43848      5  0.020191    10/10\n",
       "13804      8  0.998878     13th\n",
       "23065      1  0.995028     2/10\n",
       "3125       4  0.995173     9/11\n",
       "10495     10  0.996240      abc\n",
       "47597      2  0.986920   abrupt\n",
       "47597      5  0.010669   abrupt\n",
       "12687      1  0.600508   acting\n",
       "12687      2  0.084836   acting\n",
       "12687      3  0.244051   acting\n",
       "12687      5  0.057713   acting\n",
       "12687      7  0.005033   acting\n",
       "12687      9  0.007829   acting\n",
       "23562      1  0.198427   action\n",
       "23562      2  0.001934   action\n",
       "23562      3  0.071267   action\n",
       "23562      5  0.288744   action\n",
       "23562      6  0.239814   action\n",
       "23562      8  0.199878   action\n",
       "26147      1  0.276921    actor\n",
       "26147      3  0.228127    actor\n",
       "26147      5  0.158622    actor\n",
       "26147      7  0.078448    actor\n",
       "26147      9  0.257780    actor\n",
       "23347      2  0.198555   affair\n",
       "23347      5  0.044866   affair\n",
       "23347      7  0.756035   affair\n",
       "49732      1  0.034589    agent\n",
       "...      ...       ...      ...\n",
       "13390      1  0.329512  writers\n",
       "13390      2  0.196968  writers\n",
       "13390      3  0.059671  writers\n",
       "13390      4  0.032212  writers\n",
       "13390     10  0.381262  writers\n",
       "2624       1  0.919029     yeah\n",
       "2624       6  0.007460     yeah\n",
       "2624       8  0.072359     yeah\n",
       "49019      1  0.083833    years\n",
       "49019      2  0.000229    years\n",
       "49019      3  0.363962    years\n",
       "49019      4  0.162855    years\n",
       "49019      5  0.036495    years\n",
       "49019      6  0.034587    years\n",
       "49019      7  0.078793    years\n",
       "49019      8  0.081847    years\n",
       "49019      9  0.075816    years\n",
       "49019     10  0.081618    years\n",
       "13519      2  0.102857    young\n",
       "13519      3  0.184540    young\n",
       "13519      4  0.079895    young\n",
       "13519      5  0.021174    young\n",
       "13519      6  0.045735    young\n",
       "13519      7  0.504027    young\n",
       "13519      8  0.001694    young\n",
       "13519      9  0.060133    young\n",
       "2132       1  0.133751   zombie\n",
       "2132       8  0.865788   zombie\n",
       "25744      1  0.068299  zombies\n",
       "25744      8  0.931519  zombies\n",
       "\n",
       "[2465 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'ylab': 'PC2', 'xlab': 'PC1'}, topic_order=[6, 3, 10, 7, 1, 9, 8, 5, 4, 2])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_tf, corpus_tf, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with LDA vectors (we'll have to use only the labeled sentences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For predictions we need only the labeled set:\n",
    "#print(\"Parsing sentences from training set...\")\n",
    "#sentences2 = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "#                                                         'review', \n",
    "#                                                         remove_html=True,\n",
    "#                                                         remove_stopwords=True,)\n",
    "#\n",
    "#print(\"Creating Dictionary\")\n",
    "#additional_stopwords=set(['n\\'t', 'movie'])\n",
    "#dictionary2 = prep_corpus(labeled_sentences, additional_stopwords)\n",
    "#dictionary2.compactify()\n",
    "#print('dictionary done')\n",
    "#\n",
    "print(\"Creating Corpora for predictions\")\n",
    "corpus_tf2 = [dictionary.doc2bow(sentence) for sentence in labeled_sentences]\n",
    "print('corpus tf done')\n",
    "\n",
    "lda_tf2 = models.LdaModel(corpus_tf2, id2word=dictionary, num_topics=10, passes=10)\n",
    "corpus_lda_tf2 = lda_tf2[corpus_tf2]\n",
    "print('corpus lda tf done')\n",
    "\n",
    "print('Creating lda vectors')\n",
    "X = gensim.matutils.corpus2csc(corpus_lda_tf2)\n",
    "X = X.T\n",
    "print('lda vectors done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[0].todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Sentences, Model and the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(os.path.join(outputs, 'sentences2.pkl'),'wb') as f:\n",
    "#    pickle.dump(sentences2,f)\n",
    "\n",
    "lda_tf2.save(os.path.join(outputs, 'model_tf2.lda'))\n",
    "corpora.MmCorpus.serialize(os.path.join(outputs, 'corpus_lda_tf2.mm'), corpus_lda_tf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Sentences, Model and the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open(os.path.join(outputs, 'sentences2.pkl'),'rb') as f:\n",
    "#    sentences2 = pickle.load(f)\n",
    "    \n",
    "lda_tf = models.LdaModel.load(os.path.join(outputs, 'model_tf2.lda'))\n",
    "corpus_lda_tf = corpora.MmCorpus(os.path.join(outputs, 'corpus_lda_tf2.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_traincv_tf, X_testcv_tf, y_traincv_tf, y_testcv_tf = train_test_split(X,\n",
    "                                                                        train[\"sentiment\"],\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "clf_LR_tf_lda = LR(penalty='l2',\n",
    "                   dual=False,\n",
    "                   tol=0.0001,\n",
    "                   C=1.0,\n",
    "                   fit_intercept=True,\n",
    "                   intercept_scaling=1,\n",
    "                   class_weight=None,\n",
    "                   random_state=0,\n",
    "                   solver='liblinear',\n",
    "                   max_iter=100,\n",
    "                   multi_class='ovr',\n",
    "                   verbose=0).fit(X_traincv_tf, y_traincv_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801\n"
     ]
    }
   ],
   "source": [
    "eval_LR_tf_lda = clf_LR_tf_lda.score(X_testcv_tf, y_testcv_tf)\n",
    "print(eval_LR_tf_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Word Vectors\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors  \n",
    "\n",
    "Introducing Distributed Word Vectors: This part of the tutorial will focus on using distributed word vectors created by the Word2Vec algorithm.\n",
    "\n",
    "Word2vec, published by Google in 2013, is a neural network implementation that learns distributed representations for words. Other deep or recurrent neural network architectures had been proposed for learning word representations prior to this, but the major problem with these was the long time required to train the models. Word2vec learns quickly relative to other models.\n",
    "\n",
    "Word2Vec does not need labels in order to create meaningful representations. This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics. Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math. The famous example is that, with highly trained word vectors, \"king - man + woman = queen.\"\n",
    "\n",
    "Distributed word vectors are powerful and can be used for many applications, particularly word prediction and translation. Here, we will try to apply them to sentiment analysis.\n",
    "\n",
    "Using word2vec in Python: In Python, we will use the excellent implementation of word2vec from the gensim package. If you don't already have gensim installed, you'll need to install it. There is an excellent tutorial that accompanies the Python Word2Vec implementation, here.\n",
    "\n",
    "Although Word2Vec does not require graphics processing units (GPUs) like many deep learning algorithms, it is compute intensive. Both Google's version and the Python version rely on multi-threading (running multiple processes in parallel on your computer to save time). ln order to train your model in a reasonable amount of time, you will need to install cython (instructions here). Word2Vec will run without cython installed, but it will take days to run instead of minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to Train a Model\n",
    "\n",
    "First, we read in the data with pandas, as we did in Part 1. Unlike Part 1, we now use unlabeledTrain.tsv, which contains 50,000 additional reviews with no labels. When we built the Bag of Words model in Part 1, extra unlabeled training reviews were not useful. However, since Word2Vec can learn from unlabeled data, these extra 50,000 reviews can now be used.  \n",
    "\n",
    "The functions we write to clean the data are also similar to Part 1, although now there are a couple of differences. First, to train Word2Vec it is better not to remove stop words because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors. For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but we leave that as an exercise for the reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Sentences (with stopwords) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"Parsing sentences from training set\")\n",
    "sentencesw2v = Text_Cleaning_Utilities.df_to_list_of_tokens(train,\n",
    "                                                         'review', \n",
    "                                                         remove_html=True,\n",
    "                                                         remove_stopwords=False,)\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "sentencesw2v += Text_Cleaning_Utilities.df_to_list_of_tokens(unlabeled_train,\n",
    "                                                          'review', \n",
    "                                                          remove_html=True,\n",
    "                                                          remove_stopwords=False,)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(os.path.join(outputs, 'sentencesw2v.pkl'),'wb') as f:\n",
    "    pickle.dump(sentencesw2v,f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(os.path.join(outputs, 'sentencesw2v.pkl'),'rb') as f:\n",
    "    sentencesw2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html  \n",
    "\n",
    "Training and Saving Your Model\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
    "\n",
    "Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "\n",
    "Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n",
    "\n",
    "Choosing parameters is not easy, but once we have chosen our parameters, creating a Word2Vec model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 2   # Minimum word count\n",
    "num_workers = -1       # Number of threads to run in parallel\n",
    "context = 8          # Context window size\n",
    "downsampling = 1e-1   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsouza/python/3/venv/local/lib/python3.5/site-packages/gensim/models/word2vec.py:879: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  logger.warn(\"supplied example count (%i) did not equal expected count (%i)\", example_count, total_examples)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "model = Word2Vec(sentencesw2v,\n",
    "                 workers = num_workers,\n",
    "                 size = num_features,\n",
    "                 min_count = min_word_count, \n",
    "                 window = context,\n",
    "                 sample = downsampling,\n",
    "                 seed=1,)\n",
    "\n",
    "##Optionally converting the model for Bigrams (to capture more context):\n",
    "\n",
    "#bigram_transformer = gensim.models.Phrases(sentencesw2v)\n",
    "#model = Word2Vec(bigram_transformer[sentencesw2v],\n",
    "#                 workers = num_workers,\n",
    "#                 size = num_features,\n",
    "#                 min_count = min_word_count, \n",
    "#                 window = context,\n",
    "#                 sample = downsampling,\n",
    "#                 seed=1,)\n",
    "\n",
    "# If you don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"{}features_{}minwords_{}context\".format(num_features, min_word_count, context)\n",
    "model.save(os.path.join(outputs,model_name))\n",
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Model Results\n",
    "\n",
    "Congratulations on making it successfully through everything so far! Let's take a look at the model we created out of our 75,000 training reviews.\n",
    "\n",
    "The \"doesnt_match\" function will try to deduce which word in a set is most dissimilar from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onion\n",
      "film\n",
      "berlin\n",
      "[('yin', 0.2213948369026184), ('sergeant-major', 0.2179437279701233), ('divide', 0.21367846429347992), ('snakebite', 0.2130860984325409), ('disciplined', 0.2125629186630249), ('primo', 0.21055549383163452), ('ritai', 0.20921048521995544), ('t-100', 0.2090751826763153), ('margo', 0.20838135480880737), ('bhajpayee', 0.20791473984718323)]\n",
      "[('extirpate', 0.25063517689704895), ('dark-skinned', 0.23022422194480896), ('career.and', 0.22532200813293457), ('magic.the', 0.22442518174648285), ('sided', 0.22419637441635132), ('billington', 0.2208838164806366), ('metal-heads', 0.21967144310474396), ('11:30', 0.21800413727760315), ('grudges', 0.21430742740631104), ('non-spoiler', 0.21369311213493347)]\n",
      "[('unfunny.the', 0.2343425750732422), ('reals', 0.22748948633670807), ('pruning', 0.22593837976455688), ('plucked', 0.22383436560630798), ('cares.the', 0.22056631743907928), ('isvery', 0.21861034631729126), ('surveyor', 0.21812129020690918), ('charlton', 0.21337270736694336), ('arbaaz', 0.21230556070804596), ('brittany', 0.21020078659057617)]\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"captain onion starship alien\".split()))\n",
    "print(model.doesnt_match(\"father mother son daughter film\".split()))\n",
    "print(model.doesnt_match(\"france england germany berlin\".split()))\n",
    "print(model.most_similar(\"man\"))\n",
    "print(model.most_similar(\"queen\"))\n",
    "print(model.most_similar(\"awful\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a reasonably good model for semantic meaning - at least as good as Bag of Words. But how can we use these fancy distributed word vectors for supervised learning? The next section takes a stab at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: More Fun With Word Vectors\n",
    "--\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors  \n",
    "\n",
    "Numeric Representations of Words\n",
    "\n",
    "Now that we have a trained model with some semantic understanding of words, how should we use it? If you look beneath the hood, the Word2Vec model trained in Part 2 consists of a feature vector for each word in the vocabulary, stored in a numpy array called \"syn0\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(os.path.join(outputs,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97995, 300)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in syn0 is the number of words in the model's vocabulary, and the number of columns corresponds to the size of the feature vector, which we set in Part 2.  Setting the minimum word count to 40 gave us a total vocabulary of 16,492 words with 300 features apiece. Individual word vectors can be accessed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05040679, -0.09797853,  0.0072781 , -0.00326661, -0.00463918,\n",
       "       -0.0306842 ,  0.09378875, -0.0299057 ,  0.06544782,  0.00169166], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'of', 'to', 'is', 'it', 'in', 'this', 'that', 'was']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words To Paragraphs, \n",
    "--\n",
    "Attempt 1:  Vector Averaging  \n",
    "--\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given paragraph\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # Index2word is a list that contains the names of the words in the model's vocabulary. \n",
    "    #Convert it to a set, for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate\n",
    "    # the average feature vector for each one and return a 2D numpy array\n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "        # Print a status message every 2000th review\n",
    "        if counter%2000. == 0.:\n",
    "            print(\"Review {} of {}\".format(counter, len(reviews)))\n",
    "        #Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the pickles from Part 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outputs, 'clean_reviews.pkl'),'rb') as f:\n",
    "    (clean_train_reviews, \n",
    "     clean_test_reviews,\n",
    "     clean_train_reviews_sw,\n",
    "     clean_test_reviews_sw) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentences2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call these functions to create average vectors for each paragraph. The following operations will take a few minutes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 18000.0 of 25000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-ab08ce0a3dde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate average feature vectors for training and testing sets, using the functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# we defined above. Notice that we now use stop word removal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-125-c089142f5d3d>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[1;34m(reviews, model, num_features)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Review {} of {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-124-f028d683985a>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[1;34m(words, model, num_features)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Index2word is a list that contains the names of the words in the model's vocabulary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#Convert it to a set, for speed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mindex2word_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Loop over each word in the review and, if it is in the model's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# vocabulary, add its feature vector to the total\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets, using the functions \n",
    "# we defined above. Notice that we now use stop word removal.\n",
    "trainDataVecs = getAvgFeatureVecs(sentences2, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "trainDataVecs = Imputer().fit_transform(trainDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the average paragraph vectors with the classifiers from Part 1.  \n",
    "Note that, as in Part 1, we can only use the labeled training reviews to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvWV, X_testcvWV, y_traincvWV, y_testcvWV = train_test_split(trainDataVecs,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 300 trees\n",
    "clf_RF_WV = RandomForestClassifier(n_estimators=300, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=False, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_RF_WV_tts = clf_RF_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_RF_WV_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_LR_WV = LR(penalty='l2',\n",
    "               dual=False,\n",
    "               tol=0.0001,\n",
    "               C=1.0,\n",
    "               fit_intercept=True,\n",
    "               intercept_scaling=1,\n",
    "               class_weight=None,\n",
    "               random_state=None,\n",
    "               solver='liblinear',\n",
    "               max_iter=100,\n",
    "               multi_class='ovr',\n",
    "               verbose=0).fit(X_traincvWV, y_traincvWV)\n",
    "\n",
    "eval_LR_WV_tts = clf_LR_WV.score(X_testcvWV, y_testcvWV)\n",
    "print(eval_LR_WV_tts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Submission  \n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews[0], model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.isnan(testDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "testDataVecs = Imputer().fit_transform(testDataVecs)\n",
    "\n",
    "print(np.isnan(testDataVecs).any()) #testando se não há valores que inviabilizam o treinamento\n",
    "print(np.isfinite(testDataVecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = clf_RF_WV.predict(testDataVecs)\n",
    "result_prob = clf_RF_WV.predict_proba(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result,})# \"probs\":result_prob[:,1]})\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv(os.path.join(outputs,'Word2Vec_AverageVectors.csv'), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that this produced results much better than chance, but underperformed Bag of Words by a few percentage points.\n",
    "\n",
    "Since the element-wise average of the vectors didn't produce spectacular results, perhaps we could do it in a more intelligent way? A standard way of weighting word vectors is to apply \"tf-idf\" weights, which measure how important a given word is within a given set of documents. One way to extract tf-idf weights in Python is by using scikit-learn's TfidfVectorizer, which has an interface similar to the CountVectorizer that we used in Part 1. However, when we tried weighting our word vectors in this way, we found no substantial improvement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering \n",
    "--\n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an average of 5 words per cluster\n",
    "word_vectors = model.syn0\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignment for each word is now stored in idx, and the vocabulary from our original Word2Vec model is still stored in model.index2word. For convenience, we zip these into one dictionary as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to a cluster number\n",
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little abstract, so let's take a closer look at what our clusters contain. Your clusters may differ, as Word2Vec relies on a random number seed. Here is a loop that prints out the words for clusters 0 through 9:\n",
    "\n",
    "Run k-means on the word vectors and print a few clusters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the first ten clusters\n",
    "for cluster in range(0,10):\n",
    "    # Print the cluster number\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        #print(len(word_centroid_map.values()))\n",
    "        #print(cluster)\n",
    "        #print(word_centroid_map.keys())\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the clusters are of varying quality. Some make sense, some cointain mostly names, and some contain related adjectives. On the other hand, some are a little mystifying. Perhaps our algorithm works best on adjectives.\n",
    "\n",
    "At any rate, now we have a cluster (or \"centroid\") assignment for each word, and we can define a function to convert reviews into bags-of-centroids. This works just like Bag of Words but uses semantically related clusters instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    # The number of clusters is equal to the highest cluster index in the word / centroid map\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function above will give us a numpy array for each review, each with a number of features equal to the number of clusters. Finally, we create bags of centroids for our training and test set, then train a random forest and extract results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ****** Create bags of centroids\n",
    "\n",
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_traincvCT, X_testcvCT, y_traincvCT, y_testcvCT = train_test_split(train_centroids,\n",
    "                                                                    train[\"sentiment\"],\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "clf_RF_CT = RandomForestClassifier(n_estimators=100, \n",
    "                                   criterion='gini', \n",
    "                                   max_depth=None, \n",
    "                                   min_samples_split=2, \n",
    "                                   min_samples_leaf=1, \n",
    "                                   min_weight_fraction_leaf=0.0, \n",
    "                                   max_features='auto', \n",
    "                                   max_leaf_nodes=None, \n",
    "                                   bootstrap=True, \n",
    "                                   oob_score=False, \n",
    "                                   n_jobs=1, \n",
    "                                   random_state=None, \n",
    "                                   verbose=0, \n",
    "                                   warm_start=False, \n",
    "                                   class_weight=None).fit(X_traincvCT, y_traincvCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_RF_CT_tts = clf_RF_CT.score(X_testcvCT, y_testcvCT)\n",
    "print(eval_RF_CT_tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = clf_RF_CT.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(os.path.join(outputs,\"BagOfCentroids.csv\"), index=False, quoting=3)\n",
    "print(\"Wrote BagOfCentroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the code above gives about the same (or slightly worse) results compared to the Bag of Words in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Comparing deep and non-deep learning methods\n",
    "--\n",
    "\n",
    "You may ask: Why is Bag of Words better?\n",
    "\n",
    "The biggest reason is, in our tutorial, averaging the vectors and using the centroids lose the order of words, making it very similar to the concept of Bag of Words. The fact that the performance is similar (within range of standard error) makes all three methods practically equivalent.  \n",
    "\n",
    "A few things to try:\n",
    "\n",
    "First, training Word2Vec on a lot more text should greatly improve performance. Google's results are based on word vectors that were learned out of more than a billion-word corpus; our labeled and unlabeled training sets together are only a measly 18 million words or so. Conveniently, Word2Vec provides functions to load any pre-trained model that is output by Google's original C tool, so it's also possible to train a model in C and then import it into Python.\n",
    "\n",
    "Second, in published literature, distributed word vector techniques have been shown to outperform Bag of Words models. In this paper, an algorithm called Paragraph Vector is used on the IMDB dataset to produce some of the most state-of-the-art results to date. In part, it does better than the approaches we try here because vector averaging and clustering lose the word order, whereas Paragraph Vectors preserves word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Deep Learning?\n",
    "\n",
    "The term \"deep learning\" was coined in 2006, and refers to machine learning algorithms that have multiple non-linear layers and can learn feature hierarchies [1].\n",
    "\n",
    "Most modern machine learning relies on feature engineering or some level of domain knowledge to obtain good results. In deep learning systems, this is not the case -- instead, algorithms can automatically learn feature hierarchies, which represent objects in increasing levels of abstraction. Although the basic ingredients of many deep learning algorithms have been around for many years, they are currently increasing in popularity for many reasons, including advances in compute power, the falling cost of computing hardware, and advances in machine learning research.\n",
    "\n",
    "Deep learning algorithms can be categorized by their architecture (feed-forward, feed-back, or bi-directional) and training protocols (purely supervised, hybrid, or unsupervised) [2]. \n",
    "\n",
    "Some good background materials include:\n",
    "\n",
    "[1] \"Deep Learning for Signal and Information Processing\", by Li Deng and Dong Yu (out of Microsoft)\n",
    "\n",
    "[2] \"Deep Learning Tutorial\" (2013 Presentation by Yann LeCun and Marc'Aurelio Ranzato)\n",
    "\n",
    "Where Does Word2Vec Fit In?\n",
    "\n",
    "Word2Vec works in a way that is similar to deep approaches such as recurrent neural nets or deep neural nets, but it implements certain algorithms, such as hierarchical softmax, that make it computationally more efficient.  \n",
    "\n",
    "See Part 2 of this tutorial for more on Word2Vec, as well as this paper: Efficient Estimation of Word Representations in Vector Space\n",
    "\n",
    "In this tutorial, we use a hybrid approach to training -- consisting of an unsupervised piece (Word2Vec) followed by supervised learning (the Random Forest). \n",
    "\n",
    "Libraries and Packages \n",
    "\n",
    "The lists below should in no way be considered exhaustive.\n",
    "\n",
    "In Python:\n",
    "\n",
    "Theano offers very low-level, nuts and bolts functionality for building deep learning systems. You can also find some good tutorials on their site.  \n",
    "Caffe is a deep learning framework out of the Berkeley Vision and Learning Center.  \n",
    "Pylearn2 wraps Theano and seems slightly more user friendly.  \n",
    "OverFeat was used to win the Kaggle Cats and Dogs competition.  \n",
    "\n",
    "\n",
    "More Tutorials  \n",
    "The O'Reilly Blog has a series of deep learning articles and tutorials:  \n",
    "\n",
    "http://radar.oreilly.com/2014/07/what-is-deep-learning-and-why-should-you-care.html  \n",
    "http://radar.oreilly.com/2014/07/how-to-build-and-run-your-first-deep-learning-network.html  \n",
    "Webcast: How to Get Started with Deep Learning in Computer Vision  \n",
    "There are several tutorials using Theano as well.  \n",
    "\n",
    "If you want to dive into the weeds of creating a neural network from scratch, check out Geoffrey Hinton's Coursera course.\n",
    "\n",
    "For NLP, check out this recent lecture at Stanford: http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/  \n",
    "\n",
    "This free, online book also introduces neural nets for deep learning: http://neuralnetworksanddeeplearning.com/  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#word2vec example\n",
    "# load up unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
    "sentences = word2vec.Text8Corpus('/tmp/text8')\n",
    "#train the skip-gram model; default window=5\n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "# ... and some hours later... just as advertised...\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "[('queen', 0.5359965)]\n",
    " \n",
    "# pickle the entire model to disk, so we can load&resume training later\n",
    "model.save('/tmp/text8.model')\n",
    "# store the learned weights, in a format the original C tool understands\n",
    "model.save_word2vec_format('/tmp/text8.model.bin', binary=True)\n",
    "# or, import word weights created by the (faster) C word2vec\n",
    "# this way, you can switch between the C/Python toolkits easily\n",
    "model = word2vec.Word2Vec.load_word2vec_format('/tmp/vectors.bin', binary=True)\n",
    " \n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)\n",
    "# which word doesn't go with the others?\n",
    "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "\n",
    "#http://rare-technologies.com/word2vec-tutorial/\n",
    "Gensim only requires that the input must provide sentences sequentially, when iterated over. No need to keep everything in RAM: we can provide one sentence, process it, forget it, load another sentence…\n",
    "\n",
    "For example, if our input is strewn across several files on disk, with one sentence per line, then instead of loading everything into an in-memory list, we can process the input file by file, line by line:\n",
    ">>> class MySentences(object):\n",
    "...     def __init__(self, dirname):\n",
    "...         self.dirname = dirname\n",
    "... \n",
    "...     def __iter__(self):\n",
    "...         for fname in os.listdir(self.dirname):\n",
    "...             for line in open(os.path.join(self.dirname, fname)):\n",
    "...                 yield line.split()\n",
    ">>>\n",
    ">>> sentences = MySentences('/some/directory') # a memory-friendly iterator\n",
    ">>> model = gensim.models.Word2Vec(sentences)\n",
    "\n",
    "Say we want to further preprocess the words from the files — convert to unicode, lowercase, remove numbers, extract named entities… All of this can be done inside the MySentences iterator and word2vec doesn’t need to know. All that is required is that the input yields one sentence (list of utf8 words) after another.\n",
    "\n",
    "calling Word2Vec(sentences) will run two passes over the sentences iterator. The first pass collects words and their frequencies to build an internal dictionary tree structure.\n",
    "\n",
    "The second pass trains the neural model.\n",
    "\n",
    "These two passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:\n",
    "\n",
    ">>> model = gensim.models.Word2Vec() # an empty model, no training\n",
    ">>> model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    ">>> model.train(other_sentences)  # can be a non-repeatable, 1-pass generator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
