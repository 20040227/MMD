{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mestrado em Modelagem Matematica da Informacao\n",
    "----------------------------------------------\n",
    "Disciplina: Modelagem e Mineracao de Dados\n",
    "------------------------------------------\n",
    "\n",
    "Master Program - Mathematical Modeling of Information\n",
    "-----------------------------------------------------\n",
    "Course: Data Mining and Modeling\n",
    "--------------------------------\n",
    "\n",
    "Professor: Renato Rocha Souza\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic: Textual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the Python Packages used:  \n",
    "http://docs.python.org/library/re.html  \n",
    "http://www.pythonware.com/library/pil/handbook/index.htm  \n",
    "http://nltk.org/  \n",
    "https://networkx.github.io/  \n",
    "https://github.com/grangier/python-goose  \n",
    "https://pypi.python.org/pypi/Topics  \n",
    "http://radimrehurek.com/gensim/  \n",
    "http://docs.python-requests.org/en/latest/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import urllib\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import re\n",
    "#import re2\n",
    "import gensim\n",
    "from collections import OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "#from goose import Goose #https://github.com/codelucas/newspaper\n",
    "from IPython.core.display import Image\n",
    "#from Topics.onlineldavb import onlineldavb\n",
    "#from Topics.visualization.wordcloud import make_wordcloud\n",
    "#from Topics.visualization.topiccloud import GenCloud\n",
    "#from Topics.visualization.printtopics import list_topics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the path to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '../datasets/'\n",
    "outputs = '../outputs/'\n",
    "oplexicon = 'oplexicon_v3.0/lexico_v3.0.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Polarizing Functions for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the polarized lexycom file - OpLexicon (BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus com 32119 palavras polarizadas\n"
     ]
    }
   ],
   "source": [
    "df_o = pd.read_csv(os.path.join(datapath,oplexicon), header=None, encoding='utf-8', usecols=[0,2], names=[u'palavra',u'polaridade'])\n",
    "df_o.drop_duplicates(subset=[u'palavra'], keep='first', inplace=True)\n",
    "df_o = df_o.set_index(u'palavra')\n",
    "print(u'Corpus com {} palavras polarizadas'.format(len(df_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polariza_texto(texto, df1):\n",
    "    polaridades = []\n",
    "    ausentes = []\n",
    "    l_palavras = texto.split()\n",
    "    for palavra in l_palavras:\n",
    "        p = palavra.lower().strip()\n",
    "        if p in df1.index:\n",
    "            polaridades.append(df1.polaridade[p])\n",
    "        else:\n",
    "            ausentes.append(p)\n",
    "    num_pal = len(l_palavras)\n",
    "    num_pol = len(polaridades)\n",
    "    razao_pol = num_pol/float(num_pal) if num_pal else 0.0\n",
    "    polaridade = sum(polaridades)/float(num_pol) if num_pol else 0.0\n",
    "    #print('Foram polarizadas {} palavras de um total de {} ({:.2%})'.format(num_pol, num_pal, razao_pol))\n",
    "    #print('A polaridade mensurada do texto é de {:.3}'.format(polaridade))\n",
    "    return polaridade, ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polariza_counter(dicionario, df1):\n",
    "    polaridades = []\n",
    "    fator_div = 0\n",
    "    ausentes = []\n",
    "    for palavra, frequencia in dicionario.iteritems():\n",
    "        p = palavra.lower().strip()\n",
    "        if p in df1.index:\n",
    "            polaridades.append(df1.polaridade[p] * frequencia)\n",
    "            fator_div += frequencia\n",
    "        else:\n",
    "            ausentes.append((palavra,frequencia))\n",
    "    num_pal = len(dicionario)\n",
    "    num_pol = len(polaridades)\n",
    "    razao_pol = num_pol/float(num_pal) if num_pal else 0.0\n",
    "    polaridade = sum(polaridades)/float(fator_div)\n",
    "    print('Foram polarizadas {} palavras de um total de {} ({:.2%})'.format(num_pol, num_pal, razao_pol))\n",
    "    print('A polaridade mensurada do texto é de {:.3}'.format(polaridade))\n",
    "    return polaridade, ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capturing some html pages from a newspaper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_uol = 'http://busca.uol.com.br/uol/?ref=homeuol&ad=on&q={}&start={}'\n",
    "query = 'Corrupção'\n",
    "results_pages_to_capture = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'Corrupção'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f97a3ae5e2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrawpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery4url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_pages_to_capture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Page {} captured'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# accept a URL or a Request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullurl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[1;32m    267\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                  method=None):\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mfull_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown url type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown url type: 'Corrupção'"
     ]
    }
   ],
   "source": [
    "rawpages = ''\n",
    "query4url = urllib.request.urlopen.quote(query.encode('utf8'))\n",
    "for i in range(0,10*(results_pages_to_capture),10):\n",
    "    try:\n",
    "        print(u'Page {} captured'.format(i)) \n",
    "        request = url_uol.format(query4url,str(i))\n",
    "        rawpages = rawpages + urllib2.urlopen(request).read()\n",
    "    except Exception as inst:\n",
    "        print(type(inst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = re.findall(r'(https?://\\S+)', rawpages)\n",
    "links = [l.strip('\"') for l in links]\n",
    "links = [l.split('\"')[0] for l in links]\n",
    "links = [l for l in links if l.endswith('shtml')]\n",
    "links = sorted(list(set(links)))\n",
    "\n",
    "print('{} extracted links'.format(len(links)))\n",
    "for link in links:\n",
    "    print link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goose_extract(htmltxt):\n",
    "    g = Goose({'use_meta_language': False, 'target_language':'pt'})\n",
    "    article = g.extract(url=htmltxt)\n",
    "    title = article.title\n",
    "    meta = article.meta_description\n",
    "    txt = article.cleaned_text\n",
    "    fig = article.top_image.src\n",
    "    return txt, title, meta, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deupau = []\n",
    "for link in links:\n",
    "    try:\n",
    "        print(link)\n",
    "        rawpage = goose_extract(link)[0]\n",
    "        if rawpage:\n",
    "            f = codecs.open(os.path.join(outputs,'html_out','{}'.format(link[29:].replace('/','_'))), mode='w', encoding='utf-8')\n",
    "            f.write(rawpage)\n",
    "            f.close()\n",
    "            print('Sucesso!\\n')\n",
    "            time.sleep(1)\n",
    "    except Exception as inst:\n",
    "        print(inst)\n",
    "        print(type(inst))\n",
    "        deupau.append(link)\n",
    "        print('Insucesso!\\n')\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for biziu in deupau:\n",
    "    print biziu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinando os arquivos e pastas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuspath = os.path.join(outputs,'html_out')\n",
    "onlydirs = [f for f in os.listdir(corpuspath) if os.path.isdir(os.path.join(corpuspath,f)) and not f.startswith('.')]\n",
    "onlyfiles = [f for f in os.listdir(corpuspath) if os.path.isfile(os.path.join(corpuspath,f)) and not f.startswith('.')]\n",
    "onlydirs.sort()\n",
    "\n",
    "print 'Files in the folder:'\n",
    "for i, w in enumerate(onlyfiles[0:]):\n",
    "    print i+1, '--' ,w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = ''\n",
    "list_raw_texts = []\n",
    "for filename in onlyfiles:\n",
    "    page_txt = codecs.open(os.path.join(corpuspath,filename), encoding='utf-8').read()\n",
    "    raw_texts += u'\\n'+ page_txt.lower()\n",
    "    list_raw_texts.append(page_txt)\n",
    "len(list_raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocorrências de determinada expressão no corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expressao = u'nova_classe_média'.lower()\n",
    "#expressao = u'Marcelo Néri'.lower()\n",
    "expressao = u'Brasil'.lower()\n",
    "#expressao = u'índice'.lower()\n",
    "#expressao = u'Instituto de Pesquisa'.lower() #muitas grafias diferentes para o Ipea\n",
    "#expressao = u'Ipea'.lower()\n",
    "#expressao = u'porta dos fundos'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0 \n",
    "positions = []\n",
    "while pos != -1:\n",
    "    position = raw_texts.find(expressao,pos+1)\n",
    "    pos = position\n",
    "    positions.append(position)\n",
    "positions.pop()\n",
    "print(u'A expressão buscada ocorre {} vezes'.format(len(positions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for position in positions: #[0:10]:\n",
    "    print raw_texts[position - 200:position + 200].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um passo opcional - retirar as stopwords que podem interferir nas análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = [w for w in nltk.corpus.stopwords.words('portuguese')]\n",
    "#ignore_words.extend([s.decode('utf-8') for s in string.punctuation])\n",
    "ignore_words.extend([u' ', u'', u'é', u'r',u'c',u'ainda',u'vai',u'ser',u'globo',u'sobre',u'nesta',u'\\u2013',u'\\u2014', u'pode',\n",
    "                     u'ter', u'disse'])\n",
    "ignore_expressions = [u'Copyright © 2013', u'Copyright © 2014',u'Todos os direitos reservados', \n",
    "                      u'Agência Estado', u'Jornal O Globo', u'Folha de S.Paulo', 'Globo Digital', u'SEGUNDO CADERNO',\n",
    "                      u'Noblat', u'Agamenon Mendes Pedreira', u'Agamenon', u'Merval Pereira', u'Merval', u'Amaury de Souza',\n",
    "                      u'Boa Viagem', u'v\\xeddeos pol\\xedticos', u'Esta\\xe7\\xe3o Jazz e Tal', u'a r\\xe1dio do blog',\n",
    "                      u'Siga o', u'Leia a', u'Ou\\xe7a a', u'Curta a P\\xe1gina', u'Visite a p\\xe1gina', u'no Twitter','no Facebook', u'Blog do', \n",
    "                      u'Tradu\\xe7\\xe3o', u'mat&eacute;ria na &iacute;ntegra', u'\\xedntegra da mat\\xe9ria', u'para assinantes',\n",
    "                      u'por exemplo', u'cada vez', 'datafolha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ignore_expressions:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige_distorcoes(texto):\n",
    "    texto = texto.replace(u'\\xe0', u'a a') #separa o a com crase\n",
    "    texto = texto.replace(u'\\u201c', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u201d', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u2018', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u2019', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'get\\xfalio vargas', u'getulio vargas') #homogeniza as formas de Getulio Vargas\n",
    "    return texto\n",
    "    \n",
    "def limpa_stopwords(texto):\n",
    "    for expression in ignore_expressions:\n",
    "        texto = texto.replace(expression.lower(),'') #retira as expressoes\n",
    "    lista = [w.strip(string.punctuation) for w in texto.split() if w.strip(string.punctuation) not in ignore_words] #retira stopw.\n",
    "    texto = u' '.join(lista)\n",
    "    return texto, lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = corrige_distorcoes(raw_texts)\n",
    "cleaned_texts, list_cleaned_words = limpa_stopwords(raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira coisa a se analisar são - pura e simplesmente - as palavras (sem stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = Counter(list_cleaned_words)\n",
    "df_freq_words = pd.DataFrame(freq_words.values(), columns = [u'Frequência'], index=freq_words.keys())\n",
    "df_freq_words = df_freq_words.sort_index(by=u'Frequência', ascending=False)\n",
    "df_freq_words.index.name = u'Tokens'\n",
    "df_freq_words[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando a polaridade total dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polariza_counter(freq_words, df_o);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar as palavras mais frequentes de várias formas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vk(lst):\n",
    "    \"\"\"Print a list of value/key pairs nicely formatted in key/value order.\"\"\"\n",
    "\n",
    "    # Find the longest key: remember, the list has value/key pairs, so the key\n",
    "    # is element [1], not [0]\n",
    "    longest_key = max([len(word) for word, count in lst])\n",
    "    # Make a format string out of it\n",
    "    fmt = '%'+str(longest_key)+'s -> %s'\n",
    "    # Do actual printing\n",
    "    for k,v in lst:\n",
    "        print fmt % (k,v)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_histogram(freqs, show=10, title=None):\n",
    "    \"\"\"Plot a histogram of word frequencies, limited to the top <show> ones.\n",
    "    \"\"\"\n",
    "    sorted_f = sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
    "\n",
    "    # Don't show the tail\n",
    "    if isinstance(show, int):\n",
    "        # interpret as number of words to show in histogram\n",
    "        show_f = sorted_f[-show:]\n",
    "    else:\n",
    "        # interpret as a fraction\n",
    "        start = -int(round(show*len(freqs)))\n",
    "        show_f = sorted_f[start:]\n",
    "\n",
    "    # Now, extract words and counts, plot\n",
    "    n_words = len(show_f)\n",
    "    ind = np.arange(n_words)\n",
    "    words = [i[0] for i in show_f]\n",
    "    counts = [i[1] for i in show_f]\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if n_words<=20:\n",
    "        # Only show bars and x labels for small histograms, they don't make\n",
    "        # sense otherwise\n",
    "        ax.bar(ind, counts)\n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(words, rotation=45)\n",
    "        fig.subplots_adjust(bottom=0.25)\n",
    "    else:\n",
    "        # For larger ones, do a step plot\n",
    "        ax.step(ind, counts)\n",
    "\n",
    "    # If it spans more than two decades, use a log scale\n",
    "    if float(max(counts))/min(counts) > 100:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words=15      \n",
    "w_items = freq_words.items()\n",
    "w_items.sort(key = lambda wc: wc[1])\n",
    "print 'Number of unique words:',len(w_items)\n",
    "#print('{} least frequent words:').format(number_words)\n",
    "#print_vk(w_items[:10])\n",
    "#print('{} most frequent words:').format(number_words)\n",
    "print_vk(w_items[:-10:-1])\n",
    "plot_word_histogram(w_items, number_words,'Frequencies for {} most frequent words'.format(number_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_words = nltk.FreqDist(freq_words)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,8)) \n",
    "fd_words.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a distribuição cumulativa das frequências:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,8)) \n",
    "fd_words.plot(30, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.array(freq_words.values())\n",
    "words = np.array(freq_words.keys())\n",
    "count = count.astype(int)\n",
    "make_wordcloud(words, count, 'test.png')\n",
    "Image(filename='test.png', width=640, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora construir um grafo de palavras para estudar co-ocorrências nos textos dos feeds\n",
    "\n",
    "Abordagem e funções aproveitadas de https://github.com/ipython/talks/blob/master/notebook/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs(items):\n",
    "    \"\"\"Make all unique pairs (order doesn't matter)\"\"\"\n",
    "    pairs = []\n",
    "    nitems = len(items)\n",
    "    for i, wi in enumerate(items):\n",
    "        for j in range(i+1, nitems):\n",
    "            pairs.append((wi, items[j]))\n",
    "    return pairs\n",
    "\n",
    "def co_occurrences(lines, words):\n",
    "    \"\"\"Return histogram of co-occurrences of words in a list of lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lines : list\n",
    "      A list of strings considered as 'sentences' to search for co-occurrences.\n",
    "\n",
    "    words : list\n",
    "      A list of words from which all unordered pairs will be constructed and\n",
    "      searched for co-occurrences.\n",
    "    \"\"\"\n",
    "    wpairs = all_pairs(words)\n",
    "\n",
    "    # Now build histogram of co-occurrences\n",
    "    co_occur = {}\n",
    "    for w1, w2 in wpairs:\n",
    "        rx = re.compile('%s .*%s|%s .*%s' % (w1, w2, w2, w1))\n",
    "        co_occur[w1, w2] = sum([1 for line in lines if rx.search(line)])\n",
    "\n",
    "    return co_occur\n",
    "\n",
    "def co_occurrences_graph(word_hist, co_occur, cutoff=0):\n",
    "    \"\"\"Convert a word histogram with co-occurrences to a weighted graph.\n",
    "    Edges are only added if the count is above cutoff.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()\n",
    "    for word, count in word_hist:\n",
    "        g.add_node(word, count=count)\n",
    "    for (w1, w2), count in co_occur.iteritems():\n",
    "        if count<=cutoff:\n",
    "            continue\n",
    "        g.add_edge(w1, w2, weight=count)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 15\n",
    "popular = w_items[-n_nodes:]\n",
    "pop_words = [wc[0] for wc in popular]\n",
    "co_occur = co_occurrences(list_raw_texts, pop_words)\n",
    "wgraph = co_occurrences_graph(popular, co_occur, cutoff=1)\n",
    "wsubgraph = list(nx.connected_component_subgraphs(wgraph))[1] #we have to choose the biggest con. comp.\n",
    "centrality = nx.eigenvector_centrality_numpy(wsubgraph)\n",
    "c = centrality.items()\n",
    "c.sort(key=lambda x:x[1], reverse=True)\n",
    "print '\\nGraph centrality'\n",
    "for node, cent in c:\n",
    "    print \"%15s: %.3g\" % (node, float(cent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad0 = 0.2\n",
    "def rescale_arr(arr, amin, amax):\n",
    "    \"\"\"Rescale an array to a new range.\n",
    "    Return a new array whose range of values is (amin, amax).\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "    amin : float\n",
    "      new minimum value\n",
    "    amax : float\n",
    "      new maximum value\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> a = np.arange(5)\n",
    "    >>> rescale_arr(a,3,6)\n",
    "    array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])\n",
    "    \"\"\"\n",
    "    # old bounds\n",
    "    m = arr.min()\n",
    "    M = arr.max()\n",
    "    # scale/offset\n",
    "    s = float(amax-amin)/(M-m)\n",
    "    d = amin - s*m\n",
    "\n",
    "    # Apply clip before returning to cut off possible overflows outside the\n",
    "    # intended range due to roundoff error, so that we can absolutely guarantee\n",
    "    # that on output, there are no values > amax or < amin.\n",
    "    return np.clip(s*arr+d,amin,amax)\n",
    "\n",
    "def plot_graph(wgraph, pos=None, fig=None, title=None):\n",
    "    \"\"\"Conveniently summarize graph visually\"\"\"\n",
    "\n",
    "    # config parameters\n",
    "    edge_min_width= 3\n",
    "    edge_max_width= 12\n",
    "    label_font = 16\n",
    "    node_font = 18\n",
    "    node_alpha = 0.4\n",
    "    edge_alpha = 0.55\n",
    "    edge_cmap = plt.cm.Spectral\n",
    "\n",
    "    # Create figure\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "    fig.subplots_adjust(0,0,1)\n",
    "\n",
    "    # Plot nodes with size according to count\n",
    "    sizes = []\n",
    "    degrees = []\n",
    "    for n, d in wgraph.nodes_iter(data=True):\n",
    "        sizes.append(d['count'])\n",
    "        degrees.append(wgraph.degree(n))\n",
    "\n",
    "    sizes = rescale_arr(np.array(sizes, dtype=float), 100, 1000)\n",
    "\n",
    "    # Compute layout and label edges according to weight\n",
    "    pos = nx.spring_layout(wgraph) if pos is None else pos\n",
    "    labels = {}\n",
    "    width = []\n",
    "    for n1, n2, d in wgraph.edges_iter(data=True):\n",
    "        w = d['weight']\n",
    "        labels[n1, n2] = w\n",
    "        width.append(w)\n",
    "\n",
    "    width = rescale_arr(np.array(width, dtype=float), edge_min_width, \n",
    "                        edge_max_width)\n",
    "\n",
    "    # Draw\n",
    "    nx.draw_networkx_nodes(wgraph, pos, node_size=sizes, node_color=degrees,alpha=node_alpha)\n",
    "    nx.draw_networkx_edges(wgraph, pos, width=width, edge_color=width, edge_cmap=edge_cmap, alpha=edge_alpha)\n",
    "    nx.draw_networkx_edge_labels(wgraph, pos, edge_labels=labels,font_size=label_font)\n",
    "    nx.draw_networkx_labels(wgraph, pos, font_size=node_font, font_weight='bold')\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=label_font)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Mark centrality axes\n",
    "    kw = dict(color='k', linestyle='-')\n",
    "    cross = [ax.axhline(0, **kw), ax.axvline(rad0, **kw)]\n",
    "    [ l.set_zorder(0) for l in cross]\n",
    "    \n",
    "def centrality_layout(wgraph, centrality):\n",
    "    \"\"\"Compute a layout based on centrality.\n",
    "    \"\"\"\n",
    "    # Create a list of centralities, sorted by centrality value\n",
    "    cent = sorted(centrality.items(), key=lambda x:float(x[1]), reverse=True)\n",
    "    nodes = [c[0] for c in cent]\n",
    "    cent  = np.array([float(c[1]) for c in cent])\n",
    "    rad = (cent - cent[0])/(cent[-1]-cent[0])\n",
    "    rad = rescale_arr(rad, rad0, 1)\n",
    "    angles = np.linspace(0, 2*np.pi, len(centrality))\n",
    "    layout = {}\n",
    "    for n, node in enumerate(nodes):\n",
    "        r = rad[n]\n",
    "        th = angles[n]\n",
    "        layout[node] = r*np.cos(th), r*np.sin(th)\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Graph visualization for query:\"\n",
    "plot_graph(wsubgraph, centrality_layout(wsubgraph, centrality), \n",
    "           plt.figure(figsize=(12,12)), \n",
    "           title = u'Centrality and term co-occurrence graph, q=\"{}\"'.format(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender os contextos, uma análise de concordâncias\n",
    "(palavras associadas à expressão escolhida, posicionadas na mesma sentença):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = nltk.tokenize.WordPunctTokenizer()\n",
    "tokens = tknzr.tokenize(raw_texts) # texto com stopwords\n",
    "#tokens = tknzr.tokenize(cleaned_texts) # texto sem stopwords\n",
    "nltk_text = nltk.Text(tokens)\n",
    "nltk_text.concordance(query.lower(), width=120, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palavras que ocorrem em contextos similares (associadas às mesmas palavras que a consulta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colocações (que não são co-ocorrências) nos informam sobre as palavras (quaisquer, não necessariamente ligadas à consulta) que ocorrem conjuntamente (bigramas frequentes):\n",
    "http://en.wikipedia.org/wiki/Collocation\n",
    "http://en.wikipedia.org/wiki/Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocations(num=100, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur.\n",
    "\n",
    "Below we are using Pointwise Mutual Information.\n",
    "\n",
    "http://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 50\n",
    "freq_min_b = 10\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = nltk.collocations.BigramCollocationFinder.from_words(nltk_text)\n",
    "finder2.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder2.apply_freq_filter(freq_min_b)\n",
    "for a, b in finder2.nbest(bigram_measures.pmi, max_items):\n",
    "    print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 50\n",
    "freq_min_t = 6\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder3 = nltk.collocations.TrigramCollocationFinder.from_words(nltk_text)\n",
    "finder3.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder3.apply_ngram_filter(lambda w1, w2, w3:  w1 in ['da', 'de', 'das'])\n",
    "finder3.apply_freq_filter(freq_min_t)\n",
    "for a, b, c in finder3.nbest(trigram_measures.pmi, max_items):\n",
    "    print a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in (finder2.above_score(bigram_measures.raw_freq,1.0 / len(list(nltk.bigrams(tokens))))):\n",
    "        print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b, c in finder3.above_score(trigram_measures.raw_freq,1.0 / len(list(nltk.trigrams(tokens)))):\n",
    "        print a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "n = 4\n",
    "xgrams = ngrams(tokens, n)\n",
    "xgrams_counter = Counter(xgrams)\n",
    "df_ngrams = pd.DataFrame(xgrams_counter.items(), columns = [u'n-gramas',u'Frequência'])\n",
    "df_ngrams = df_ngrams.sort_index(by=u'Frequência', ascending=False)\n",
    "df_ngrams.set_index([u'n-gramas'], inplace=True)\n",
    "df_ngrams[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.dispersion_plot([u'fgv', \n",
    "                      u'ipea', \n",
    "                      u'ibge',\n",
    "                      u'aécio',\n",
    "                      u'dilma',\n",
    "                      u'lula',\n",
    "                      u'renda',\n",
    "                      u'cps-fgv',\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções a seguir permitem extrair as frases mais significativas do texto (sumarização automática)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(sentences, important_words):\n",
    "    # Approach taken from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "    CLUSTER_THRESHOLD = 5  # Distance between words to consider\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "    #for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "    punktw2 = nltk.tokenize.WordPunctTokenizer() #trocando pelo punkt (melhor?)\n",
    "    for s in [punktw2.tokenize(s) for s in sentences]:    \n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "        # For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                # Compute an index for where any important words occur in the sentence\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError, e: # w not in this particular sentence\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "        # It is possible that some sentences may not contain any important words at all\n",
    "        if len(word_idx)== 0: continue\n",
    "        # Using the word index, compute clusters by using a max distance threshold\n",
    "        # for any two consecutive words\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "        # Score each cluster. The max score for any given cluster is the score \n",
    "        # for the sentence\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                * significant_words_in_cluster / total_words_in_cluster\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "        scores.append((sentence_idx, score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(txt):\n",
    "    TOP_SENTENCES = 10  # Number of sentences to choose on \"top n\"\n",
    "    N = 100  # Number of words to consider\n",
    "    #sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    punkts = nltk.tokenize.PunktSentenceTokenizer() #trocando pelo punkt (melhor?)\n",
    "    punktw = nltk.tokenize.WordPunctTokenizer() #trocando pelo punkt (melhor?)\n",
    "    sentences = [s for s in punkts.tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in punktw.tokenize(sentence)]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    top_n_words = [w[0] for w in fdist.items() if w[0] not in ignore_words][:N]\n",
    "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
    "    # First approach:\n",
    "    # Filter out non-significant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "    # Second Approach: \n",
    "    # Return only the top N ranked sentences\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "    # Decorate the post object with summaries\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumarios = summarize(raw_texts)\n",
    "sumarios['top_n_summary'] = set(sumarios['top_n_summary'])\n",
    "sumarios['mean_scored_summary'] = set(sumarios['mean_scored_summary'])\n",
    "\n",
    "print(u'frases mais importantes(1):\\n')\n",
    "for s in sumarios['top_n_summary']:\n",
    "    print(u'{}\\n'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'\\nfrases mais importantes(2):\\n')\n",
    "for s in sumarios['mean_scored_summary']:\n",
    "    print(u'{}\\n'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração de palavras relevantes usando TF-IDf  \n",
    "http://radimrehurek.com/gensim/models/tfidfmodel.html  \n",
    "http://radimrehurek.com/gensim/tutorial.html  \n",
    "http://radimrehurek.com/gensim/tut2.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_once = [key for key in freq_words.keys() if freq_words[key]==1]\n",
    "texts = [[word.strip(string.punctuation) for word in document.lower().split() if word not in ignore_words]\n",
    "         for document in list_raw_texts]\n",
    "texts = [[word for word in text if word not in tokens_once and len(word) > 1] for text in texts]\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "for doc in corpus_tfidf:\n",
    "    print(u'\\nRelevant Words in Document {}:\\n'.format(x))\n",
    "    relev = 0\n",
    "    top_w = ''\n",
    "    x+=1\n",
    "    for w_in_dic, tfidf_w in doc:\n",
    "        if tfidf_w > relev:\n",
    "            top_w = w_in_dic\n",
    "            relev = tfidf_w\n",
    "    if top_w != '' and relev > 0:\n",
    "        print('{}\\t{}'.format(dictionary[top_w], relev)) #Rever questão do UTF-8\n",
    "    else:\n",
    "        print('Nothing relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
